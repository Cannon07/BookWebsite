# Neural Equivariant Potentials \{#chap:neural_equivariant_potentials\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

A broad goal of molecular modeling is to derive a machine learned
potential that is as accurate as an ab initio potential but as fast as a
classical potential. Machine learned potentials have the powerful
advantage of having linear cost in the number of atoms $n$ and with the
potential for ab initio accuracy at near classical speeds due to the
power of the universal approximation theorem.

In previous chapters, we have studied various techniques for
representing atomic systems for machine learned potentials. One emerging
idea is that representations of atomic system for machine learned
potentials should transform correctly under transformations of three
dimensional space such as rotation, translation, reflection and
identical atom permutation. In this chapter, we introduce an equivariant
architecture for predicting properties of molecular and atomic systems.

## Invariances and Equivariances in Molecular Architectures

Recall that $E(3)$ is the group of isometries in Euclidean space
$\mathbb\{R\}^3$ that leave distances constant. Note that any function $f$
that depends only on $\|r_i - r_j\|$ inputs is $E(3)$ invariant. Thus
the following function is invariant to $E(3)$

$$\begin\{aligned\}
    f(r_1-r_2, r_1-r_3,\dotsc)
\end\{aligned\}$$

This fact has been used in classical potentials and not just for machine
learned potentials. The limitation of invariance in this context is that
it only applies to scalar quantities. To handle vector and tensor
properties, we can extend invariance to equivariance. Recall that
formally, a function $f: X \to Y$ is equivariant to group $G$ that acts
on both $X$ and $Y$ if for all $g \in G$, $x \in X$ we have

$$\begin\{aligned\}
f(D_X(g)x) = D_Y(g)f(x)  
\end\{aligned\}$$

where $f: X \to Y$ is a vector space map. In our case $G = E(3)$. The
Behler-Parinello representations we saw earlier are almost equivariant,
but the interatomic cutoffs imposed in that architecture break the
equivariance.

## Constructing Invariant Representations

In this section, we discuss how to build a general family of $E(3)$
invariant operators. Invariance is constructed by composing a number of
properties:

-   Translation: Operate on interatomic distances $r_i - r_j$ to enforce
    translation invariance.

-   Permutation: Summation is permutation invariant, so the computed
    atomic feature vector sums over neighboring atoms vectors.

-   Rotational: Constrain filters to be $SO(3)$ invariant.

-   Parity: Add a output contribution rule that respects parity.

## Features and Convolutions

At a high level, NequIP follows the Behler-Parinello atomic energy
contribution paradigm $$\begin\{aligned\}
    E &= \sum_\{i\} E_i
\end\{aligned\}$$ where $E_i$ are the atomic energy contributions for atom
$i$. Every atom is associated with scalar, vector and tensor features.
Feature vectors here are geometric objects that are direct sums of
irreducible representations of $O(3)$ The feature vectors are given by
$$\begin\{aligned\}
    V^\{l,p\}_\{acm\}
\end\{aligned\}$$ where $l = 0, 1, \dotsc$ is the rotation order, and
$p \in \{+1,-1\}$ is the parity. $l, p$ together label the relevant
irreducble representation of $O(3)$. $a$ is the atom, $c$ is the channel
in the feature vector, and $m$ is the representation index in $[-l, l]$.
Convolutions are performed on these geometric objects and are defined by

$$\begin\{aligned\}
F(\vec\{r\}_\{ij\}) &= R(r_\{ij\})Y^\{(l)\}_m(\hat\{r\}_\{ij\})
\end\{aligned\}$$ where $Y^l_m$ is a spherical harmonic function
(equivariant under $SO(3)$ Here $$\begin\{aligned\}
\vec\{r\}_\{ij\} &= \vec\{r\}_i - \vec\{r\}_j \\
\hat\{r\}_\{ij\} &= \frac\{\vec\{r\}_\{ij\}\}\{\|\vec\{r\}_\{ij\}\|\} \\
r_\{ij\} &= \|\vec\{r\}_\{ij\}\|
\end\{aligned\}$$ $R$ is a rotationally invariant function given by
$$\begin\{aligned\}
    R(r_\{ij\}) &= W_n\sigma(\dotsc \sigma(W_2\sigma(W_1 B(r_\{ij\}))))
\end\{aligned\}$$

Here $W_i$ are weight matrices, $\sigma$ is the nonlinear activation,
and $B$ is defined as

$$\begin\{aligned\}
    B(r_\{ij\}) &= \frac\{2\}\{r_c\} \frac\{\sin(\frac\{n\pi\}\{r_c\})\}\{r_\{ij\}\}f_\{env\}(r_\{ij\}, r_c)
\end\{aligned\}$$ where $r_c$ is a radial cutoff distance and $f_\{env\}$ is
a polynomial Note that the use of $r_\{ij\}$ to compute all features
provides translational invariance. Parity is enforced by placing a
constraint $$\begin\{aligned\}
    p_o &= p_i p_f
\end\{aligned\}$$

The full convolution is given by

$$\begin\{aligned\}
o: l_i \otimes l_f &\to l_o \\
L^\{(l_o, p_o, l_f, l_i, p_f, p_i)\}_\{acm_o\}(\vec\{r\}_a, V^\{(l_i, p_i)\}_\{acm\}) &= \sum_\{m_f, m_i\} C^\{(l_o, m_o)\}_\{(l_i, m_i), (l_f, m_f)\} \sum_\{b \in S\} R^\{(l_f, l_i, p_f, p_i)\}_c(r_\{ab\})Y^\{(l_f)\}_\{m_f\}(\hat\{r\}_\{ab\}) V^\{(l_i, p_i)\}_\{bcm_i\}
\end\{aligned\}$$

Features and filters are combined through equivariant tensor product.
$$\begin\{aligned\}
    D_X \otimes D_Y
\end\{aligned\}$$

Message passing neural networks have emerged as a powerful type of
network for neural potentials. Let $M_t$ be a message function. $h^t_v$
is a vector at node $v$.

$$\begin\{aligned\}
    m^\{t+1\}_v &= \sum_\{w \in N(v)\} M_t(h^t_v, h^t_\{w\}, e_\{vw\}) \\
    h^\{t+1\}_v &= U(h^t_v, m^\{t+1\}_v)
\end\{aligned\}$$

## Equivariant Architecture

![A diagram of the NequIP
architecture.](figures/Differentiable Physics/Neural Potentials/NequIP/NequIP architecture.png)\{#fig:nequip_architecture\}

NequIP[@batzner2021se] is an $E(3)$ equivariant message passing network.
The difference between NequIP and SchNet is that atoms don't only carry
feature vectors of scalars but also vector and tensor features.
Exploiting equivariance on these higher geometric objects allows for
richer learned representations.

In addition, convolutions are not invariant to $E(3)$ but are
equivariant using a functional form from tensor field networks.

## Sample Efficiency

A powerful advantage of leveraging equivariances is increased sample
efficiency. Empirically, equivariant architectures like NequIP are able
to learn with orders of magnitude less data than simpler architectures
in some cases.

One idea for the sample efficiency is that the fact that internal
features are vectors means that richer internal representations can be
exploited.

The resulting architectures also appear to be more generalizable, with
demonstrated applicability to problems in amorphous materials
characterization, protein characterizations, dihedral calcluations and
more.
