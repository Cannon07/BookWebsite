# Numerical Solution of Ordinary Differential Equations \{#chap:numerical_ode\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\},
[\[chap:numerical_integration\]](#chap:numerical_integration)\{reference-type="ref+label"
reference="chap:numerical_integration"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter we will study how to numerically solve ordinary
differential equations. While the analytical solution of many ordinary
differential equations is possible, it is all too easy to end up with a
complex set of equations that aren't amenable to analytical solution.
Being comfortable with numerical methods for solution help avoid these
issues.

### First Order ODEs

First-order ODEs form the basis for solving governing equations of
engineering systems. Let us first consider different approaches for
solving first-order ODEs. As discussed in the previous lecture,
*one-step* methods and *multi-step* methods are two widely used methods
for solving these first order ODEs. *One-Step methods* attempt to trace
out the trajectory of the solution into the future by extrapolating from
an old value $y_i$ to a new value $y_\{i+1\}$ over a distance $h$, given
the value of the increment function (first-derivative of the function)
at a single point $i$. They are also referred to as Runge-Kutta methods
after the two applied mathematicians who first discussed them in the
early 1900s. *Multi-Step methods* use information from several previous
points as the basis for extrapolating to a new value. We will be
discussing one-step methods in particular for the next two lectures.
Also, emphasis is given on solving first order ODEs since higher order
ODEs can be reduced to a system of first order ODEs.

Runge-Kutta methods are a family of iterative methods, usually employed
for solving first order ODEs. Among these methods, the simplest approach
to solve a first order ODE is to use the differential equation to
estimate the slope in the form of the first derivative at $t_i$ . In
other words, the slope at the beginning of the interval is taken as an
approximation of the average slope over the whole interval. This
approach is called the *Euler's method*. It is the most basic method for
numerical integration of ordinary differential equations and is the
simplest Runge--Kutta method.

### Euler's Method

Let's look at the first order ODEs of the general form
$\frac\{dy\}\{dt\} = f(t,y)$. The first derivative of the function provides
a direct estimate of the slope at the initial value $(t_i,y_i)$. This is
given as $\phi = \frac\{dy\}\{dt\} = f(t_i,y_i)$, where $f(t_i,y_i)$ is the
differential equation evaluated at $t_i$ and $y_i$. The estimate from
Euler's method can be obtained by moving over by a step size along this
slope estimate from the initial value. The estimate at the next step is
given as

$$y_\{i+1\} = y_\{i\} + f(t_i,y_i)h$$

This formula is referred to as *Euler's method* (or the Euler-Cauchy or
point-slope method). A new value of y is predicted using the slope
(equal to the first derivative at the original value of t) to
extrapolate linearly over the step size $h$. It can be observed that the
Euler's method primarily draws its motivation from the Taylor's series
by utilizing the first two-terms of the function's approximation about
the initial value.

### Error Analysis for Euler's Method

The numerical solution of ODEs involves two types of error

1.  *Truncation* or discretization Errors - These are caused by the
    nature of the techniques employed to approximate values of y.

2.  *Roundoff* Errors - These are caused by the limited numbers of
    significant digits that can be retained by a computer.

The truncation errors are composed of two parts. The first is a *local
truncation error* that results from an application of the method in
question over a single step. The second is a *propagated truncation
error* that results from the approximations produced during the previous
steps. The sum of the two is the total error. It is referred to as the
*global truncation error*.

Insight into the magnitude and properties of the truncation error can be
gained by deriving Euler's method directly from the Taylor series
expansion. To do this, realize that the differential equation being
integrated will be of the general form of $\frac\{dy\}\{dt\} = f(t,y)$ with
an initial value of ($t_i,y_i$). If the solution, i.e. the function
describing the behavior of y has continuous derivatives, it can be
represented by a Taylor series expansion about a starting value
($t_i ,y_i$), as in

$$y_\{i+1\} = y_\{i\} + y_\{i\}'h + \frac\{y_i''\}\{2!\}h^2 + \dots + \frac\{y_i^\{n\}\}\{n!\}h^n + R_n$$

where $h = t_\{i+1\} - t_\{i\}$ and $R_n =$ the remainder term defined as

$$R_n = \frac\{y^\{n+1\}(\xi)\}\{(n+1)!\}h^\{n+1\}$$

where $\xi$ lies somewhere in the interval from $t_i$ to $t_\{i+1\}$. The
above Taylor Series expansion can also be re-written as

$$y_\{i+1\} = y_\{i\} + f(t_i,y_i)h + \frac\{f'(t_i,y_i)\}\{2!\}h^2 + \dots + \frac\{f^\{(n-1)\}(t_i,y_i)\}\{n!\}h^n + O(h^\{n+1\})$$

where $O(h^\{n+1\})$specifies that the local truncation error is
proportional to the step size raised to the $(n + 1)$th power. From the
above equation, it can be seen that the Euler's method covers only the
first two terms of the Taylor's expansion. Thus, the truncation error in
Euler's method is attributable to the remaining terms in the Taylor
series expansion that were not included and is given as

$$E_t = \frac\{f'(t_i,y_i)\}\{2!\}h^2 + \dots + \frac\{f^\{(n-1)\}(t_i,y_i)\}\{n!\}h^n + O(h^\{n+1\})$$

where $E_t$ is the *true local truncation error*. For sufficiently small
$h$, the higher-order terms are usually negligible, and the result is
often represented as

$$E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2 = O(h^2)$$

where $E_a$ is the *approximate local truncation error*.

It can also be shown that the global truncation error is $O(h)$, i.e. it
is proportional to the step size. These observations lead to some useful
conclusions:

1.  The global error can be reduced by decreasing the step size.

2.  The method will provide error-free predictions if the underlying
    function is linear, because for a straight line the second
    derivative would be zero.

This latter conclusion makes intuitive sense because Euler's method uses
straight-line segments to approximate the solution. Hence, Euler's
method is referred to as a *first-order method*.

### Stability of Euler's Method

The stability of a solution method is another important consideration
that must be considered when solving ODEs. A numerical solution is said
to be unstable, if errors grow exponentially for a problem for which
there is a bounded solution. The stability of a particular application
can depend on three factors: the differential equation, the numerical
method, and the step size.

Insight into the step size required for stability can be examined by
studying the following simple ODE:

$$\frac\{dy\}\{dt\} = -ay$$

If $y(0)=y_0$, the true solution can be determined using calculus as

$$y = y_0e^\{-at\}$$

The above solution starts at $y_0$ and asymptotically approaches zero.
Solving the same problem using Euler's method gives the following
result,

$$y_\{i+1\} = y_i + \frac\{dy_i\}\{dt\}h = y_i - ay_ih = y_i(1-ah)$$

The parenthetical quantity $1-ah$ is called an *amplification factor*.
If its absolute value is greater than unity, the solution will grow in
an unbounded fashion. So clearly, the stability depends on the step size
*h*. Based on this analysis, Euler's method is said to be *conditionally
stable*.

Note that there are certain ODEs where errors always grow regardless of
the method. Such ODEs are called *ill-conditioned*. Inaccuracy and
instability are often confused. This is usually because both represent
situations where the numerical solution breaks down or both are affected
by step size.

### Adaptive Step-Sizing

We have seen how to solve ODEs numerically using Euler's method using a
fixed step size. However, it is noticed that the local trunctation error
is proportional to the step-size chosen. By changing this step-size
dynamically at every step, an upper bound on the local truncation error
can be imposed with minimum number of steps. *Adaptive step-sizing*
methods make use of this to come up with a new step-size at every step.

The local truncation error for Euler's method is given by
$E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2$. The step-size is simply obtained by
establishing an upper bound on this truncation error ($e_\{max\}$).

$$E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2 \leq e_\{max\} \implies h \leq \sqrt\{\dfrac\{2e_\{max\}\}\{f'(t_i,y_i)\}\}$$

This condition is used for limiting the local truncation error while
achieving greater performance on segments where the derivative of $f$ is
small.

### Stiff ODEs

subtle, difficult, and important concept in the numerical solution of
ordinary differential equations. It depends on the differential
equation, the initial conditions, and the numerical method. A *stiff*
system is one involving rapidly changing components together with slowly
changing ones. In some cases, the rapidly varying components are
ephemeral (lasting a short time) transients that die away quickly, after
which the solution becomes dominated by the slowly varying components.
Although the transient phenomena exist for only a short part of the
integration interval, they can dictate the time step for the entire
solution.

Both individual ODEs and system of ODEs can be stiff. A simple example
of a stiff ODE is given below for visualization.

$$\frac\{dy\}\{dt\} = -1000y+3000-2000e^\{-t\}$$

Let the initial condition be $y(0)=0$. The analytical solution for this
problem can be solved as $y(t) = 3 - 0.998e^\{-1000t\}-2.002e^\{-t\}$.

This solution is plotted and as it can be seen, the solution is
initially dominated by the fast exponential term of $e^\{-1000t\}$.
Although the solution appears to start at 1, there is actually a fast
transient from $y$ = $0$ to $1$ that occurs in less than the 0.005 time
unit. This transient is perceptible only when the response is viewed on
the finer timescale in the inset. After a short period (t \< 0.005),
this transient dies out and the solution becomes governed by the slow
exponential ($e^\{-t\}$).

Insight into the step size required for stability of such a solution can
be gained by examining the homogeneous part of the differential
equation.

$$\frac\{dy\}\{dt\} = -1000y$$

If $y(0)=y_0$, calculus can be used to determine the solution as
$y = y_0e^\{-1000t\}$. Thus, the solution starts at $y_0$ and
asymptotically approaches zero. Euler's method can be used to solve the
same problem numerically:

$$y_\{i+1\} = y_i + \dfrac\{dy_i\}\{dt\}h$$

For this ODE, the above equation is simplified to

$$y_\{i+1\} = y_i -1000y_ih = y_i(1-1000h)$$

The stability of this formula clearly depends on the step size $h$. That
is, $|1-1000h|$ must be less than $1$. Thus, if $h>2/1000$,
$|y_i| \rightarrow \infty$ as $i \rightarrow \infty$. For the fast
transient part of the solution ($e^\{-1000t\}$ term), this criterion shows
that the step size to maintain stability must be less than
$2/1000 = 0.002$. In addition, it should be noted that, whereas this
criterion maintains stability (i.e., a bounded solution), an even
smaller step size would be required to obtain an accurate solution.
Thus, although the transient occurs for only a small fraction of the
integration interval, it controls the maximum allowable step size.

### Implicit methods and Adaptive time stepping

One way of solving the above problem is to use adaptive time-stepping,
to avoid the small time-step throughout. This is obtained by bounding
the local truncation error as discussed in the last lecture. The matlab
script for finding the adaptive time steps to solve the integral is
given below.

In contrast to explicit approaches, implicit methods offer an
alternative remedy. Such representations are called implicit because the
unknown appears on both sides of the equation. An implicit form of
Euler's method can be developed by evaluating the derivative at the
future time. An implicit form of Euler's method would be:

$$y_\{i+1\} = y_i + \dfrac\{dy_\{i+1\}\}\{dt\}h$$

## Runge-Kutta Methods

In the previous lecture, we have discussed about Euler's method which is
accurate to the first two terms of a Taylor series expansion. By
evaluating the slope at multiple places (instead of just the initial
value), a better estimate can be obtained. Runge-Kutta methods use this
methodology and achieves the accuracy of a Taylor series approach
without requiring the calculation of higher derivatives. Many variations
exist but all can be cast in the generalized form of
$y_\{i+1\} = y_\{i\} + \phi h$, where $\phi$ is called an increment
function, which can be interpreted as a representative slope over the
interval. The increment function can be written in general form as

$$\phi = a_\{1\}k_\{1\} + a_2k_2 + \dots + a_nk_n$$ where the $a$'s are
constants and the $k$'s are

$$\begin\{aligned\}
    \begin\{split\}
        k_1 &= f(t_i,y_i) \\
        k_2 &= f(t_i + p_1h,y_i + q_\{11\}k_1h)\\
        k_3 &= f(t_i + p_2h,y_i + q_\{21\}k_1h+ q_\{22\}k_2h)\\ 
        \vdots&\\
        k_n &= f(t_i + p_\{n-1\}h,y_i + q_\{n-1,1\}k_1h + \dots + q_\{n-1,n-1\}k_\{n-1\}h)
    \end\{split\}
\end\{aligned\}$$

where the $p$'s, $q$'s are constants and $f$ is the derivative function
$dy/dt$. Notice that the $k$'s are recurrence relationships. That is,
$k_1$ appears in the equation for $k_2$, which appears in the equation
for $k_3$, and so forth.

Various types of Runge-Kutta methods can be devised by employing
different numbers of terms in the increment function as specified by
$n$. It can be noted that the first-order RK method with $n$ = 1 is, in
fact, Euler's method. Once $n$ is chosen, values for the $a$'s, $p$'s,
and $q$'s are evaluated by setting the above equation equal to terms in
a Taylor series expansion. The second-order RK methods (uses an
increment function with two terms) and the fourth-order RK methods (uses
an increment function with four terms) will be discussed herewith. For
second-order methods, because terms with $h^3$ and higher are dropped
during the derivation, the local truncation error is O($h^3$) and the
global error is O($h^2$), Similarly, for fourth-order methods, the
global truncation error is O($h^4$).

### Second-Order Runge-Kutta Methods

The classical second-order Runge-Kutta method is simply obtained from
the Taylor series expansion of $y$. It is simplified as

$$\begin\{aligned\}
y_\{i+1\} = y_i + f(t_i,y_i)h + \frac\{1\}\{2!\}f'(t_i,y_i)h^2 + O(h^3)   \\
y_\{i+1\} = y_i + f(t_i,y_i)\frac\{h\}\{2\} + (\frac\{1\}\{2\}f(t_i,y_i) + \frac\{1\}\{2\}f'(t_i,y_i)h)h + O(h^3)\\
y_\{i+1\} = y_i + (\frac\{1\}\{2\}f(t_i,y_i) + \frac\{1\}\{2\}f(t_i+h,y_i + f(t_i,y_i)h))h + O(h^3)
\end\{aligned\}$$

::: marginfigure
Using Taylor series expansion, $$\begin\{aligned\}
    \begin\{split\}
f(t_i+h,y_i + f(t_i,y_i)h))& = f(t_i,y_i) + \frac\{\partial f\}\{\partial t\}h \\
& + \frac\{\partial f\}\{\partial y\}f(t_i,y_i)h+ O(h^2) \\
& = f(t_i,y_i) + \frac\{\partial f\}\{\partial y\}\frac\{\partial y\}\{\partial t\}h \\
& = f(t_i,y_i) + f'(t_i,y_i)h 
\end\{split\}
\end\{aligned\}$$
:::

This variant of the second-order Runge-kutta method is known as the
*Heun Method*. It can be re-written as
$$y_\{i+1\} = y_\{i\} + (\frac\{1\}\{2\}k_1 + \frac\{1\}\{2\}k_2)h$$ where
$$k_1 = f(t_i,y_i);\hspace\{15pt\}  k_2 = f(t_i + h,y_i + k_1h)$$ However,
other variants of these second order RK-methods exist. The general
second-order version of the RK equation $y_\{i+1\} = y_\{i\} + \phi h$ is
given as

$$y_\{i+1\} = y_\{i\} + (a_1k_1 + a_2k_2)h$$ where
$$k_1 = f(t_i,y_i) ; \hspace\{15pt\} k_2 = f(t_i + p_1h,y_i + q_\{11\}k_1h)$$

The values of $a$'s, $p$ and $q$ are obtained by setting the above
equation equal to a second-order Taylor series. The first three terms of
the Taylor series expansion of y is written as

$$y_\{i+1\} = y_i + f(t_i,y_i)h + \frac\{1\}\{2!\}f'(t_i,y_i)h^2 + O(h^3)$$

It can be noted that $dy/dt = f(t,y)$. Also,

$$f'(t_i,y_i) =\frac\{\partial f\}\{\partial t\} + \frac\{\partial f\}\{\partial y\}\frac\{\partial y\}\{\partial t\}$$

Substituting this back into the Taylor series expansion, we get

$$y_\{i+1\} = y_i + f(t_i,y_i)h + \frac\{1\}\{2\}(\frac\{\partial f\}\{\partial t\} + \frac\{\partial f\}\{\partial y\}\frac\{\partial y\}\{\partial t\})_\{|_\{(t_i,y_i)\}\}h^2 + O(h^3)$$

In a similar fashion, $k_2$ can be expanded as

$$k_2 = f(t_i + p_1h,y_i + q_\{11\}k_1h) = f(t_i,y_i) + p_1h\frac\{\partial f\}\{\partial t\} +  q_\{11\}k_1h\frac\{\partial f\}\{\partial y\} + O(h^2)$$

Now, $y_\{i+1\} = y_\{i\} + (a_1k_1 + a_2k_2)h$ is simplified as

$$\begin\{aligned\}
\begin\{split\}
y_\{i+1\} & = y_i + \{a_1f(t_i,y_i) +  a_2(f(t_i,y_i) + p_1h\frac\{\partial f\}\{\partial t\} +  q_\{11\}k_1h\frac\{\partial f\}\{\partial y\} + O(h^2))\}h \\
 & = y_i + (a_1+a_2)hf(t_i,y_i) + a_2p_1h^2\frac\{\partial f\}\{\partial t\} + a_2q_\{11\}f(t_i,y_i)h^2\frac\{\partial f\}\{\partial y\}+ O(h^3)
\end\{split\}
\end\{aligned\}$$

On comparing the above equation with (1), three equations can be derived
to evaluate the four unknown constants.

$$a_1 + a_2 = 1 ; \hspace\{10pt\}a_2p_1 = 1/2; \hspace\{10pt\} a_2q_\{11\} = 1/2$$

These equations are *underdetermined*, since we have 3 equations with 4
unknowns. Therefore, a value is assumed for one of the unknowns to
determine the other three. Suppose that we specify a value for $a_2$,
then the equations can be solved as

$$a_1 = 1 - a2; \hspace\{10pt\} p_1 = q_\{11\} = \dfrac\{1\}\{2a_2\}$$

Because we can choose an infinite number of values for $a_2$, there are
an infinite number of second-order RK methods. Every version would yield
exactly the same results if the solution to the ODE were quadratic,
linear, or a constant. However, they yield different results when (as is
typically the case) the solution is more complicated. Two of the most
commonly used and preferred versions apart from the Heun's method are
discussed here.

`The Midpoint Method (`$a_2 = 1$`):` In this method, $a_2$ is assumed to
be 1 and can be solved for $a_1$ = 0 and $p_1$ = $q_\{11\}$ = 1/2 to
obtain

$$y_\{i+1\} = y_\{i\} + k_2h$$ where

$$k_1 = f(t_i,y_i); \hspace\{15pt\}k_2 = f(t_i + \frac\{1\}\{2\}h,y_i + \frac\{1\}\{2\}k_1h)$$

`Ralston’s Method (`$a_2 = 2/3$`):`Ralston and Rabinowitz (1978)
determined that choosing $a_2$ = 2/3 provides a minimum bound on the
truncation error for the second-order RK algorithms. In this method,
$a_2$ is assumed to be 2/3 and can be solved for $a_1$ = 1/3 and $p_1$ =
$q_\{11\}$ = 3/4 to obtain

$$y_\{i+1\} = y_\{i\} + (\frac\{1\}\{3\}k_1 + \frac\{2\}\{3\}k_2)h$$ where
$$k_1 = f(t_i,y_i);\hspace\{15pt\}  k_2 = f(t_i + \frac\{3\}\{4\}h,y_i + \frac\{3\}\{4\}k_1h)$$

This is called the *backward* or *implicit* Euler's method. For the ODE
under consideration, the above equation is simplified to

$$y_\{i+1\} = y_i - 1000y_\{i+1\}h$$

which can be solved for as

$$y_\{i+1\} = \dfrac\{y_i\}\{1+1000h\}$$ For this case, regardless of the size
of the step, $|y_i|\rightarrow0$ as $i\rightarrow \infty$. Hence, the
approach is called *unconditionally stable*.

For linear ODEs, the equations need to be rearranged to solve for the
next ODE update in the implicit Euler's method approach. For nonlinear
ODEs, the solution becomes even more difficult since it involves solving
a system of nonlinear simultaneous equations. Even though stability is
gained through implicit approaches, it is at the cost of added solution
complexity. We will look at how to solve such a non-linear system in the
example below.
