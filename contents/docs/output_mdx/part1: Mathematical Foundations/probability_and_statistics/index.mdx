# Probability and Statistics \{#chap:probability\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:math_basics\]](#chap:math_basics)\{reference-type="ref+label"
reference="chap:math_basics"\}\
**Difficulty Level:** \*\

------------------------------------------------------------------------

This chapter introduces the concept of random sampling and empirical (or
observational) probability. The chapter starts by introducing the
foundational notion of a probability distribution. We then will learn to
use permutations and multinomial expansions to determine discrete
probability distributions and how to determine probabilities and $m$-th
moments using continuous probability distributions.

## Random Sampling and Empirical Probability

Consider a large population, so large in fact that it cannot analyzed
from the study of all its component. We want to infer the behavior of
the whole population, based on what can be extracted from a small
sample. Ideally this sample should be selected randomly with no inherent
bias in the sampling technique. However since sampling is random, some
random draws will give a better representation of the population than
others. Thus, there is an uncertainty about how much the sample differs
from the population. This uncertainty is measured by the probability.

One of the most known methods to determine the probability is the
theoretical or classical method, which considers the nature of the
phenomena to be studied. An example is the flipping coin experiment. In
this case, the nature of the experiment defines that there would be one
of only two possible results, *head* or *tail*, and thus the probability
is defined as $1/2$.

The theoretical method is preferable to determine the probability since
is very intuitive. However it is not always applicable, specially in the
case of large samples, where the nature of the experiment is hard to
predict. An alternative of this is the so called empirical or
observational method, which consist on using several trials or samples
of the population to determine the probability.

Consider the coin flipping experiment. In this case we would like to
find the probability of get a head. To do this, we would count the
number of heads we can get after a number $N$ of samples. The samples
are completely random since the next experiment is not correlated with
the previous. If we track the evolution of the outcomes
(Fig.[\[fig1a\]](#fig1a)\{reference-type="ref" reference="fig1a"\}), we
would observe that no conclusions can be drawn from the first few
samples, but rather after performing a big number of experiment, where
we would be able to see a clear tendency of $1/2$.

## Permutation and Multinomial Expansion

Consider the experiment of finding the probability of getting $N_H$
heads and $N_T$ tails, regardless of order, from a sample of $N$ coins.
In this case, we need to include a factor associated with the number of
permutations with $N_H$ and $N_T$. To show this, we adopt the view of a
random walk, where a head is a positive step and a tail is a negative
step. For example the experiment $HHTHHTTH$ results in an end position

$$\begin\{aligned\}
X = 1+1-1+1+1-1-1+1 = 2
\end\{aligned\}$$

Count the number of ways $M(X,N)$ of ending at any location $X$ after
$N$ steps, and assume probability is proportional to this number.
Separate $N$ steps into positive steps $N_H$ and negative steps $N_T$.
Thus, $$\begin\{aligned\}
\label\{Eq1\}
N = N_T + N_H
\end\{aligned\}$$

The end location of a path $X$ is given by: $$\begin\{aligned\}
\label\{Eq2\}
X = N_H - N_T
\end\{aligned\}$$

The combinatoric number of ways of distributing $N_H$ and $N_T$ steps
into $N = N_H + N_T$ is: $$\label\{Eq3\}
M(N_H,N_T)=\frac\{(N_H+N_T)!\}\{N_H!N_T!\}=\frac\{N!\}\{N_H!N_T!\}$$

Using ([\[Eq1\]](#Eq1)\{reference-type="ref" reference="Eq1"\}) and
([\[Eq2\]](#Eq2)\{reference-type="ref" reference="Eq2"\}), we have
$N_H=(N+X)/2$ and $N_T=(N-X)/2$. This gives (Fig.
[\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}): $$\label\{Eq4\}
M(X,N)=\frac\{N!\}\{[(N+X)/2]![(N-X)/2]!\}$$

This equation is valid for $|X| \leq N$, and integer values of
$(N \pm X)=2$.

Alternatively, $M(X,N)$ can be built recursively using:
$$\begin\{aligned\}
\label\{Eq5\}
M(X,N+1)=M(X+1,N) + M(X-1,N)
\end\{aligned\}$$

The probability of getting $N_H$ heads and $N_T$ tails is given by:
$$\begin\{aligned\}
\label\{Eq6\}
P(N_H,N_T)=M(N_H,N_T)P_H^\{N_H\}P_T^\{N_T\}=\frac\{N!\}\{N_H!N_T!\}P_H^\{N_H\}P_T^\{N_T\}
\end\{aligned\}$$

This probability is summed over the number of heads to give the binomial
expansion: $$\begin\{aligned\}
\label\{Eq7\}
\sum_\{N_H=0\}^N \frac\{N!\}\{N_H!(N-N_H)!\}P_H^\{N_H\}P_T^\{N_T\}&=\sum_\{N_H\}\sum_\{N_T\}*\frac\{N!\}\{N_H!N_T!\}P_H^\{N_H\}P_T^\{N_T\}\notag\\ &=(P_H+P_T)^N
\end\{aligned\}$$

where the $*$ indicates a sum with the constraint $N_H+N_T=N$.

We can generalize this to $N$ independent experiments with $r$ possible
outcomes (*e.g.* a dice has 6 possible outcomes). For a given
experiment, the probabilities are $P_1, P_2, . . . , P_r$. The
probability of getting $N_1, N_2, . . . , N_r$ is: $$\label\{Eq8\}
\frac\{N!\}\{N_1!N_2!...N_r!\}P_1^\{N_1\}P_2^\{N_2\}...P_r^\{N_r\}$$

which is a term in the multinomial expansion: $$\label\{Eq9\}
\sum_\{N_1\}\sum_\{N_2\}...\sum_\{N_r\}*\frac\{N!\}\{N_1!N_2!...N_r!\}P_1^\{N_1\}P_2^\{N_2\}...P_r^\{N_r\}=(P_1+P_2+...+P_r)^N$$

## Probability Density

From the experiment of coin flipping, we can observe that if we
continuously increase the number of coins, the distribution of discrete
probabilities resemble a continuous probability distribution
(Fig. [\[fig4\]](#fig4)\{reference-type="ref" reference="fig4"\}). We call
this distribution the probability density, and is use it to study the
statistics of continuous variables or large scale samples.

We can also observe that for the case of coin flipping, the probability
density resembles a normal distribution, also know as Gaussian
distribution, defined as:

$$\begin\{aligned\}
\label\{Eq20\}
f(x) = \frac\{1\}\{(2\pi)^\{1/2\}\sigma\}\exp\left[-\frac\{\left(x - \langle x \rangle \right)^2\}\{2\sigma^2\}\right]
\end\{aligned\}$$

where $\langle x \rangle$ is the expected value of $x$, and $\sigma$ the
standard deviation from the expected value.

The shape for the probability distribution for the coin flipping
experiment is not arbitrary, it is a direct consequence of the central
limit theorem. The theorem states that for $N$ independent random
variables $x_1, x_2,..., x_N$ selected from a probability distribution
with variance $\sigma^2_x$, and a sample mean $$\begin\{aligned\}
\bar\{x\} = \frac\{\left(x_1+x_2+...+x_N\right)\}\{N\},
\end\{aligned\}$$ the probability distribution for sufficiently large $N$
approaches a Gaussian distribution with standard deviation
$\frac\{\sigma_y\}\{\sqrt\{N\}\} = \frac\{\sigma_x\}\{\sqrt\{N\}\}=N^\{1/2\}$
regardless of the distribution for $x$. For example, individual
non-interacting particles may obey a complex probability distribution
with mean energy $\langle \epsilon \rangle$ and standard deviation
$\Delta \epsilon$; however, a collection of $N$ particles tends to a
Gaussian distribution with standard deviation $\Delta \epsilon/N^\{1/2\}$

The simple example of coin flipping or dice throwing has equal
probabilities for all outcomes (flat distribution). However, we are
generally interested in the properties of systems with more complex
probability distributions. Consider an experiment that can have integer
outcomes $n$ ranging from $0$ to $\infty$. For a given experiment, the
probability that the outcome of that experiment is $n$ is $P_n$ (with
$P_n\geq 0$ for all $n$), and each experiment is uncorrelated from the
previous experiments. Our goal now is to evaluate averages from the
distribution and to define several properties of the distribution. To
proceed, we use a particular distribution as a model probability
distribution, namely the Poisson distribution given by
(Fig. [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}):

::: marginfigure
![image](figures/Mathematical Foundations/probability_and_statistics/poisson.png)\{width="\\linewidth"\}
:::

$$\label\{Eq10\}
P_n=\frac\{a^n\}\{n!\}e^\{-a\}$$

where $a$ is a characteristic spread in the distribution.

Normalization of the probability distribution requires that:
$$\label\{Eq11\}
\sum_\{n=0\}^\{\infty\}P_n=\sum_\{n=0\}^\{\infty\}\frac\{a^n\}\{n!\}e^\{-a\}=1$$

which requires the property $\sum_\{n=0\}^\{\infty\}\frac\{a^n\}\{n!\}=e^a$.

We define the $m$th moment of the distribution to be: $$\label\{Eq12\}
\langle n^m\rangle =\sum_\{n=0\}^\{\infty\}n^mP_n$$

For example the mean $\langle n \rangle$ is given by: $$\begin\{aligned\}
\label\{Eq13\}
\langle n \rangle &= \sum_\{n=0\}^\{\infty\} n \frac\{a^n\}\{n!\}e^\{-a\} =  \sum_\{n=1\}^\{\infty\} \frac\{a^n\}\{(n-1)!\}e^\{-a\} \notag\\
&= \sum_\{n=1\}^\{\infty\} a \frac\{a^\{n-1\}\}\{(n-1)!\}e^\{-a\} = a \sum_\{n=0\}^\{\infty\} \frac\{a^\{n\}\}\{(n)!\}e^\{-a\}=a
\end\{aligned\}$$

We define the *variance* $\sigma^2$ of the distribution to be
$$\begin\{aligned\}
\sigma^2=\langle \left( n - \langle n \rangle \right)^2 \rangle = \langle n^2 \rangle - \langle n \rangle ^2, 
\end\{aligned\}$$

which gives $\sigma^2 = a$ for the Poisson distribution. The variance is
a simple measure of the spread in the distribution (variance is the
square of the standard deviation $\sigma$).

A more complete characterization of the uncertainty in the distribution
is given by the *information entropy* $S$ of the distribution, given by:
$$\begin\{aligned\}
\label\{Eq16\}
S = -\sum_\{n=0\}^\{\infty\}P_n \log P_n
\end\{aligned\}$$

which tends to zero if the distribution is single-valued and increases
with an increase in the spread of the distribution.

We extend these concepts to a random variable X that can take r discrete
values $(x_1,x_2,...,x_r)$. Similar to before, the probability
distribution satisfies $P(x_i) \geq 0$ and is normalized such that
$\sum_\{i=1\}^r P(x_i)=1$. The moments of the distribution are given by:
$$\begin\{aligned\}
\label\{Eq17\}
\langle X^m \rangle = \sum_\{i=1\}^r x_i^m P(x_i)
\end\{aligned\}$$

The mean is $\langle X \rangle$, and the variance $\sigma_X^2$ is
$\langle X^2 \rangle - \langle X \rangle^2$.

The probability distribution for a continuous variable $x$ is stated by
the *probability density* $f(x)$, which gives the probability:

$$\begin\{aligned\}
P(a\leq x \leq b) = \int_a^b f(x)dx 
\end\{aligned\}$$

The probability density is nonnegative $(f(x) \geq 0$ for all $x)$, and
normalized (complete) such that: $$\begin\{aligned\}
\int_\Omega f(x)dx = 1
\end\{aligned\}$$

where $\Omega$ implies integration over the entire range of $x$.
