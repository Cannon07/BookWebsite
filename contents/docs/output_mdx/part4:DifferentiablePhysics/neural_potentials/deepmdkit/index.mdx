# Generating Training Data with Active Learning Methods \{#chap:neural_potential\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

One of the major challenges facing deep potential methods is
constructing suitable training datasets that allow for a neural
potential to achieve sufficient accuracy [@zhang2020dp].

Concurrent learning methods monitor the error of the current version of
an approximate force field on sampled datapoints. Those points which
have high error are sent to a more expensive computational process which
generates a higher quality label. This higher quality label is used to
retrain the model to achieve higher accuracy. This procedure holds out
the hope of automatically generating high quality force fields given
sufficient access to compute.

Let $\mathcal\{R\}$ denote the atomic positions of the system. Let
$E_\theta(\mathcal\{R\})$ denote the current potential energy surface
where $\theta$ is the current set of parameters. The generative training
process constructs a sequence of models $$\begin\{aligned\}
    E_\{\theta_0\} \to E_\{\theta_1\} \to \dotsc \to E_\{\theta_n\}
\end\{aligned\}$$ that progressively become more accurate as training
proceeds.

Exploration of the configuration space is driven by a sampler function
$\varphi_t$ that updates the current set of atomic configurations to a
new set through the update rule $$\begin\{aligned\}
    \mathcal\{R\}_t &= \varphi_t(\mathcal\{R\}_0, E_\{\theta\})
\end\{aligned\}$$

``` \{.python language="python"\}

def generate_new_training_set($E_\{\theta\}$ : $\mathbb\{R\}^\{Nd\}$): $\mathbb\{R\}^\{Nd\}$:
  R_t = $\varphi_t$(R_0, $E_\{\theta\}$)
  return R_t
```
