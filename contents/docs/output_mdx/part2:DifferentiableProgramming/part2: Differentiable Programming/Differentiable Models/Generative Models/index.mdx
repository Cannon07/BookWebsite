# Normalizing Flows \{#chap:normalizing_flows\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:probability\]](#chap:probability)\{reference-type="ref+label"
reference="chap:probability"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

We'll start off this chapter with a mathematical review of the
underpinnings of generative modeling.

## Mathematical Underpinnings

Let's consider a probability distribution $\mathcal\{P\}$ over
$\mathbb\{R\}^N$. For now, we can informally represent this probability
distribution by a probability density function
$p: \mathbb\{R\}^n \to \mathbb\{R\}$. We previously introduced datasets by
noting that training samples $x_i$ are sampled from this underlying data
distribution $\mathcal\{P\}$, which we write mathematically as
$x_i \sim \mathcal\{P\}$. In some simple toy cases, $\mathcal\{P\}$ may be a
tractable distribution which can be described analytically. However,
even for analytical distributions, it can be quite challenging to draw
samples from $\mathcal\{P\}$. Let's illustrate this with an example.

### Multivariate Gaussians

The multivariate Gaussian distribution is a fundamental mainstay of the
modern sciences. We typically write the distribution as
$\mathcal\{N\}(\mu, \Sigma)$ where $\mu \in \mathbb\{R\}^N$ is the mean
vector and $\Sigma \in \mathbb\{R\}^\{N \times N\}$ is the covariance
matrix. Roughly, the multivariate Gaussian encodes a probability
distribution centered around the mean $\mu$, where vectors that are
closer to $\mu$ are more likely and where vectors further away grow
progressively less likely. This rough intuition is formally encoded by
the probability density function

$$p_N(x) = \frac\{1\}\{\sqrt\{(2\pi)^N |\Sigma|\}\} \exp\left (-\frac\{1\}\{2\} (x - \mu)^T\Sigma^\{-1\}(x-\mu)\right )$$

Here, $|\Sigma|$ is the determinant of the covariance matrix $\Sigma$.
How can we sample from this distribution? Creating a suitable sampling
algorithm turns out to require a bit of subtlety, so let's consider a
simpler question. Note that in the case $N = 1$, then the covariance
matrix reduces to the scalar variance $\sigma^2$. We can then simplify
the probability density function to

$$p_1(x) = \frac\{1\}\{\sigma\sqrt\{2 \pi\}\} \exp \left ( -\frac\{1\}\{2\} \left ( \frac\{x-\mu\}\{\sigma\} \right )^2 \right )$$

How can we sample from this simpler distribution? Let's assume that we
have a method to sample from the uniform distribution $\mathcal\{U\}$ on
the interval $[0, 1]$. Let's draw two samples $U_1$ and $U_2$ from
$\mathcal\{U\}$. Then let

$$\begin\{aligned\}
Z_0 &= \sqrt\{-2 \ln U_1\} \cos(2\pi U_2) \\
Z_1 &= \sqrt\{- 2 \ln U_1\} \sin(2 \pi U_2) \\
\end\{aligned\}$$

We claim that $Z_0$, and $Z_1$ are both samples from the univariate
Guassian distribution with mean $\mu = 0$ and variance $\sigma^2 = 1$.
This technique is commonly referred to as the Box-Muller transform. If
you haven't seen this technique before, it may be somewhat mysterious to
understand. Let's break down the derivation a bit. It might help if we
solve these equations for $U_1, U_2$ (the derivation is left as an
exercise).

$$\begin\{aligned\}
U_1 &= \exp\left(-\frac\{1\}\{2\}(Z_0^2 + Z_1^2) \right) \\
U_2 &= \frac\{1\}\{2\pi\} \arctan\left (\frac\{Z_1\}\{Z_0\}\right) \\
\end\{aligned\}$$

Let $f:\mathbb\{R\}^2 \to \mathbb\{R\}^2$ be defined as

$$f(x_1, x_2) = \begin\{bmatrix\}
    \exp\left(-\frac\{1\}\{2\}(x_1^2 + x_2^2) \right) \\
    \frac\{1\}\{2\pi\} \arctan\left (\frac\{x_2\}\{x_1\}\right) \\
    \end\{bmatrix\}$$

such that $f(Z_0, Z_1) = U_1, U_2$. You can show via a simple
calculation (left to the exercises) that

$$|\mathcal\{J\}(f)| = \frac\{1\}\{2\pi\}\exp \left ( -\frac\{1\}\{2\} (x_1^2 + x_1^2) \right ) = p_1(x_1) p_2(x_2)$$

That is, $|\mathcal\{J\}(f)|$ is the product of the probability density
functions of two one dimensional Gaussians!

## Introduction to Normalizing Flows

The core idea of a normalizing flow is to transform a simple probability
distribution such as a Gaussian distribution into more sophisticated
function which can model a real world data distribution through the use
of a suitable transformation function $g$. Suppose that
$$\begin\{aligned\}
    z &\sim \mathcal\{N\}(\mu, \Sigma) \\
    x &= f(z)
\end\{aligned\}$$ Then the probability density of $x$ is given by
$$\begin\{aligned\}
    P(x) &= P_z(f(x))\left | \det \mathcal\{J\}(f) \right |
\end\{aligned\}$$ We constrain the function $g$ to be one-to-one and onto.
A core idea of using a deep network to train a normalizing flow is that
we can consider a chain of such transformations $$\begin\{aligned\}
    z &\sim \mathcal\{N\}(\mu, \Sigma) \\
    x_1 &= f_1(z) \\
    x_2 &= f_2(x_1) \\
\end\{aligned\}$$ Then the final distribution is given by
$$\begin\{aligned\}
    P(x_2) &= P_z(f(x))\left | \det \mathcal\{J\}(f_2) \right | \left | \det \mathcal\{J\}(f_1) \right |
\end\{aligned\}$$ The loss for the normalizing flow is simply given by the
log-likelihood $$\begin\{aligned\}
    \mathcal\{L\} &= -\log P_z[f_n(\dotsc (f_1(x))] - \sum_i \log \left | \det \mathcal\{J\}(f_i) \right |
\end\{aligned\}$$

``` \{.python language="python"\}
class AutoregressiveNetwork(params: $\mathbb\{N\}$, hidden_units: [$\mathbb\{N\}$], activations: String)

class MaskedAutoregressiveFlow(net: Module)

class TransformedDistribution(zdist: $\mathbb\{R\}^N$, bijection: $\mathbb\{R\}^N \to \mathbb\{R\}^N$)

class NormalizingFlow(num_layers : $\mathbb\{N\}$, ndims: $\mathbb\{N\}$):
  bijects = []
  # loop over desired bijectors and put into list
  for i in num_layers:
    anet = AutoregressiveNetwork(
        params=ndim, hidden_units=[128, 128], activation="relu")
    ab = MaskedAutoregressiveFlow(anet)
    bijects.append(ab)
    # Now permute 
    permute = Permute([1, 0])
    bijects.append(permute)
  chained = Chain(bijects)
    
  def $\lambda$(zdist):
    # make transformed dist
    td = TransformedDistribution(zdist, bijector=chained)
```

``` \{.python language="python"\}
x : $\mathbb\{R\}^\{N, 2\}$
log_prob = log_prob(x)
model = Model(x, log_prob)
def neg_loglik(log_prob):
  return -log_prob
```

## Exercises \{#exercises .unnumbered\}

1.  Let $U_1$ and $U_2$ be two samples from the uniform distribution on
    the interval $[0, 1]$. Let $$\begin\{aligned\}
    Z_0 &= \sqrt\{-2 \ln U_1\} \cos(2\pi U_2) \\
    Z_1 &= \sqrt\{- 2 \ln U_1\} \sin(2 \pi U_2) \\
    \end\{aligned\}$$ Solve these equations for $U_1$ and $U_2$. Prove
    that $$\begin\{aligned\}
    U_1 &= \exp\left(-\frac\{1\}\{2\}(Z_0^2 + Z_1^2) \right) \\
    U_2 &= \frac\{1\}\{2\pi\} \arctan\left (\frac\{Z_1\}\{Z_0\}\right) \\
    \end\{aligned\}$$

2.  Let $f:\mathbb\{R\}^2 \to \mathbb\{R\}^2$ be defined as

    $$f(x_1, x_2) = \begin\{bmatrix\}
        \exp\left(-\frac\{1\}\{2\}(x_1^2 + x_2^2) \right) \\
        \frac\{1\}\{2\pi\} \arctan\left (\frac\{x_2\}\{x_1\}\right) \\
        \end\{bmatrix\}$$

    Prove that
    $|\mathcal\{J\}(f)| = \frac\{1\}\{2\pi\}\exp(-\frac\{1\}\{2\} (x_1^2 + x_2^2))$


# Energy-based-models \{#chap:energy_models\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Energy based models adapt ideas from statistical mechanics to create a
tool for modeling probability distributions. We will return to study the
relevant statistical mechanics in more depth later in the book, but in
this chapter will focus on their capacity as machine learning models.

## Boltzmann Machine

The Boltzmann Machine is a form of an energy based model. An example of
this is given in Figure
[\[fig:boltzmann_machine\]](#fig:boltzmann_machine)\{reference-type="ref"
reference="fig:boltzmann_machine"\}. The energy function for a BM is
given by $$E(v,h)= -b\sum_\{i=1\}^\{n_v\} v_i -c\sum_\{i=1\}^\{n_h\} h_i 
- \sum_\{i=1\}^\{n_v\}\sum_\{j=1\}^\{n_v\} v_i v_j u_\{i,j\}
- \sum_\{i=1\}^\{n_h\}\sum_\{j=1\}^\{n_h\} h_i h_j v_\{i,j\}
- \sum_\{i=1\}^\{n_v\}\sum_\{j=1\}^\{n_h\} v_i h_j w_\{i,j\}$$

Here $v$ is the visible layer, and $h$ is the hidden layer. The
statistical mechanics experts reading along may note the similarity to
the Hamiltonian for the Ising model.

These large networks often take a long time to reach their equilibrium
distributions, when distributions are complex multi-modal distribution.
These difficulties can be overcome by simplifying the form of the energy
function as we can see in the next section.

::: marginfigure
![image](figures/Differentiable Models/autoencoders/Boltzmann machine (2).png)\{width="\\textwidth"\}
:::

``` \{.python language="python"\}
class BoltzmannMachine(b: $\mathbb\{R\}$, c: $\mathbb\{R\}$, U: $\mathbb\{R\}^\{n_v \times n_v\}$, V: $\mathbb\{R\}^\{n_h \times n_h\}$, W: $\mathbb\{R\}^\{n_h \times n_v\}$):

  def $\lambda$(v, h):
    return (-b * sum(v) - c * sum(h) - sum(U * (v $\otimes$ v)) 
            - sum(V * (h $\otimes$ h)) - sum(W * (v $\otimes$ h)))
```

### Restricted Boltzmann Machines

Restricted Boltzmann machines are some of the most common building
blocks of deep probabilistic models. RBMs are undirected probabilistic
graphical models containing a layer of observable variables and a single
layer of latent variables. RBMs may be stacked (one on top of the other)
to form deeper models. It is a bipartite graph, with no connections
permitted between any variables in the observed layer or between any
units in the latent layer (see Fig
[\[fig:rbm\]](#fig:rbm)\{reference-type="ref" reference="fig:rbm"\}).

::: marginfigure
![image](figures/Differentiable Models/autoencoders/Restricted Boltzmann machine1.png)\{width="\\textwidth"\}
:::

Let the observed layer consist of a set of $n_v$ binary random variables
which will be referred to collectively with the vector $v$. The latent
or hidden layer of the network will consist of a set of $n_h$ binary
random variables and will be referred to as $h$. The restricted
Boltzmann machine is also an energy based model with the joint
probability distribution specified by its energy function:
$$\begin\{aligned\}
P(v, h)=\frac\{1\}\{Z\}\exp(-E(v,h))
\end\{aligned\}$$ The energy function for an RBM (in close analogy to the
mean field approximation of the Ising Model) is given by
$$\begin\{aligned\}
E(v,h)= -b\sum_\{i=1\}^\{n_v\} v_i -c\sum_\{i=1\}^\{n_h\} h_i - \sum_\{i=1\}^\{n_v\}\sum_\{j=1\}^\{n_h\} v_i h_j w_\{i,j\}
\end\{aligned\}$$ and the partition function, Z, is given by:
$$\begin\{aligned\}
Z=\sum_v \sum_h \exp\left(-E(v,h)\right)
\end\{aligned\}$$ It is apparent from the definition of the partition
function Z that the naive method of computing Z (exhaustively summing
over all states) could be computationally intractable, unless a cleverly
designed algorithm could exploit regularities in the probability
distribution to compute Z faster. In the case of restricted Boltzmann
machines, the partition function Z is intractable. The intractable
partition function Z implies that the normalized joint probability
distribution P(v) is also intractable to evaluate.

Though $P$ is intractable, the bipartite graph structure of the RBM has
the very special property that its conditional distributions $P(h|v)$
and $P(v|h)$ are factorial and relatively simple to compute and to
sample from. Deriving the conditional distributions from the joint
distribution is straightforward: $$\begin\{aligned\}
    P(h|v) &= \frac\{P(h,v)\}\{P(v)\} = \frac\{1\}\{Z'\}\prod^\{n_h\}_\{j=1\}\exp\Big(c_jh_j+v^TW_\{:,j\}h_j\Big)
\end\{aligned\}$$ Since the equations are conditioning on the visible
units v, they can be treated as constant with respect to the
distribution $P(h|v)$. The factorial nature of the conditional $P(h|v)$
follows immediately from our ability to write the joint probability over
the vector h as the product of (unnormalized) distributions over the
individual elements, $h_j$. It is now a simple matter of normalizing the
distributions over the individual binary $h_j$. $$\begin\{aligned\}
    P(h_j=1|v)=\sigma(c_j+v^TW_\{:,j\})
    \label\{hidden\}
\end\{aligned\}$$ A similar derivation will show the following expression
for the other condition of interest, $P(v|h)$: $$\begin\{aligned\}
    P(v_j = 1|h)=\sigma\Big(b_i+W_\{i,:\}h\Big)
    \label\{visible\}
\end\{aligned\}$$

``` \{.python language="python"\}
class RestrictedBoltzmannMachine(b: $\mathbb\{R\}$, c: $\mathbb\{R\}$, W: $\mathbb\{R\}^\{n_h \times n_v\}$):

  def $\lambda$(v, h):
    return (-b * sum(v) - c * sum(h) - sum(W * (v $\otimes$ h)))
```

### Training Restricted Boltzmann Machines

Because the RBM admits efficient evaluation and differentiation of
$\Tilde\{P\}(v)$ and efficient MCMC sampling in the form of block Gibbs
sampling, it can readily be trained with techniques such as Contrastive
Divergence and Stochastic Maximum Likelihood. Compared to other
undirected models used in deep learning, the RBM is relatively
straightforward to train because $P(h|v)$ can be computed exactly in
closed form. The update equations for this model are: $$\begin\{aligned\}
    -\frac\{\partial \log p(v)\}\{\partial W_\{ij\}\} &= -v_j^\{(i)\}\sigma(W_iv^\{(i)\}+c_i)+E[p(h_i|v)v_j]
    \label\{weights1\} \\
    -\frac\{\partial \log p(v)\}\{\partial c_i\} &= -\sigma(W_iv^\{(i)\}+c_i)+E[p(h_i|v)]
    \label\{weights2\} \\
    -\frac\{\partial \log p(v)\}\{\partial b_j\} &= -v_j^\{(i)\}+E[p(v_j|h)]
    \label\{weights3\} \\
\end\{aligned\}$$

### Contrastive Divergence

The gradient equation that needs to be solved with the partition
function is: $$\begin\{aligned\}
    \nabla_\theta \log Z &= E_\{x\sim p(x)\} \left [ \nabla_\theta \Tilde\{p\}(x) \right ]
\end\{aligned\}$$ The naive way of implementing this equation is to
compute it by burning in a set of Markov chains from a random
initialization every time the gradient is needed. When learning is
performed using stochastic gradient descent, this means the chains must
be burned in once per gradient step. The high cost of burning in the
Markov chains in the inner loop makes this procedure computationally
infeasible, but this procedure is the starting point that other more
practical algorithms aim to approximate.

This approach can viewed as trying to achieve balance between two
forces, one pushing up on the model distribution where the data occurs,
and another pushing down on the model distribution where the model
samples occur. The two forces correspond to maximizing $\log \Tilde\{p\}$
and minimizing $\log Z$. Several approximations to the negative phase
are possible. Each of these approximations can be understood as making
the negative phase computationally cheaper but also making it push down
in the wrong locations.

<figure id="cd">
<p><img
src="figures/Differentiable Models/autoencoders/contrastive divergence1.png"
alt="image" /> <span id="cd" data-label="cd"></span></p>
<figcaption>Illustration of contrastive divergence.</figcaption>
</figure>

The contrastive divergence (CD, or CD-k to indicate CD with k Gibbs
steps) algorithm initializes the Markov chain at each step with samples
from the data distribution. Obtaining samples from the data distribution
is free, because they are already available in the data set. Initially,
the data distribution is not close to the model distribution, so the
negative phase is not very accurate. Fortunately, the positive phase can
still accurately increase the models probability of the data. After the
phase has had some time to act, the model distribution is closer to the
data distribution, and the negative phase starts to become accurate.

The pseudo-code of the contrastive divergence algorithm is as follows:

1.  Set the visible units to a training vector

2.  While not converged:

    1.  Update all hidden units in parallel using Eq.
        [\[hidden\]](#hidden)\{reference-type="ref" reference="hidden"\}

    2.  Update all visible units in parallel to get "reconstructions"
        using Eq. [\[visible\]](#visible)\{reference-type="ref"
        reference="visible"\}

    3.  Update all hidden units in parallel again using Eq.
        [\[hidden\]](#hidden)\{reference-type="ref" reference="hidden"\}

    4.  Update the coefficients of the energy model using Eqs.
        [\[weights1\]](#weights1)\{reference-type="ref"
        reference="weights1"\}-[\[weights3\]](#weights3)\{reference-type="ref"
        reference="weights3"\}

    5.  Select another training vector

We can convert our description into Physika code

``` \{.python language="python"\}
# Set $\epsilon$, the step size, to a small positive number
# Set $k$, the number of Gibbs steps high enough to allow a 
# Markov chain sampling from $p(x;\theta)$ to mix when 
# initialized from $p_\{data\}$ (about 1 to 20 to train an RBM
# on a small image patch).     
class BoltzmannMachine:
  def contrastive_divergence(X: $\mathbb\{R\}^\{N\times d\}$,  $\epsilon: \mathbb\{R\}$, $k: \mathbb\{N\}$):
    while True:      # While not converged:
      # Sample a minibatch of m examples 
      # $\{x^\{(1)\},..,x^\{(m)\}\}$ from the training set
      $\{x^\{(1)\},..,x^\{(m)\}\}$ = Sample(X)
      $g = \frac\{1\}\{m\}\sum_\{i=1\}^m \nabla_\theta log\ \Tilde\{p\}(x^\{(i)\};\theta)$
      for i = 1 to m:
        $\Tilde\{x\}^\{(i)\}=x^\{(i)\}$
      for i = 1 to k 
        $\Tilde\{h\}^\{(i)\}=\textrm\{gibbs_update\}(\Tilde\{h\}^\{(i)\}, \Tilde\{x\})$
      for j = 1 to m 
        $\Tilde\{x\}^\{(j)\}=\textrm\{gibbs_update\}(\Tilde\{x\}^\{(j)\}, \Tilde\{h\})$
      for i = 1 to k 
        $\Tilde\{h\}^\{(i)\}=\textrm\{gibbs_update\}(\Tilde\{h\}^\{(i)\}, \Tilde\{x\})$
      $g = g - \frac\{1\}\{m\}\sum_\{i=1\}^m \nabla_\theta log\ \Tilde\{p\}(\Tilde\{x\}^\{(i)\};\theta)$
      $\theta = \theta + \epsilon g$
```

Of course, CD is still an approximation to the correct negative phase.
The main way that CD qualitatively fails to implement the correct
negative phase is that it fails to suppress regions of high probability
that are far from actual training examples. These regions that have high
probability under the model but low probability under the data
generating distribution are called spurious modes. Essentially, it is
because modes in the model distribution that are far from the data
distribution will not be visited by Markov chains initialized at
training points, unless k is very large. The CD estimator is biased for
RBMs and fully visible Boltzmann machines, in that it converges to
different points than the maximum likelihood estimator. Since the bias
is small, CD could be used as an inexpensive way to initialize a model
that could later be fine-tuned via more expensive MCMC methods.

## Performing Classification

To use a Restricted Boltzmann machine to solve classification tasks, the
visible vector is set to a concatenation of the feature vector and the
label $$\begin\{aligned\}
v &= x_i | y_i
\end\{aligned\}$$ For test data, the visible vector is partially
initialized with just the feature vector and the label is sampled from
the distribution.


# Autoencoders \{#chap:autoencoders\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Often in scientific machine learning applications, we lack any labeled
training data. We may have a collection of molecular structures or
materials formulas, but may not have access to any associated
experimental labels for these data points. Can we still do machine
learning?

## Autoencoder Reconstruction

Suppose the input $x$ is in space $\mathcal\{S\}$. An encoder is a
function $f_\{\theta\}: \mathcal\{S\} \to \mathbb\{R\}^n$ that transforms
elements of $\mathcal\{S\}$ into $n$ dimensional vectors parameterized by
$\theta$. Let $g_\{\theta\}: \mathbb\{R\}^n \to \mathcal\{S\}$ be a map in the
opposite direction. An autoencoder posits that the encoding and decoding
an input can be a meaningful tool to force learning of the structure of
a dataset; the challenge of representation learning is picking $\theta$
such that $$\begin\{aligned\}
    \theta^* = \argmin_\{\theta\} \|g_\{\theta\}(f_\{\theta\}(s)) - s \| 
\end\{aligned\}$$

One way to think about $g_\{\theta\} \circ f_\{\theta\}$ is as a learned
version of the identity. There are a couple of reasons learning the
identity may be useful. Suppose that $$\begin\{aligned\}
\mathcal\{S\} = \mathbb\{R\}^M,\quad M \gg n
\end\{aligned\}$$ In this case, $g_\{\theta\} \circ f_\{\theta\}$ is a
compressed version of the identity that mandates an internal compression
to $n$-dimensional space.

![Autoencoders encode the input into a suitable embedding space and
decode the embedded vector into the output space. This encoding and
decoding process can help networks learn about the meaningful structure
of the
data.](figures/Differentiable Models/autoencoders/Autoencoder.png)\{#fig:auto_compression\}

As a slightly more complex example, we may have
$\mathcal\{S\} = \mathbb\{R\}^\{a \times b \times c\}$ be a space of images
with $a \times b$ pixels and $c$ colors. In this case, the encoder
$f_\{\theta\}$ transforms these images into $n$-dimensional feature
vectors.

We can choose different architectures to encode and decode, but a common
simple choice is to just use fully connected networks.

``` \{.python language="python"\}

class Autoencoder(f: FullyConnectedNetwork, g: FullyConnectedNetwork):
  def $\lambda$(X: $\mathbb\{R\}$[N, T, d])) $\to$ $\mathbb\{R\}$[N, T, d]): 
    g(f(X))
    
def reconstruction_loss(model, X):
  return $\|$model(X) - X$\|^2$
```

## Variational Autoencoder

A variational autencoder can be viewed as a classical autoencoder with
the addition of a regularization term to the loss. Here we train the
encoder $f$ to produce the means and standard deviations of the encoded
distribution. $$\begin\{aligned\}
    \mu_x, \sigma_x &= f(x)
\end\{aligned\}$$

Then during decoding, we sample from this distribution $$\begin\{aligned\}
    z \sim \mathcal\{N\}(\mu_x, \sigma_x)
\end\{aligned\}$$ And then reconstruct the output. $$\begin\{aligned\}
    \mathrm\{out\} &= g(z)
\end\{aligned\}$$

Converting into Physika, we obtain

``` \{.python language="python"\}
class VariationalAutoencoder(f: FullyConnectedNetwork, g: FullyConnectedNetwork):
  def $\lambda$(X):
    $\mu_x$, $\sigma_x$ = f(X)
    z = sample($\mathcal\{N\}$($\mu_x$, $\sigma_x$))
    g(z)
```

Note that the variational autoencoder provides a natural mechanism to
draw samples from a learned distribution by sampling random $z$ and then
decoding accoding to $g$. The latent distribution is controlled by
adding a KL-divergence.

$$\begin\{aligned\}
    \mathcal\{L\} &= \|x - f(\textrm\{sample\}(g(x)))\|^2 + \mathrm\{KL\}(\mathcal\{N\}(\mu_x, \sigma_x), \mathcal\{N\}(0, I))
\end\{aligned\}$$

The gradient descent step here is more complex since we have to do a
backwards update over a sampling step. Empirically, we can compute this
quantity in expectation, but proofs of correctness are quite tricky for
these probabilistic losses.
