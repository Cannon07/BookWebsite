# Quantum Statistical Mechanics \{#chap:quantum_statistical_mechanics\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:schrodinger\]](#chap:schrodinger)\{reference-type="ref+label"
reference="chap:schrodinger"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

The core mathematical object of statistical mechanics is the partition
function. To introduce a theory of quantum statistical mechanics, we
will consequently find it useful to introduce the following quantum
partition function $$\begin\{aligned\}
    Z_Q &= \Tr e^\{-\beta H\} &= \Tr U(\tau &= \beta \hbar)
\end\{aligned\}$$ Here $H$ is the usual Hamiltonian and $U$ is the
imaginary time evolution operator $$\begin\{aligned\}
    U(\tau) &= e^\{-\frac\{\tau\}\{\hbar\}H\}
\end\{aligned\}$$


# Molecular Dynamics \{#chap:molecular_dynamics\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:newtons_laws\]](#chap:newtons_laws)\{reference-type="ref+label"
reference="chap:newtons_laws"\},
[\[chap:electrostatics\]](#chap:electrostatics)\{reference-type="ref+label"
reference="chap:electrostatics"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Molecular dynamics methods numerically solve Newton's equations for a
system of interacting particles. Molecular dynamics methods offer the
powerful advantage of efficiency compared with quantum mechanical
simulation methods, but at the cost of modeling accuracy.

## The Basics of Molecular Dynamics

At its core, molecular dynamics is roughly a variant of Euler's method
for solving systems of ODEs. The gradient of the energy (the force) is
used to update the positions of all atoms in the system repeatedly. This
basic formula is modified to enforce boundary conditions, and
temperature and pressure controls.

![Conceptual pseudocode of molecular dynamics simulation
algorithm](figures/part2b/molecular_dynamics/MD_algorithm.png)\{#fig:my_label\}

We can define the basic molecular dynamics algorithm in Physika as
follows

``` \{.python language="python"\}
class MolecularDynamics($r: \mathbb\{R\}^\{3N\}$, $v: \mathbb\{R\}^\{3N\}$, $\Delta t: \mathbb\{R\}$):

  def $\lambda$(N : $\mathbb\{N\}$): $(\mathbb\{R\}^\{3N\}, \mathbb\{R\}^\{3N\})$:
    for _ in range(N):
      $r$ = $r$ + $v \Delta t$ + $\frac\{1\}\{2\} \alpha \Delta t^2$
      $F$ = $-\nabla$ ForceField($r$)
      $r$ = $r$ + ForceUpdate(F, $\Delta t$)
      $v$ = $v$ + VelocityUpdate(F, $\Delta t$)
    return (r, v)
```

## Cleanup

One of the most important practical steps in molecular dynamics is
cleaning up input protein structure files. Common cleanup steps include
fixing protonation, creating disulfide bonds, and placing side-chains
correctly.

## Force Fields

Force fields are the technical term for the classical potential
functions that are used in molecular dynamics systems to model
interesting physics. Force fields typically correspond to classical
energies are are made up of constituent energy terms. A common such term
is the Lennard-Jones potential

$$\begin\{aligned\}
    U(r) &= 4 \epsilon \left [ \left ( \frac\{\sigma\}\{r\} \right )^\{12\} + \left( \frac\{\sigma\}\{r\} \right )^6 \right ]
\end\{aligned\}$$

The Lennard-Jones potential provides an approximate model for Van der
Waals interactions

Force fields are typically parameterized by a collection of tunable
parameters. Traditionally, force field parameters were manually tuned
but as we will learn in later chapters, force fields are increasingly
learned using the techniques of machine learning and differentiable
programming.

## Neighbor Lists

One of the most important steps in a molecular dynamics update operation
is computing the neighbors of a given atom which contribute to force
estimates. The set of current neighbors of an atom is maintained by a
data structure known as a neighbor list. Any molecular dynamics code
must implement a neighbor list.

## Ewald Summation

The most inefficient step in the computation of a force field is
computing pairwise interaction terms which requires $O(n^2)$
computations. Ewald summation provides a method for reducing the
computational cost of the force computation to $O(n \log n)$.

## Stability of Integration

One of the challenges of molecular dynamics, especially with learned
force fields, is stability of integration. Integration can cause a
system to become unstable and \"blow apart\" in simulation if the force
field isn't well calibrated. Heuristically designed force fields are
known to be well calibrated in practice, but learned force fields are
still unstable. There are some signs that this instability may be due to
the numerical instability of using gradient information across very long
timescales.


# Partition Functions for Non-Interacting Systems \{#chap:noninteracting\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:stat_thermo\]](#chap:stat_thermo)\{reference-type="ref+label"
reference="chap:stat_thermo"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

From the principles of statistical mechanics we determine the partition
functions for different ensembles, such as $$\begin\{aligned\}
Z &=\sum_\nu \exp(-E_\nu/k_BT)\\
\Xi &= \sum_\nu \exp[-(E_\nu -\mu N_\nu)/k_BT]
\end\{aligned\}$$ for the canonical and grand canonical ensemble,
respectively. These are the Boltzmann weighted sums over all possible
fluctuations, that is, all microscopic states permitted by the
constraints with which the system is controlled. In the first sum,
energy fluctuates and only the states with same number number of
particles are allowed; in the second, particles and energy fluctuates,
and the chemical potential term accounts for the energy of changing
particle number.

Generally speaking, in order to determine the partition functions, a
systematic study of all possible fluctuations must be carried out,
together with an adequate characterization of these states, which is
usually a complicated task. However, there are a number of practical
methods for sampling relevant fluctuations. The simplest of these
approximation is when the system is composed of non-interacting degrees
of freedom.

In this chapter we will study systems of non-interacting particles.
Later in this chapter, we will use our derivations for the analysis of a
two-state, non-interacting particle system.

## Non-interacting particles

A system of non-interacting particles can be regarded as a system of
*distinguishable particles*, meaning that they can be uniquely
identified according to some criterion (*e.g.* fixed spatial position of
particles on a surface or in space); or *indistinguishable particles*
where particles cannot be distinguished from each other (*e.g.*
particles in the gas phase cannot be distinguished from each other).

## Distinguishable particles

Consider a system of 2 distinguishable particles. The two particles
contain energy levels $\epsilon_i^A$ (with $i=1,2,...,a$) and
$\epsilon_i^B$ (with $i=1,2,...,b$). In the canonical ensemble, the
partition function $Z$ is given by: $$\begin\{aligned\}
Z &= \sum_\mu \exp \left(-\frac\{E_\mu\}\{k_BT\}\right)\notag\\ 
&= \sum_\{i=1\}^a \sum_\{j=1\}^b \exp \left[-\frac\{\varepsilon_i^A+\varepsilon_j^B\}\{k_BT\}\right] \notag\\
 &= \left[\sum_\{i=1\}^a \exp \left(-\frac\{\varepsilon_i^A\}\{k_BT\}\right)\right] \left[\sum_\{j=1\}^b \exp \left(-\frac\{\varepsilon_j^B\}\{k_BT\}\right)\right] \notag\\
 &= z_Az_B \notag
\end\{aligned\}$$

where $z_A$ and $z_B$ are the single-particle partition functions for
the two particles.

Generally, for a system with N non-interacting, distinguishable
particles, each with a single-particle partition function $z$, the
partition function is given by: $$\begin\{aligned\}
 \label\{part_dist\}
Z = z^N
\end\{aligned\}$$

## Indistinguishable particles

For a system of $N$ non-interacting, indistinguishable particles, the
partition function summation contains instances where particles exist in
different states but cannot be distinguished. On one extreme, if all
particles exist in different states, the over-counting factor is $N!$
(number of ways of reassigning $N$ particle labels). On the opposite
extreme, if all particles are in the same state, the
indistinguishability and the degeneracy are the same, so there is no
over-counting factor.

At finite temperature, the particles are more likely to exist in a
diversity of states that are over-counted by the factor $N!$. Thus to
correct for the indistinguishability at finite temperature, the
partition function is given by: $$\begin\{aligned\}
 \label\{part_indist\}
Z = \frac\{1\}\{N!\} z^N
\end\{aligned\}$$

## Two-state, non-interacting particles

Consider a collection of non-interacting, distinguishable particles that
can exist in two states: the ground state (with zero energy) and the
excited state (with energy $\varepsilon_0$). We will analyze this system
using two of the ensembles derived in the previously to show
equivalence.

## Microcanonical Ensemble

For a system of N particles, we consider a state with a fixed energy
$E = \varepsilon_0 n$, where $n$ is the number of particles in the
excited state. The degeneracy in the number of states with this fixed
energy $E$ is given by: $$\begin\{aligned\}
\Omega = \frac\{N!\}\{n!\left(N-n\right)!\} \notag
\end\{aligned\}$$

which is the number of permutations of $n$ in $N$ ($N$ choose $n$).

Therefore, the entropy of the system is given by: $$\begin\{aligned\}
S = k_B \log \left[\frac\{N!\}\{n!(N-n)!\}\right] \notag
\end\{aligned\}$$

To approximate the entropy for large number of particles $N$
(*thermodynamic limit*), we use *Stirling's approximation* [^1]

The entropy is now written as: $$\begin\{aligned\}
S &= k_B\log \left[\frac\{N!\}\{n!\left(N-n\right)!\}\right] \notag\\
&\approx k_B \left[N\log N -N -n\log n+n-(N-n)\log (N-n) + (N-n)\right] \notag\\
&= -k_B N [(1-x) \log (1-x) + x \log x] \notag
\end\{aligned\}$$

where $x=n/N = E/\left(\varepsilon_0 N\right)$ is the fraction in the
excited state.

Using thermodynamic relations, the temperature is found to be:
$$\begin\{aligned\}
\frac\{1\}\{T\} &= \left(\frac\{\partial S\}\{\partial E\}\right)_\{V,N\} = \left(\frac\{\partial x\}\{\partial E\}\right)_\{V,N\} \left(\frac\{\partial S\}\{\partial x\}\right)_\{V,N\} \notag\\ 
&= \frac\{k_B N\}\{\varepsilon_0 N\} [\log (1-x) - \log x] \notag
\end\{aligned\}$$

Thus: $$\begin\{aligned\}
\frac\{x\}\{1-x\} &= \exp \left(-\frac\{\varepsilon_0\}\{k_B T\}\right) = \alpha
 \hspace\{0.2cm\} \text\{or\} \hspace\{0.2cm\} x = \frac\{\alpha\}\{1+\alpha\} \notag
\end\{aligned\}$$

The Helmholtz free energy per particle $f = F/N$ is given by:
$$\begin\{aligned\}
f &= e-Ts \notag\\ 
&=\varepsilon_0x+k_BT\left[\frac\{1\}\{1+\alpha\}\log \left(\frac\{1\}\{1+\alpha\}\right) + \frac\{\alpha\}\{1+\alpha\}\log \left( \frac\{\alpha\}\{1+\alpha\}\right)\right] \notag\\
&= -k_BT\frac\{\alpha\}\{1+\alpha\} \log \alpha + k_BT\log (1+\alpha) +k_BT \frac\{\alpha\}\{1+\alpha\} \log \alpha \notag\\
&= -k_BT \log \left[ 1+\exp \left(-\frac\{\varepsilon_0\}\{k_BT\}\right)\right] \notag \\
&= -k_B T \log z\label\{Helmholtz_Cano\}
\end\{aligned\}$$

::: marginfigure
![image](figures/Physics/statistical_mechanics/non_interacting_systems/Fig1b.png)\{width="\\linewidth"\}
[]\{#fig1a label="fig1a"\}
:::

::: marginfigure
![image](figures/Physics/statistical_mechanics/non_interacting_systems/Fig1a.png)\{width="\\linewidth"\}
:::

where we define $z = 1+\exp \left(-\frac\{\varepsilon_0\}\{k_BT\}\right)$,
which will become the single-particle canonical partition function.

## Canonical ensemble

The canonical partition has a fixed temperature $T$ and number of
particles $N$, thus the energy states fluctuate according to the
Boltzmann distribution found in the previously. For this system, the
partition function $Z$ is written as: $$\begin\{aligned\}
Z &= \sum_\{m_1\}\sum_\{m_2\}...\sum_\{m_N\} \exp \left(-\frac\{\varepsilon_0 m_1\}\{k_BT\}-\frac\{\varepsilon_0 m_2\}\{k_BT\} ... -\frac\{\varepsilon_0 m_N\}\{k_BT\}\right) \notag\\
&= \left[\sum_\{m=0\}^1\exp \left(-\frac\{\varepsilon_0 m\}\{k_BT\}\right)\right]^N \notag\\ 
&= \left[ 1+\exp \left(-\frac\{\varepsilon_0\}\{k_BT\}\right)\right]^N = z^N \notag
\end\{aligned\}$$

where we again define the single-particle partition function $z$.

An alternative method to derive the partition function is to extend the
microcanonical solution to the canonical solution, such that:
$$\begin\{aligned\}
Z &= \sum_E \Omega (E) \exp \left(-\frac\{E\}\{k_BT\}\right) \\ 
&= \sum_\{n=0\}^N \frac\{N!\}\{n!(N-n)!\}\exp \left(-\frac\{\varepsilon_0\}\{k_BT\}\right)^n 1^\{N-n\} \\
&= \left[ 1 + \exp \left( -\frac\{\varepsilon_0\}\{k_BT\} \right)\right]^N = z^N 
\end\{aligned\}$$

which makes use of the binomial expansion.

The Helmholtz free energy is found to be: $$\begin\{aligned\}
F &= -k_BT\log Z = -Nk_BT \log z \\ 
&= -N k_B T \log \left[ 1+ \exp \left(-\frac\{\varepsilon_0\}\{k_BT\}\right)\right] \\
&= -Nk_BT \log z 
\end\{aligned\}$$

which is in agreement with
([\[Helmholtz_Cano\]](#Helmholtz_Cano)\{reference-type="ref"
reference="Helmholtz_Cano"\}) from the microcanonical approach.

Using $Z$ as a generating function, the average energy is given by:
$$\begin\{aligned\}
e &= \frac\{\langle E \rangle\}\{N\} =-\frac\{1\}\{N\} \left(\frac\{\partial \log Z\}\{\partial \beta\}\right)_\{V,N\} \\ 
&= - \left(\frac\{\partial \log z\}\{\partial \beta\}\right)_\{V,N\} \\
&= \frac\{\varepsilon_0 e^\{-\beta \varepsilon_0\}\}\{1+e^\{-\beta \varepsilon_0\}\} 
\end\{aligned\}$$

where $\beta = 1/(k_BT)$.

## Non-interacting particles obeying Bose and Fermi statistics

Consider a system of non-interacting, indistinguishable particles that
can have energies $\varepsilon_\alpha$ $(\alpha = 1,2,...)$ associated
with their quantum mechanical states. The state of the system can be
specified by the number of particles at each energy level, *i.e.*
$n_\alpha$ is the number of particles at energy state $\alpha$.

-   Total number of particles is $\sum_\alpha n_\alpha=N$.

-   Total system energy is
    $\sum_\alpha n_\alpha \varepsilon_\alpha = E$.

In the canonical ensemble, we write the partition function:
$$\begin\{aligned\}
\label\{Eq1\}
Z(T,V,N) = \sum_\{n_1,n_2,...\}\delta \left(N-\sum_\alpha n_\alpha\right) \exp \left(-\beta \sum_\alpha n_\alpha \varepsilon_\alpha\right)
\end\{aligned\}$$

where we include the delta-function constraint on the summation in order
to fix $\sum_\alpha n_\alpha=N$, and we define $\beta = 1/(k_BT)$.

The indistinguishability of the particles is properly accounted for in
this representation since any given set of $n_\alpha$ contributes a
single term without over-counting indistinguishable states. In the grand
canonical ensemble, we write the grand canonical partition function:
$$\begin\{aligned\}
\Xi(T,V,\mu) &=\sum_\{N=0\}^\infty Z(T,V,N) \exp(\beta \mu N) \notag\\
  &= \sum_\{N=0\}^\infty \sum_\{n_1,n_2,...\} \delta \left(N-\sum_\alpha n_\alpha\right) \exp \left(-\beta \sum_\alpha n_\alpha \varepsilon_\alpha -\beta \mu N\right) \notag\\
  &= \sum_\{n_1,n_2,...\} e^\{-\beta \sum_\alpha n_\alpha \varepsilon_\alpha +\beta \mu \sum_\alpha n_\alpha\} = \prod_\alpha \left(\sum_\{n_\alpha\} e^\{-\beta n_\alpha \varepsilon_\alpha -\beta \mu n_\alpha\} \right) \notag
\end\{aligned\}$$

The Landau potential is written as: $$\begin\{aligned\}
\label\{Eq2\}
pV &= k_BT \log \Xi \\
&= k_BT\sum_\alpha \log \left(\sum_\{n_\alpha\} e^\{-\beta n_\alpha \varepsilon_\alpha -\beta \mu n_\alpha\} \right)
\end\{aligned\}$$

If the particles are bosons, there are no restrictions on the number of
particles in a given state $n_\alpha$ $(\alpha = 1,2,...)$, thus:
$$\begin\{aligned\}
\label\{Eq3\}
\sum_\{n_\alpha = 0\}^\infty \left(e^\{-\beta \varepsilon_\alpha + \beta \mu \}\right)^\{n_\alpha\} = \frac\{1\}\{1 - e^\{-\beta \varepsilon_\alpha + \beta \mu \}\}
\end\{aligned\}$$

If the particles are fermions, any given state can only have either
$n_\alpha = 0$ or $n_\alpha = 1$ particles, thus: $$\begin\{aligned\}
\label\{Eq4\}
\sum_\{n_\alpha = 0\}^1 \left(e^\{-\beta \varepsilon_\alpha + \beta \mu \}\right)^\{n_\alpha\} = 1 + e^\{-\beta \varepsilon_\alpha + \beta \mu \}
\end\{aligned\}$$

Therefore, we generally write: $$\begin\{aligned\}
\label\{Eq5\}
pV = \mp k_BT\sum_\alpha \log \left(1 \mp e^\{-\beta \varepsilon_\alpha + \beta \mu\} \right)
\end\{aligned\}$$

where \"-\" is for Bosons, and \"+\" is for Fermions.

From this, we find the average number of particles: $$\begin\{aligned\}
\label\{Eq6\}
\langle N \rangle = \left(\frac\{\partial \log \Xi\}\{\partial \beta
\mu\}\right)_\{\beta\} = \sum_\alpha \frac\{e^\{-\beta \varepsilon_\alpha + \beta \mu\}\}\{1 \mp e^\{-\beta \varepsilon_\alpha + \beta \mu\}\} = \sum_\alpha \langle n_\alpha \rangle
\end\{aligned\}$$

where $\langle n_\alpha \rangle$ is the average occupation number in the
$\alpha$ state.

[^1]: For large $N$: $$\begin\{aligned\}
    \log N! &= \sum\limits_\{n=1\}^N \log n \\
    & \approx \int\limits_1^N \log n dn \\ 
    &= N \log N - N +1 \\
    & \approx N \log N -N
    \end\{aligned\}$$


# Introduction to Statistical Thermodynamics \{#chap:stat_thermo\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:newtons_laws\]](#chap:newtons_laws)\{reference-type="ref+label"
reference="chap:newtons_laws"\},
[\[chap:electrostatics\]](#chap:electrostatics)\{reference-type="ref+label"
reference="chap:electrostatics"\},
[\[chap:classical_thermo\]](#chap:classical_thermo)\{reference-type="ref+label"
reference="chap:classical_thermo"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Classical mechanics, electromagnetism, and quantum mechanics provide
powerful tools to model simple systems with a limited number of
microscopic or macroscopic entities. How can we model more complex
macroscale systems with a very large number of possibly interacting
objects? In this chapter we introduce the foundations of statistical
thermodynamics which seeks to understand how the macroscopic behavior of
systems emerges from the microscopic.

We refer to a state determined from either quantum or classical
mechanics as a *microstate*, to distinguish it from a *state* or
*macrostate*, which is a collection of microstates. The macroscopic
state of the system is determined by the probability distribution of the
microstates, that are accessible to a system through its thermal
fluctuations. Our goal then is to find the respective probability of
each microscopic state to the constraints imposed from the perspective
of different ensembles. Once the distribution is found, it is possible
to determine the statistical information of a macrostate. As we will
learn in the rest of this chapter, even when the form of the probability
for each microscopic state might change according to its local ensemble,
the *expectation value*, which defines a macroscopic observable, remains
intact.

## Basic Quantities

An ensemble in statistical mechanics is a probability distribution over
a set of microstates such that every such microstate satisfies relations
defined in terms of the basic quantities.
Figure [1.1](#stat_thermo:ensembles)\{reference-type="ref"
reference="stat_thermo:ensembles"\} illustrates some ensembles.

![Illustrations of statistical ensembles.
](figures/Physics/statistical_mechanics/statistical_thermodynamics/Statistical ensembles.png)\{#stat_thermo:ensembles
width="\\linewidth"\}

The probability distribution of the system is defined over the phase
space

$$\begin\{aligned\}
&p_1,\dotsc,p_n \in \mathbb\{R\}^3\\
&q_1,\dotsc,q_n \in \mathbb\{R\}^3
\end\{aligned\}$$ This definition can be directly translated to Physika.

    ps : $\mathbb\{R\}^\{3\}[n]$
    qs : $\mathbb\{R\}^\{3\}[n]$
    ClassicalPhaseSpace[n] = ($\mathbb\{R\}^3[n]$, $\mathbb\{R\}^3[n]$)

It's worth noting that the definitions we've given thus far assume that
the phase space of the system has a fixed number of particles $n$. At
times, we will want to model open systems that can have a variable
number of particles. An alternative definition of the phase space

    OpenClassicalPhaseSpace[s] = ($n_1$: $\mathbb\{N\}$,$\dotsc$,$n_s$: $\mathbb\{N\}$, ClassicalPhaseSpace[$\sum_\{i\} n_i$] )

Here there are $s$ kinds of particles and the number of particles of
each type is encoded by $n_i$. The type of the phase space is
parameterized by the total number of particles $$\begin\{aligned\}
n = n_1+\dotsc+n_s
\end\{aligned\}$$ Note that `OpenClassicalPhaseSpace` is an example of a
dependent type.

Note that `ClassicalPhaseSpace` and `OpenClassicalPhaseSpace` are very
large uncountable spaces. As a result, phase space is often defined into
tiny cubes $$\begin\{aligned\}
\delta p_1 \times \dotsc \delta p_n \times \delta q_1 \times \dotsc \delta q_n
\end\{aligned\}$$ We can encode this as a type

    Microstate[$\delta$,$n$] = Interval[$\delta$, $\mathbb\{R\}$]$^n$
    Ensemble[$\delta$, $n$] = ProbabilityDistribution[Microstate[$\delta$, $n$]]

This discretization process was viewed classically as a mathematical
hack, but turns out to have deeper meaning in quantum mechanics. Due to
the Heisenberg's uncertainly principle, $\delta$ can be no smaller than
the Planck constant $\hbar$.

Statistical mechanical systems are typically parameterized by some
macroscopic variables defined on phase space: internal energy $U$,
volume $V$, and number of particles $N$, and temperature $T$. We can
define some types for these three quantities.

    System = ClassicalPhaseSpace
    U, V, T: System $\to$ $\mathbb\{R\}$, 

Statistical mechanics studies a series of relationships between these
functions defined on phase space. There are a set of known relationships
that govern interactions between common quantities like internal energy,
temperature, volume, and number of particles. We say that these values
all lie on a common surface defined by $$\begin\{aligned\}
    f(U, V, N, T) &= 0
\end\{aligned\}$$

The Boltzmann entropy is defined as $$\begin\{aligned\}
    S &= k_B \log \Omega
\end\{aligned\}$$ where $\Omega$ is the number of microstates
corresponding to a given macrostate. We can parameterize $S$ as a
function of other macroscopic variables $$\begin\{aligned\}
S(U, V, N, T)
\end\{aligned\}$$ The joint surface that all these macroscopic quantities
lies on satisfies a rich set of relationships.

An important aspect of thermodynamics is that many macroscopic
quantities can be expressed in terms of each other. For example, we can
reparameterize the internal energy in terms of the entropy of
$U(S, V, N, T)$. Many results are derived by shifting the choice of
reparameterization.

Another fundamental quantity that we will study is the partition
function.

    Z: PhaseSpace $\to$ $\mathbb\{R\}$

The partition function is arguably the most fundamental quantity in
statistical mechanics. As we will learn in future chapters, all other
quantities can be derived from the partition function by various
formulas.

As a final note, we mention that the physics literature on
thermodynamics follows a particular notation. When we write

$$\begin\{aligned\}
\left ( \frac\{\partial f\}\{\partial V\} \right )_T
\end\{aligned\}$$ We mean the derivative of function $f$ with respect to
$V$ with $T$ held constant. This type of notation is useful for
specifying quantities in thermodynamics since we often express
quantities in terms of multiple variables and it may not be clear which
variables may vary and which must be held constant.

### Extensive and Intensive Variables

Macroscopic properties in thermodynamics are classified as either
extensive or intensive. Extensive quantities depend on $N$, the number
of atoms in the system, while intensive quantities don't depend on $N$.
Quantities such as mass, volume, and entropy are extensive, while
temperature is intensive.

There is a conjugate relationship between extensive and intensive
quantities where an extensive change is associated with an associated
intensive quantity change. For example, a change in entropy (extensive)
is tied to a change in temperature (intensive).


# Introduction to Classical Thermodynamics \{#chap:classical_thermo\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:simple_physics\]](#chap:simple_physics)\{reference-type="ref+label"
reference="chap:simple_physics"\},
[\[chap:newtons_laws\]](#chap:newtons_laws)\{reference-type="ref+label"
reference="chap:newtons_laws"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In macroscopic systems, particles constantly change between any of the
available energy states predicted by classical or quantum mechanics.
Therefore, *ignorance is a law of nature* for many particle systems, and
this ignorance requires a statistical description of observations and
the admittance of ever-present fluctuations. The presence of such
microscopic fluctuations, however, does not prevent a system from
reaching macroscopic equilibrium. Thus, before we venture into the
subject of characterizing fluctuations, it is necessary to understand
the concept of *equilibrium*, which is the subject of classical
thermodynamics.

In the first part of this chapter we will review the concept of
thermodynamic equilibrium under the view of classical thermodynamics.
Using the first and second law of thermodynamics we will derive the
relations that defines a state of macroscopic equilibrium.

## Heat, Work, and Entropy

We will denote heat by $Q$, work by $W$ and entropy with $S$. As we will
see in the remainder of this part, understanding the relationships
between these quantities will be crucial to deepen our understanding of
thermodynamics.

## First Law of Thermodynamics

::: marginfigure
![image](figures/Physics/statistical_mechanics/classical_thermodynamics/Internal energies.png)\{width="103%"\}
:::

The first law of thermodynamics, also known as the conservation of
energy principle, provides the basis for studying the relationships
among the various forms of energy and energy interactions. The law
states that *although energy assumes many forms, the total quantity of
energy is constant, and when energy disappears in one form it appears
simultaneously in other forms*.

The energy of a system is measured by its internal energy $U$, and is
defined as the net sum of the kinetic and potential energies of all the
molecules in a macroscopic material. The internal energy is an extensive
property, meaning that the internal energy depends linearly on the size
of the system (Fig.[\[fig:Fig1\]](#fig:Fig1)\{reference-type="ref"
reference="fig:Fig1"\}).

According to the First Law of Thermodynamics any change in the internal
energy occurs due to the addition of heat $Q$ and work $W$, according to
the relation:

$$\label\{First Law\}
dU = \delta Q + \delta W$$

The difference in the notation of the change of internal energy $(dU)$
from the addition of heat $(\delta Q)$ or work $(\delta W)$, lies in the
fact that internal energy is a state variable, while heat and work are
path dependent. However, once heat and work are applied, they are
indistinguishable on a macroscale.

## Second Law of Thermodynamics

The origins of the second law lies in a statement that it is impossible
for any self-acting process or machine to produce net flow of heat from
a region of low temperature. The law states that an isolated system
always proceeds from an ordered to a more disordered state, or else it
undergoes no change at all.

The level of order from a system is measured by the entropy $S$, and
according to the second law *the entropy of the universe tends to a
maximum* (Rudolf Claussius, 1865). This is stated mathematically as:

$$\label\{Second Law\}
\{\left( dS \right)\}_\{U,V,N\} \ge 0$$

where the subscripts $U$, $V$ and $N$ indicates constant internal
energy, volume and number of particles, respectively.

## Thermodynamics Driving Forces

A thermodynamic system can be defined according to their *extensive*
variables, *i.e.*, variables that vary linearly with the system size.
For example, we define the internal energy $U$ to be a function of
extensive variables $S$, $V$, and $N = (N_1,N_2,...,N_r)$, where $r$ is
the number of chemical species in the system. Therefore, we have
$U = U(S,V,N)$. Differential changes in the extensive variables leads to
a differential change in the internal energy, such that:
$$\label\{Internal Energy Change\}
dU =\{ \left(\frac\{\partial U\}\{\partial S\}\right) \}_\{V,N_1,...,N_r\} \hspace\{-1cm\} dS 
\hspace\{0.6cm\} + 
\{ \left(\frac\{\partial U\}\{\partial V\}\right) \}_\{S,N_1,...,N_r\} \hspace\{-1cm\} dV
\hspace\{0.6cm\} + 
\{ \left(\frac\{\partial U\}\{\partial N_i\}\right) \}_\{S,V,N_\{j\neq i\}\} \hspace\{-1cm\} dN_i$$

The partial derivatives are *intensive* variables, *i.e.* variables that
do not depended on the size of the system. We define these as:

$$\label\{intensive variables\}
\{ \left(\frac\{\partial U\}\{\partial S\}\right) \}_\{V,N_1,...,N_r\} \hspace\{-1cm\} \equiv T \hspace\{0.5cm\} 
\{ \left(\frac\{\partial U\}\{\partial V\}\right) \}_\{S,N_1,...,N_r\} \hspace\{-1cm\} \equiv -p\ 
\hspace\{0.5cm\} 
\{ \left(\frac\{\partial U\}\{\partial N_i\}\right) \}_\{S,V,N_\{j\neq i\}\} \hspace\{-1cm\} \equiv \mu_i$$

Where $T$ is temperature, $p$ is pressure, and $\mu_i$ is chemical
potential of species $i$. The change in internal energy can then be
written as:

$$\label\{Eq5\}
dU = TdS - pdV + \sum_\{i=1\}^\{r\} \mu_idN_i$$

Since $U$ is a state variable, we can choose any path to find a
differential change. Equation ([\[Eq5\]](#Eq5)\{reference-type="ref"
reference="Eq5"\}) states that any change in the internal energy of the
system, is the result of a quasi-static heat flux ($TdS$), mechanical
work ($- pdV$) and chemical work ($\mu_idN_i$) to the system.

Equivalently, we can define the system according to its entropy
$S=S(U,V,N)$, resulting in a differential change:

$$\label\{Internal Energy\}
dS = \frac\{1\}\{T\}dU + \frac\{p\}\{T\}dV - \sum_\{i=1\}^r\frac\{\mu_i\}\{T\}dN_i$$

This formulation is useful to discuss the second law of thermodynamics.

::: marginfigure
![image](figures/Physics/statistical_mechanics/classical_thermodynamics/Isolated systems.png)\{width="\\linewidth"\}
:::

Consider a closed, isolated system which cannot exchange heat, work, or
particles with its surroundings
(Fig. [\[fig:Fig2\]](#fig:Fig2)\{reference-type="ref"
reference="fig:Fig2"\}). We initially prepare the system such that part
of the system (subsystem 1) has parameter values $U^\{(1)\}$, $V^\{(1)\}$,
and $N^\{(1)\}$. We similarly define $U^\{(2)\}$, $V^\{(2)\}$, and $N^\{(2)\}$
for the rest of the system (subsystem 2). Making the statement that the
entropy is additive, *i.e.* $S^\{(1)\} + S^\{(2)\} = S$), we can write:

$$\begin\{aligned\}
\label\{Entropy Change1\}
dS = \frac\{1\}\{T^\{(1)\}\}dU^\{(1)\} + \frac\{p^\{(1)\}\}\{T^\{(1)\}\}dV^\{(1)\} - \sum_\{i=1\}^r\frac\{\mu_i^\{(1)\}\}\{T^\{(1)\}\}dN_i^\{(1)\} + \notag\\ \frac\{1\}\{T^\{(2)\}\}dU^\{(2)\}+ \frac\{p^\{(2)\}\}\{T^\{(2)\}\}dV^\{(2)\} - \sum_\{i=1\}^r\frac\{\mu_i^\{(2)\}\}\{T^\{(2)\}\}dN_i^\{(2)\}  \hspace\{0.3cm\} 
\end\{aligned\}$$

Since the total system is closed and isolated, we can write
$dU^\{(2)\}=-dU^\{(1)\}$, $dV^\{(2)\}=-dV^\{(1)\}$, and
$\{dN\}_i^\{(2)\}=-\{dN\}_i^\{(1)\}$.

Therefore, we can write:

$$\begin\{aligned\}
\label\{Entropy Change2\}
dS = \left(\frac\{1\}\{T^\{(1)\}\}-\frac\{1\}\{T^\{(2)\}\}\right)dU^\{(1)\} + \left(\frac\{p^\{(1)\}\}\{T^\{(1)\}\}-\frac\{p^\{(2)\}\}\{T^\{(2)\}\}\right)dV^\{(1)\} \notag\\ - \sum_\{i=1\}^r\left(\frac\{\mu_i^\{(1)\}\}\{T^\{(1)\}\}-\frac\{\mu_i^\{(2)\}\}\{T^\{(2)\}\}\right)dN_i^\{(1)\} \hspace\{2.8cm\} 
\end\{aligned\}$$

According to the second law of thermodynamics, any spontaneous process
requires $dS \geq 0$. At equilibrium, the entropy will achieve its
maximum and hence satisfy the condition $dS=0$ for arbitrary small
changes $dU^\{(1)\}$, $dV^\{(1)\}$, and $dN^\{(1)\}$:

-   Thermal equilibrium occurs when $T^\{(1)\}=T^\{(2)\}$

-   Mechanical equilibrium occurs when $p^\{(1)\}=p^\{(2)\}$

-   Chemical equilibrium (no reactions) occurs when
    $\mu_i^\{(1)\}=\mu_i^\{(2)\}$

Notice that although thermal and mechanical equilibrium implies spatial
homogeneity of $T$ and $p$, the condition of spatial homogeneity $\mu_i$
should not be interpreted as homogeneous concentration or density of
species $i$.

In the case $T^\{(1)\}=T^\{(2)\}$, $p^\{(1)\}=p^\{(2)\}$:

-   If $\mu_i^\{(1)\}>\mu_i^\{(2)\}$ the condition $dS\geq 0$ is met only if
    $dN_i^\{(1)\}<0$

-   If $\mu_i^\{(1)\}<\mu_i^\{(2)\}$ the condition $dS\geq 0$ is met only if
    $dN_i^\{(1)\}>0$

-   Matter flows from high chemical potential to low (not always high
    concentration to low).

### Helmholtz Free Energy

The Helmholtz free energy is a quantity defined by the following
equation

$$\begin\{aligned\}
F &= U-\left(\frac\{\partial U\}\{\partial S\}\right)_\{V,N\} S \\
&= U -TS
\end\{aligned\}$$

which has a differential change: $$\begin\{aligned\}
dF &= dU -TdS-SdT \\
&=-SdT-pdV + \sum_\{i=1\}^r\mu_idN_i
\end\{aligned\}$$

    F: System $\to$ $\mathbb\{R\}$

### Gibbs Free Energy

The Gibbs free energy is defined by the following quantity

$$\begin\{aligned\}
G &= U - \left(\frac\{\partial U\}\{\partial S\}\right)_\{V,N\}S - \left(\frac\{\partial U\}\{\partial V\}\right)_\{S,N\}V \\
&= U - TS + pV
\end\{aligned\}$$ which has a differential change: $$\begin\{aligned\}
dG &= dU -TdS - SdT + pdV +Vdp \\
&= -SdT +Vdp + \sum_\{i=1\}^r\mu_idN_i 
\end\{aligned\}$$

    G: System $\to$ $\mathbb\{R\}$

## Euler Equation

The internal energy is a homogeneous, first-order function, which
requires that the internal energy must increase linearly with the size
of the system. Mathematically, this is written as:
$$U(\lambda S,\lambda V, \lambda N) = \lambda U(S,V,N)$$

where $\lambda$ is an arbitrary scaling factor for the system size. If
we take the derivative of this equation with respect to $\lambda$, we
arrive at:

$$\frac\{\partial U(\lambda S,\lambda V, \lambda N)\}\{\partial \lambda\} = U(S,V,N)$$

The left-hand side is written as: $$\begin\{aligned\}
\left(\frac\{\partial U(\lambda S,\lambda V, \lambda N)\}\{\partial (\lambda S)\}\right)_\{\lambda V, \lambda N\} \frac\{\partial (\lambda S)\}\{\partial \lambda\} + \left(\frac\{\partial U(\lambda S,\lambda V, \lambda N)\}\{\partial (\lambda V)\}\right)_\{\lambda S, \lambda N\} \frac\{\partial (\lambda V)\}\{\partial \lambda\} + \notag \\
\sum_\{i=1\}^r\left(\frac\{\partial U(\lambda S,\lambda V, \lambda N)\}\{\partial (\lambda N_i)\}\right)_\{\lambda S, \lambda V, \lambda N_\{j \neq i\}\} \frac\{\partial (\lambda N_i)\}\{\partial \lambda\}  = TS - pV + \sum_\{i=1\}^r\mu_iN_i 
\end\{aligned\}$$

Therefore, the total Legendre Transform equals zero, which is the Euler
equation: $$\label\{Euler equation\}
U = TS -pV +\sum_\{i=1\}^r\mu_iN_i$$


# Ideal Gases \{#chap:ideal_gas\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:stat_thermo\]](#chap:stat_thermo)\{reference-type="ref+label"
reference="chap:stat_thermo"\},
[\[chap:noninteracting\]](#chap:noninteracting)\{reference-type="ref+label"
reference="chap:noninteracting"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter we will make use of some quantum mechanics to build a
simple model that describes the macroscopic behavior of ideal gases,
with special emphasis on ideal gases of diatomic molecules. Although the
characterizations used here might seem simple, they are highly accurate
in representing the thermal properties of the macroscopic system, and
they are also fundamental to understand the origins of some well known
relations, like the ideal gas equation of state.

## Ideal Gas of Diatomic Molecules

Consider $N$ non-interacting, diatomic molecules in a fixed volume $V$
and temperature $T$ (canonical ensemble). The molecules in the gas phase
are indistinguishable, thus the partition function is given by
$Q = \frac\{1\}\{N!\} q^N$, where $q$ is a single-molecule partition
function. We previously analyzed the quantum mechanics of a diatomic
molecule, resulting in translational, rotational, vibrational, and
electronic quantum mechanical modes
(Fig. [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/ideal_gas/quantum mechanical modes.png)\{width="\\linewidth"\}
:::

Quantum mechanics predicts the energetic contributions for each mode to
be:

  --------------------------------------------------------------------------------------- ---------------------
                                                                                          
  $\displaystyle E_\{\vec\{n\}\}^\{trans\} = \frac\{h^2\}\{8ML^2\}\left(n_x^2+n_y^2+n_z^2\right)$     Translational modes
                                                                                          
  $\displaystyle E_l^\{rot\} = \frac\{\hslash^2\}\{2\mu R^2\}l\left(l+1\right)$                      Rotational modes
                                                                                          
  $\displaystyle E_j^\{vib\} = \left(j+\frac\{1\}\{2\}\right)\hslash \nu$                           Vibrational modes
                                                                                          
  --------------------------------------------------------------------------------------- ---------------------

where $M=m_1+m_2$, $\mu=\frac\{m_1 m_2\}\{m_1 + m_2\}$,
$\nu = \sqrt\{\frac\{k\}\{\mu\}\}$, $R$ is the atomic separation, and $k$ is
the spring stiffness.

Owing to the fact that these quantum mechanical modes are decoupled, the
single-particle partition function is separable into quantum-mode
partition functions, such that: $$\begin\{aligned\}
 \label\{part_idealGas\}
q = q_\{trans\} \, \cdot q_\{rot\} \, \cdot q_\{vib\} \, \cdot q_\{elec\}
\end\{aligned\}$$

where $q_\{trans\}$ is the sum over translational modes, $q_\{tot\}$ is the
sum over rotational modes, etc. We find solutions for each of these
quantum-mode partition functions:

## Translational Partition Function

The translational partition function is written as $$\begin\{aligned\}
\sum_\{n_x=1\}^\infty\sum_\{n_y=1\}^\infty\sum_\{n_z=1\}^\infty\exp\left[-\frac\{h^2\}\{8k_BTML^2\}\left(n_x^2+n_y^2+n_z^2\right)\right]
\end\{aligned\}$$

Define the translational temperature
$\theta_\{trans\} = \frac\{h^2\}\{8k_BML^2\}$, resulting in the translational
partition function: $$\begin\{aligned\}
\label\{part_trans\}
q_\{trans\} = \left[\sum_\{n=1\}^\infty \exp \left(-\frac\{\theta_\{trans\}\}\{T\}n^2\right)\right]^3
\end\{aligned\}$$

The translational temperature is generally very small. For example,
$O_2$ in a box with $L = 1\textrm\{cm\}$ results in
$\theta_\{trans\}= 1.5\times10^\{-15\}\textrm\{K\}$. This temperature
represents the temperature where the translational modes become excited.

For any appreciable temperature $(T\gg\theta_\{trans\})$ we can
approximate the summation by an integral: $$\begin\{aligned\}
\label\{part_trans_simple\}
q_\{trans\} &\approx \left[\int_0^\infty  e^\{\left(-\frac\{\theta_\{trans\}\}\{T\}n^2\right)\}dn\right]^3  \\ 
&= \left(\frac\{\pi T\}\{4\theta_\{trans\}\}\right)^\{3/2\} \\
&= \left(\frac\{2\pi Mk_BT\}\{h^2\}\right)^\{3/2\} V 
\end\{aligned\}$$

where we have used $V=L^3$, and

$$\begin\{aligned\}
\int_0^\infty e^\{ax^2\} dx = \frac\{1\}\{2\}\sqrt\{\frac\{\pi\}\{a\}\}
\end\{aligned\}$$

We can define the *de Broglie wavelength* $$\begin\{aligned\}
\Lambda = \sqrt\{\frac\{h^2\}\{2\pi Mk_BT\}\}
\end\{aligned\}$$ to arrive at the final result
$q_\{trans\} = \frac\{V\}\{\Lambda^3\}$.

## Rotational Partition Function

For each $l$-index value, there are $(2l+1)$ $m$-index values
(degeneracy). Therefore, the rotational partition function is:
$$\begin\{aligned\}
\label\{part_rot\}
q_\{rot\}=\sum_\{l=0\}^\infty (2l+1)\exp \left[-\frac\{\hslash^2\}\{2k_BT\mu R^2\}l(l+1)\right]
\end\{aligned\}$$

We define the rotational temperature
$\theta_\{rot\}=\frac\{\hslash^2\}\{2k_B\mu R^2\}$, resulting in

$$\begin\{aligned\}
q_\{rot\} &= \sum_\{l=0\}^\infty (2l+1)\exp\left[-\frac\{\theta_\{rot\}\}\{T\}l(l+1)\right]
\end\{aligned\}$$

The rotational temperature is generally small (though larger than
$\theta_\{trans\}$), *e.g.* the rotational temperature for $O_2$ is
$\theta_\{rot\}=2.08K$. For any appreciable temperature
$(T\gg \theta_\{rot\})$, the rotational partition function is approximated
by the integral: $$\begin\{aligned\}
\label\{part_rot_simple\}
q_\{rot\} &\approx \int_0^\infty (2l+1) \exp \left[-\frac\{\theta_\{rot\}\}\{T\}l(l+1)\right] dl \\
&= \frac\{T\}\{\theta_\{rot\}\}
\end\{aligned\}$$

We must correct for rotational symmetries in the molecule, such that
$q_\{rot\} = \frac\{T\}\{\sigma \theta_\{rot\}\}$ . The symmetry factor $\sigma$
accounts for the number of equivalent orientations ($\sigma = 1$ for
heteronuclear diatomic molecules and $\sigma = 2$ for homonuclear
diatomic molecules).

## Vibrational Partition Function

The vibrational partition function is written as:

$$\begin\{aligned\}
\label\{part_vib\}
q_\{vib\}= \sum_\{j=0\}^\infty \exp \left[ - \frac\{\hslash \nu\}\{k_BT\}\left(j+\frac\{1\}\{2\}\right)\right]
\end\{aligned\}$$

We define the vibrational temperature
$\theta_\{vib\}=\frac\{\hslash \nu\}\{k_BT\}$, which dictates when vibrational
modes become excited. This temperature is usually large; for example,
the vibrational temperature for $O_2$ is $q_\{vib\} = 2274K$.

Using the property $\sum_\{j=0\}^\infty z^j = \frac\{1\}\{1-z\}$, we have:

$$\begin\{aligned\}
\label\{part_vib_simple\}
q_\{vib\} &= e^\{-\theta_\{vib\}/(2T)\}\sum_\{j=0\}^\infty \exp \left(-\frac\{\theta_\{vib\}\}\{T\}\right) \notag\\
&= \frac\{ e^\{-\theta_\{vib\}/(2T)\}\}\{1- e^\{-\theta_\{vib\}/(T)\}\}
\end\{aligned\}$$

This exact result for $q_\{vib\}$ is valid over the entire temperature
range, which is necessary since $\theta_\{vib\}$ tends to be very large.
In comparison, the approximate solutions for $q_\{trans\}$ and $q_\{rot\}$
are valid at temperatures $T\gg \theta_\{trans\},\theta_\{rot\}$ (note,
$\theta_\{trans\}$, $\theta_\{rot\}$ are very small).

## Electronic partition function

The thermal excitation of the electronic states in a molecule leads to
the electronic partition function:

$$\begin\{aligned\}
\label\{part_elec\}
q_\{elec\} = g_0 + g_1e^\{-\Delta\varepsilon_1/k_BT\}+g_2e^\{-\Delta\varepsilon_2/k_BT\}...
\end\{aligned\}$$

where $g_i$ is the electronic degeneracy of the $i$th state, and
$\Delta\varepsilon_i$ are the electronic excitation energy of the $i$th
state relative to the ground state.

We define the electronic temperature
$\theta_\{elec\}=\Delta\varepsilon_1/k_B$, which is typically extremely
large ($\theta_\{elec\} \approx 10^4 - 10^5 K$).

For most applications with $T\ll\theta_\{elec\}$, the electronic partition
function is written as $q_\{elec\} \approx g_0$. The ground-state
electronic degeneracy $g_0$ must be found for the molecule (or atom) of
interest. Typically, these are tabulated for many simple molecules. For
$O_2$, the ground-state degeneracy is $g_0 = 3$

## Statistical thermodynamics of $O_2$

The partition function for $O_2$ is given by: $$\begin\{aligned\}
\label\{part_O2\}
Q = \frac\{1\}\{N!\}q^N=\frac\{1\}\{N!\}\left[\left(\frac\{V\}\{\Lambda^3\}\right)\left(\frac\{T\}\{\sigma \theta_\{rot\}\}\right)\left(\frac\{e^\{-\theta_\{vib\}/(2T)\}\}\{1-e^\{-\theta_\{vib\}/(T)\}\}\right)g_0\right]^N
\end\{aligned\}$$

where
$\Lambda = \sqrt\{\frac\{h^2\}\{2\pi Mk_BT\}\} = \frac\{4.4\times 10^\{-10\} mK^\{1/2\}\}\{T^\{1/2\}\}$,
$\theta_\{rot\}=2.08K$, $\sigma = 2$, $\theta_\{vib\}=2274K$, and $g_0=3$
(valid for $\theta_\{trans\}$, $\theta_\{rot\} \ll T \ll \theta_\{elec\}$).

The Helmholtz free energy is found using $F=-k_BT\log Q$ to be:
$$\begin\{aligned\}
\label\{Helmholtz_O2\}
F &=-k_BT\left[N\log \left(\frac\{V\}\{\Lambda^3\}\right)+N\log \left(q_\{rot\}q_\{vib\}q_\{elec\} \right) - log N! \right] \notag\\
&=-k_BTN \left[\log \left(\frac\{V\}\{N \Lambda^3\}\right)+\log \left(q_\{rot\}q_\{vib\}q_\{elec\} \right) +1 \right]
\end\{aligned\}$$

which exhibits the necessary property that $F(T,V,N)$ is a homogeneous
first-order function of $V$ and $N$, which only occurs when the
indistinguishability factor of $\frac\{1\}\{N!\}$ is included in $Q$.

From thermodynamics, we write the equation of state: $$\begin\{aligned\}
\left(\frac\{\partial F\}\{\partial V\}\right)_\{T,N\} &= -p = -\frac\{Nk_BT\}\{V\} \implies
pV= Nk_BT
\end\{aligned\}$$

which is the ideal gas equation of state.

The heat capacity is found to be: $$\begin\{aligned\}
 \label\{HeatCap_O2\}
C_V &= T\left(\frac\{\partial S\}\{\partial T\}\right)_\{V,N\} = -T \left(\frac\{\partial^2 F\}\{\partial T^2\}\right)_\{V,N\} \notag\\
&=\frac\{3\}\{2\}Nk_B + Nk_B+Nk_B\frac\{\theta_\{vib\}^2\}\{T^2\}\frac\{e^\{\theta_\{vib\}/T\}\}\{\left(1 - e^\{\theta_\{vib\}/T\}\right)^2\}
\end\{aligned\}$$

which is valid for $\theta_\{trans\}$,
$\theta_\{rot\} \ll T \ll \theta_\{elec\}$.

The first contribution of $3Nk_B/2$ is associated with the $3N$
translational degrees of freedom ($k_B/2$ from each direction), and the
second term of $Nk_B$ is associated with the $2N$ rotational angles
$\theta$ and $\phi$ (contributing $k_B/2$ each). As $T$ passes through
$\theta_\{vib\}$, the heat capacity goes from $5Nk_B/2$ to $7Nk_B/2$, thus
a single vibrational mode contributes $k_BT$ to the heat capacity. As
the temperature is raised, the individual degrees of freedom get turned
on in order to further maximize the entropy, since more active degrees
of freedom (non-zero probability) contribute to the entropy
(Fig. [\[fig3\]](#fig3)\{reference-type="ref" reference="fig3"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/ideal_gas/Fig3.png)\{width="\\linewidth"\}
:::

## Equipartition Theorem

For $T\gg \theta_\{trans\}$, $\theta_\{rot\}$ , the heat capacity receives
$k_B/2$ for each degree of freedom, thus the average energy per particle
to be: $$\begin\{aligned\}
\langle \varepsilon \rangle = \langle \varepsilon_\{trans\} \rangle+ \langle \varepsilon_\{rot\} \rangle = 3\frac\{k_BT\}\{2\} + 2\frac\{k_BT\}\{2\}
\end\{aligned\}$$

which suggests that the average energy of a thermally active degree of
freedom contributes $k_BT/2$ to the energy.

Consider a microscopic energy function that is quadratic, such that
$\varepsilon(x) = cx^2$, where $x$ is a generalized degree of freedom.
Translational modes and rotational modes both fit in this category,
since: $$\begin\{aligned\}
E_n^\{trans\} &= \frac\{h^2\}\{8mL^2\}n^2\\ E_n^\{rot\} &= \frac\{\hslash^2\}\{2\mu R^2\} l(l+1)
\end\{aligned\}$$ The average energy is: $$\begin\{aligned\}
\langle \varepsilon \rangle &= \frac\{\sum_x \varepsilon (x) \exp \left(-\frac\{\varepsilon (x)\}\{k_BT\}\right)\}\{\sum_x \exp \left(-\frac\{\varepsilon (x)\}\{k_BT\}\right)\} \\
&= \frac\{\sum_x cx^2 \exp \left(-\frac\{cx^2\}\{k_BT\}\right)\}\{\sum_x \exp \left(-\frac\{cx^2\}\{k_BT\}\right)\}
\end\{aligned\}$$ At large $T$, the sums can be approximated by the
integrals: $$\begin\{aligned\}
\langle \varepsilon \rangle \approx \frac\{\int_\{-\infty\}^\{\infty\}  cx^2 \exp \left(-\frac\{cx^2\}\{k_BT\}\right)dx\}\{\int_\{-\infty\}^\{\infty\} \exp \left(-\frac\{cx^2\}\{k_BT\} \right)dx\}
\end\{aligned\}$$ Noting the mathematical properties: $$\begin\{aligned\}
C_0 &= \int_\{-\infty\}^\{\infty\}  e^\{-ax^2\} dx = \frac\{\pi^\{1/2\}\}\{a^\{1/2\}\}\\
C_1 &= \int_\{-\infty\}^\{\infty\} x^2e^\{-ax^2\} dx   = -\frac\{dC_0\}\{da\}=\frac\{\pi^\{1/2\}\}\{2a^\{3/2\}\}
\end\{aligned\}$$ and setting $a = c/(k_BT)$, we have: $$\begin\{aligned\}
\label\{Eq21\}
\langle \varepsilon \rangle = c \frac\{\pi^\{1/2\}\}\{2\left[c/(k_BT)\right]^\{3/2\}\} \frac\{\left[c/(k_BT)\right]^\{1/2\}\}\{\pi^\{1/2\}\} = \frac\{k_BT\}\{2\}
\end\{aligned\}$$

The equipartition theorem states that the total energy will partition
the amount $k_BT/2$ into each thermally active degree of freedom,
provided the microscopic energies obey quadratic energy functions.

Consider a microscopic energy function that is linear, such that
$\varepsilon(x) = cx$, where $x$ is a generalized degree of freedom.
Vibrational modes fit in this category, since $$\begin\{aligned\}
E^\{(vib)\}_j = \left(j + \frac\{1\}\{2\}\right)\hslash \nu
\end\{aligned\}$$ The average energy for this function gives:
$$\begin\{aligned\}
\label\{Eq22\}
\langle \varepsilon \rangle = \frac\{\int_\{0\}^\{\infty\}  cx \exp \left(-\frac\{cx\}\{k_BT\}\right)dx\}\{\int_\{0\}^\{\infty\} \exp \left(-\frac\{cx\}\{k_BT\} \right)dx\} = k_BT
\end\{aligned\}$$ The equipartition theorem predicts that the average
energy per particle for temperatures
$\theta_\{vib\} \ll T \ll  \theta_\{elec\}$ is given by: $$\begin\{aligned\}
\label\{Eq23\}
\langle \varepsilon \rangle = 3\frac\{k_BT\}\{2\} + 2\frac\{k_BT\}\{2\} + k_BT
\end\{aligned\}$$

where 3 translational modes contribute $k_BT/2$ each, 2 rotational modes
contribute $k_BT/2$ each, and 1 vibrational mode contributes $k_BT$

This picture is only valid at temperatures where the modes are active.
For example for $T \ll \theta_\{vib\}$, the discrete vibrational quantum
modes are frozen down to the ground state energy level
$\varepsilon_\{vib\} = \hslash \nu /2$ (independent of temperature).


# Photon Gas \{#chap:photons\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:phase_equilibrium\]](#chap:phase_equilibrium)\{reference-type="ref+label"
reference="chap:phase_equilibrium"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter, we analyze the thermodynamics of photon gas, which
constitutes the description of the radiation properties of Black Bodies.
Here we consider an electromagnetic field in thermal equilibrium with
its container. Under this condition, quantum theory predicts that the
Hamiltonian of the system can be written as a sum of terms, each having
the form of the harmonic oscillator. Thus, the energy of each photon is
given by $\hslash\omega$. This provides our starting point for the
derivation of all the thermodynamic properties of photons from the basis
of statistical mechanics. In addition to the thermodynamic properties of
photon gases, some fundamental laws for Blackbody Radiation will be
obtained, such as Planck's Law, Wien's Law, Rayleigh-Jeans law, and
Stefan-Boltzmann Law.

## Black Body Radiation

We are all familiar with the idea that hot objects emit radiation, a
light bulb, for example. In the hot wire filament, an electron,
originally in an excited state drops to a lower energy state and the
energy difference is given off as a photon,
($\epsilon = h\nu = \epsilon_2 -\epsilon_1$). We are also familiar with
the absorption of radiation by surfaces. For example, clothes in the
summer absorb photons from the sun and heat up. Black clothes absorb
more radiation than lighter ones. This means, of course that lighter
colored clothes reflect a larger fraction of the light falling on them.

A black body is defined as one which absorbs all the radiation incident
upon it, *i.e.* a perfect absorber. It also emits the radiation
subsequently. If radiation is falling on a black body, its temperature
rises until it reaches equilibrium with the radiation. At equilibrium,
it re-emits as much radiation as it absorbs so there is no net gain in
energy and the temperature remains constant. In this case, the surface
is in equilibrium with the radiation and the temperature of the surface
must be the same as the temperature of the radiation.

To develop the idea of radiation temperature we construct an enclosure
having walls which are perfect absorbers (see
Fig. [\[fig8\]](#fig8)\{reference-type="ref" reference="fig8"\}). Inside
the enclosure is radiation. Eventually this radiation reaches
equilibrium with the enclosure walls, equal amounts are emitted and
absorbed by the walls. Also, the amount of radiation travelling in each
direction becomes equal and is uniform. In this case the radiation may
be regarded as a gas of photons in equilibrium having a uniform
temperature. The enclosure is then called an isothermal enclosure.

An enclosure of this type containing a small hole is itself a black
body. Any radiation passing through the hole will be absorbed. The
radiation emitted from the hole is characteristic of a black body at the
temperature of the photon gas. The properties of the emitted radiation
is then independent of the materials of the wall provided they are
sufficiently absorbing that essentially all radiation entering the hole
is absorbed. This universal radiation is called Black Body Radiation.

::: marginfigure
![image](figures/Physics/statistical_mechanics/photon_gas/Fig8.png)\{width="\\linewidth"\}
:::

An everyday example of a photon gas is the background radiation in the
universe. This photon gas is at a temperature of about 5 $K$. Thus the
earth's surface, at a temperature of about 300 $K$, is not in
equilibrium with this gas. The earth is a net emitter of radiation
(excluding the sun) and this is why it is dark at night and why it is
coldest on clear nights when there is no cloud cover to increase the
reflection of the earth's radiation back to the earth. A second example
is a Bessemer converter used in steel manufacture containing molten
steel. These vessels actually contain holes like the isothermal
enclosure of Fig. [\[fig8\]](#fig8)\{reference-type="ref"
reference="fig8"\}. The radiation emitted from the hole is used in steel
making to measure the temperature in the vessel, by means of an optical
pyrometer.

## Statistical Thermodynamics

To derive Planck's radiation law directly from our statistical
mechanics, we note that number of photons in the gas is not fixed. The
photons are absorbed and re-emitted by the enclosure walls. Since the
photons are non-interacting it is by this absorption and re-emission
that equilibrium is maintained in the gas. Since, also the free energy
$F(T, V, N)$ is constant in equilibrium (at constant $T$ and $V$ ) while
N varies it follows that $\partial F/\partial N = 0$, that is
$$\mu = \frac\{\partial F\}\{\partial N\}\bigg|_\{V,N\} = 0 \notag$$

The photon gas is then a Bose gas with $\mu= 0$, so that the canonical
partition function is given directly as $$\label\{Part_photon\}
Q = \sum_\{s_1,s_2,...s_j,... = 0\}^\infty e^\{- \beta \sum_\{l=1\}^\infty s_l\epsilon_l\} = \prod_\{s=1\}^\infty \frac\{1\}\{1-e^\{-\epsilon_\{s\}\beta\}\}$$

Similarly to our previous treatment for phonons, we express the average
energy of the system per unit of volume $\langle E\rangle/V$ considering
a continuum distribution of energies: $$\begin\{aligned\}
\label\{Energy_Density\}
\frac\{\langle E \rangle\}\{V\} = -\frac\{1\}\{V\}\frac\{\partial \log Q\}\{\partial \beta\} &= \frac\{1\}\{V\}\sum_\{l=1\}^\infty \frac\{\epsilon_le^\{-\beta\epsilon_l\}\}\{1 - e^\{-\beta\epsilon_l\}\} \notag\\
&= \int_0^\infty \frac\{1\}\{V\}g(\epsilon)  \frac\{\epsilon\}\{e^\{\beta\epsilon\}-1\} d\epsilon \notag\\
&= \int_0^\infty \frac\{1\}\{V\}\epsilon(\nu)g(\nu)\bar\{n\}(\nu) d\nu
\end\{aligned\}$$

where $\epsilon=h\nu$, according to the energy states defined by the
quantum theory of electromagnetic field;
$g(\nu)=\frac\{4\pi V\}\{c_0^3\}\nu^2$ is the density of states, and
$\bar\{n\}(\nu)$ is the Bose occupation given by:
$$\bar\{n\}(\nu) = \frac\{1\}\{\exp(\epsilon_\{s\}\beta)-1\} \notag$$

The integrand from
([\[Energy_Density\]](#Energy_Density)\{reference-type="ref"
reference="Energy_Density"\}) can be expressed as: $$\label\{Planck\}
u(\nu) = \frac\{8\pi\}\{c^\{3\}\}\frac\{h\nu\}\{\exp(\beta h\nu)-1\}\nu^\{2\}$$

Which is Planck's Radiation Law. We may also recover Wien and
Rayleigh-Jeans laws as limits of Planck's law,

1.  Long wavelength, $\frac\{hc\}\{kT\lambda\} << 1$. Here $$\label\{Eq29\}
    \bar\{\epsilon\}= \frac\{hc/\lambda\}\{\exp(hc/kT\lambda)-1\} \simeq kT$$

    And ([\[Eq29\]](#Eq29)\{reference-type="ref" reference="Eq29"\})
    becomes $$\label\{Eq30\}
    u(\lambda)= \frac\{1\}\{V\} \bar\{\epsilon\}(\lambda) g(\lambda) \simeq \frac\{8 \pi\}\{\lambda^\{4\}\} kT$$

    Which is Wien's law valid at long wavelengths.

2.  Short wavelength, $\frac\{hc\}\{kT\lambda\} >> 1$. Here $$\label\{Eq31\}
    \bar\{\epsilon\}= \frac\{hc\}\{\lambda\} \exp(-hc/kT\lambda)$$

    And ([\[Eq30\]](#Eq30)\{reference-type="ref" reference="Eq30"\})
    becomes $$\label\{Eq32\}
    u(\lambda)= \frac\{1\}\{V\} \bar\{\epsilon\}(\lambda) g(\lambda) \simeq \frac\{8 \pi hc\}\{\lambda^\{5\}\} \exp(-hc/kT\lambda)$$

    Which is the Wien's law valid at short wavelength.

Employing our statistical mechanics we readily obtained Planck's
radiation law.

We may also derive the Stefan-Boltzmann law for: $$\label\{Eq33\}
u = \int_0^\{\infty\} u(\nu) d\nu = \frac\{8\pi\}\{c^\{3\}\} \int_0^\{\infty\} \frac\{h\nu\}\{\exp(\beta h\nu)-1\}\nu^\{2\}d\nu$$

Introducing $x=\beta h\nu$, this reduces to $$\label\{Eq34\}
u = \frac\{8\pi k^\{4\}\}\{(hc)^\{3\}\} \int_0^\{\infty\} dx \frac\{x^\{3\}\}\{\exp(x)-1\} T^\{4\} = aT^\{4\}$$

Where $$\label\{Eq35\}
a = \frac\{8\pi k^\{4\}\}\{(hc)^\{3\}\} \frac\{\pi^\{4\}\}\{15\}$$

In this way we obtain, using statistical mechanics, a law derived
previously using thermodynamics including all the numerical factors.
This gives Stefan's constant $\sigma$ in $$\label\{Eq36\}
E = \frac\{1\}\{4\} caT^\{4\} = \sigma T^\{4\}$$

As $$\label\{Eq37\}
\sigma =  \frac\{2\pi ck^\{4\}\}\{(hc)^\{3\}\} \frac\{\pi^\{4\}\}\{15\} = 5.67 \times 10^\{-5\} \frac\{erg\}\{cm^\{2\}-Sec-K^\{4\}\}$$

A measurement of $\sigma$ could then be used, for example, to determine
Planck's constant. Planck, in fact, determined h as the constant needed
in his radiation law to fit the observed spectral distribution law. This
gave him the value h = 6.55 $\times$ 10$^\{-27\}$ erg.sec which compares
with the present value of h = 6.625 $\times$ 10$^\{-27\}$ erg.sec

## Thermodynamic Properties of Photons

We may calculate all the thermodynamic properties of black body
radiation using statistical mechanics through the partition function
$Q$, where $$\label\{Eq38\}
F =  -kT\log(Q)$$

And $Q$ was derived as shown in
([\[Part_photon\]](#Part_photon)\{reference-type="ref"
reference="Part_photon"\}). This is the basic method of statistical
thermodynamics. The aim is to reproduce all the thermodynamic properties
with all factors and constants evaluated. This gives $$\begin\{aligned\}
\label\{Eq39\}
F &=  -kT\log\left(\prod_\{s=1\}^\{r\} (1-\exp(-\epsilon_\{s\}\beta))^\{-1\}\right) \notag \\
F &=  -kT\sum_\{s=1\}^\{r\} \log(1-\exp(-\epsilon_\{s\}\beta))^\{-1\} \notag \\
F &= -2kT  \int \frac\{d\Gamma\}\{h^\{3\}\} \log (1-\exp(-\epsilon_\{s\}\beta))^\{-1\} 
\end\{aligned\}$$

Where the $\epsilon_\{s\}$ are the single photon states and the factor of
2 arises from the two polarizations available to each photon. This can
be integrated in a variety of ways. Perhaps the most direct is to
integrate over phase space $(d\Gamma=dV\,\,4\pi p^\{2\}\,\,dp)$ and write
$\epsilon=pc$. Introducing the dimensionless variable
$x = \beta \epsilon = \beta pc$, the Helmholtz free energy is:
$$\label\{Eq40\}
F = -\frac\{1\}\{3\} \left\lbrace -\frac\{8\pi k^\{4\}\}\{(hc)^\{3\}\} \int_\{0\}^\{\infty\} d(x)^\{3\} \,\, \log(1-\exp(-x)) \right\rbrace \,\, VT^\{4\}$$

The dimensionless integral here can be transformed into that appearing
in the constant of $a$ of ([\[Eq35\]](#Eq35)\{reference-type="ref"
reference="Eq35"\}), by an integration by parts, i.e., $$\begin\{aligned\}
\label\{Eq41\}
I &= -  \int_\{0\}^\{\infty\} d(x)^\{3\} \log(1-\exp(-x)) \notag \\
&= - (x^\{3\}) \log(1-\exp(-x))\bigg|_\{0\}^\{\infty\} + \int_\{0\}^\{\infty\} x^\{3\} d[\log(1-\exp(-x))]
\end\{aligned\}$$

The first term vanishes because:

$$\begin\{aligned\}
\lim_\{x \to \infty\} x^\{3\} \log(1-\exp(-x)) &\simeq \lim_\{x \to \infty\} x^\{3\} \exp(-x) = 0 \\
\lim_\{x \to 0\} x^\{3\} \log(1-\exp(-x)) &\simeq \lim_\{x \to 0\} x^\{3\} \log(x) = 0
\end\{aligned\}$$

And $$\begin\{aligned\}
\label\{Eq42\}
I &= \int_\{0\}^\{\infty\} dx \frac\{x^\{3\}\}\{\exp(x)-1\} = \frac\{\pi\}\{15\}
\end\{aligned\}$$

Comparing ([\[Eq40\]](#Eq40)\{reference-type="ref" reference="Eq40"\}) and
([\[Eq35\]](#Eq35)\{reference-type="ref" reference="Eq35"\}), we get:
$$\label\{Eq43\}
F = -\frac\{1\}\{3\} aVT^\{4\}$$

From F we may determine all other thermodynamic properties by
differentiation. For example, the entropy is $$\label\{Eq44\}
S = -\left(\frac\{\partial F\}\{\partial T\}\right) \bigg|_\{V\} = \frac\{4\}\{3\} aVT^\{3\}$$

The internal energy is: $$\label\{Eq45\}
U = F +TS = avT^\{4\}$$

The pressure is: $$\label\{Eq46\}
p = -\left(\frac\{\partial F\}\{\partial V\}\right) \bigg|_\{T\}  = \frac\{1\}\{3\} aT^\{4\}$$

Finally, the Gibbs free energy is: s $$\label\{Eq47\}
G = F + pV =  - \frac\{1\}\{3\} aVT^\{4\} + \frac\{1\}\{3\} aVT^\{4\} = 0$$

This is zero as required $G=\mu N$ and the chemical potential $\mu=0$.
We may use these expressions to further verify thermodynamic
consistency, for example that
$C_\{V\} = T\frac\{\partial S\}\{\partial T\}\bigg|_\{V\}= \frac\{\partial U\}\{\partial T\}\bigg|_\{V\}$.

In summary, we have obtained the spectral distribution from the Bose
occupation in much the same way as we obtained the Maxwell-Boltzmann
distribution for a classical gas. The only other required ingredient was
the density of states. We have also obtained all the thermodynamic
properties using the partition function $Q$.


# Imaginary Time Path Integrals \{#chap:imaginary_path\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:imaginary_qm\]](#chap:imaginary_qm)\{reference-type="ref+label"
reference="chap:imaginary_qm"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

[TODO: Cite Shankar]\{style="color: red"\} Consider the transition
probability $$\begin\{aligned\}
    U(x, x'; \tau) &= \langle x | U(\tau) | x'\rangle
\end\{aligned\}$$ We can write this transition density $$\begin\{aligned\}
    \langle x | U(\tau) | x' \rangle &= \int [\mathcal\{D\}x] \exp \left [ -\frac\{1\}\{\hbar\} \int_0^\tau  \mathcal\{L\}_E(x,\bar\{x\}) d\tau \right]
\end\{aligned\}$$ Here the path integral measure and lagrangian is given
by $$\begin\{aligned\}
    \int [ \mathcal\{D\}x] &= \lim_\{N \to \infty\} \left ( \frac\{m\}\{2\pi\hbar \epsilon\} \right )^\{1/2\}\prod_0^\{N-1\} \left ( \frac\{m\}\{2\pi\hbar \epsilon\}\right ) dx_i \\
    \mathcal\{L\}_E &= \frac\{m\}\{2\}\left ( \frac\{dx\}\{d\tau\}\right )^2 + V(x)
\end\{aligned\}$$ This Lagrangian is called the Euclidean lagrangian.
([TODO: Explain how the Euclidean Lagrangian and the usual Lagrangian
differ]\{style="color: red"\}). Similarly here the Euclidean action is
given


# Imaginary Time Quantum Mechanics \{#chap:imaginary_qm\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:quantum_mechanics\]](#chap:quantum_mechanics)\{reference-type="ref+label"
reference="chap:quantum_mechanics"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

[TODO: Cite Shankar]\{style="color: red"\} Quantum mechanics has built up
a rich body of mathematical techniques. Can we adapt these techniques to
solve problems in statistical mechanics? It turns out that a simple
mathematical transformation can do the trick $$\begin\{aligned\}
    t &= -i\tau
\end\{aligned\}$$ That is, we allow time to become a complex variable in
the equations of quantum mechanics rather than a real valued variable.
Applying this transformation to Schrodinger's equation, we get the
imaginary-time Schrodinger equation $$\begin\{aligned\}
-\hbar \frac\{d|\psi(\tau)\rangle\}\{d\tau\} &= H|\psi(\tau)\rangle
\end\{aligned\}$$ This operation is also commonly referred to as a
\"rotation\" to the imaginary axis ([TODO: Insert picture of the complex
plane.]\{style="color: red"\}) The propagator for this system is then
$$\begin\{aligned\}
    U(\tau) &= e^\{-\frac\{H\}\{\hbar\}\tau\}
\end\{aligned\}$$ Note that the propagator is a Hermitian operator and not
a unitary operator. Let $|n\rangle$ be the eigenstates of the
Hamiltonian $$\begin\{aligned\}
    H|n\rangle &= E_n|n\rangle
\end\{aligned\}$$ We can expand the propagator in terms of these
eigenstates $$\begin\{aligned\}
    U(\tau) &= \sum |n\rangle \langle n | e^\{-\frac\{1\}\{\hbar\}E_n\tau\}
\end\{aligned\}$$ For the case of the harmonic oscillator, we can simply
insert $t &= -i\tau$ in the closed form expression for the propagator to
obtain ([TODO: Flesh out the derivation]\{style="color: red"\})
$$\begin\{aligned\}
    U(x,x';\tau) &= \frac\{m\omega\}\{2\pi \hbar \sinh \omega \tau\} \exp \left [ -\frac\{m\omega\}\{2\hbar \sing \omega \tau\}((x^2 + x'^2)\cosh \omega \tau - 2xx' \right ]
\end\{aligned\}$$ The Heisenberg operators $\Omega(\tau)$ can then be
defined in terms of SChrodinger operator $\Omega$ ([TODO: Which
Schrodinger operator is this?]\{style="color: red"\}) $$\begin\{aligned\}
    \Omega(\tau) &= e^\{\frac\{H\}\{\hbar\} \tau\} \Omega e^\{-\frac\{H\}\{\hbar\} \tau\}
\end\{aligned\}$$


# Common Statistical Ensembles \{#chap:statistical_ensemble\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:stat_thermo\]](#chap:stat_thermo)\{reference-type="ref+label"
reference="chap:stat_thermo"\},
[\[chap:noninteracting\]](#chap:noninteracting)\{reference-type="ref+label"
reference="chap:noninteracting"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In the previous chapter, we briefly introduced the notion of a
statistical ensemble. In this chapter, we introduce common ensembles
that are widely studied in statistical mechanics.

Mathematically, an ensemble is a probability density function over phase
space. We can further separate ensembles into closed and open ensembles
depending on whether the number of particles in the system is allowed to
vary.

    ClosedEnsemble = ProbabilityDistribution[ClassicalPhaseSpace]
    OpenEnsemble = ProbabilityDistribution[OpenClassicalPhaseSpace]
    Ensemble = OpenEnsemble | ClosedEnsemble

Different ensembles are defined in terms of different fundamental
physical quantities.

## Defining Relationships

The governing thermodynamic potential of the closed, isolated system is
the entropy $S(U,V,N)$, *i.e.* the control or canonical variables are
$U$, $V$, and $N$. However, the closed, isolated system is only one
possible thermodynamic system. In many instances, it is desirable to
have different control variables, where we replace one (or more)
extensive variable with their conjugate intensive variable
(Fig. [\[fig:Fig4\]](#fig:Fig4)\{reference-type="ref"
reference="fig:Fig4"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/statistical_ensembles/four ensembles.png)\{width="\\linewidth"\}
:::

For convenience, we start from the equation of state $U(S,V,N)$, which
is equivalent to $S(U,V,N)$. The extensive variables and their conjugate
intensive variables are related through the relationships

$$\begin\{aligned\}
\label\{Entropy Change2\}
S \leftrightarrow T = \left(\frac\{\partial U\}\{\partial S\}\right)_\{V,N_1,...,N_r\}\\ 
V \leftrightarrow -p = \left(\frac\{\partial U\}\{\partial V\}\right)_\{S,N_1,...,N_r\}\\
N_i \leftrightarrow \mu_i = \left(\frac\{\partial U\}\{\partial N_i\}\right)_\{S,V,N_\{j \neq i\}\}
\end\{aligned\}$$ Recall the notation we defined in the previous chapter
where underscored quantities on partial derivatives are held constant.

A thermodynamic ensemble is defined by the canonical variables of the
system. Changing from one ensemble to another amounts to shifting from
one thermodynamic potential to another that depends on the new canonical
variables. Replacing an extensive variable for its conjugate intensive
variable is effectively replacing the control variable with the slope of
the control variable.

## Microcanonical Ensemble

The microcanonical ensemble represents the possible states of a closed,
isolated system with a specified total amount of energy. This system is
thermodynamically characterized by the entropy $$\begin\{aligned\}
S = S(U,V,N)
\end\{aligned\}$$.

The statistical distribution for the molecular states of the system
needs to obeys the first and second laws of thermodynamics. The internal
energy $U$ identifies the total kinetic and potential energy of the
molecules in the system, thus in order to satisfy the first law we
require that only states with total energy $E = U$ are permitted in this
ensemble. The second law is satisfied by finding the probability
distribution that maximizes the system entropy.

::: marginfigure
![image](figures/Physics/statistical_mechanics/statistical_ensembles/Isolated_closed and open.png)\{width="\\linewidth"\}
:::

For fixed $E$, $V$, and $N$, there are many degenerate quantum states.
We define the system state as $\nu$, characterizing the quantum state of
the system for a given $E$, $V$, $N$, and the total degeneracy as
$\Omega$. The probability that the system exists in state $\nu$ at any
given time is $P_\nu$.

The connection between the microscopic and macroscopic approaches is
given by the Gibbs Entropy formula, which defines the system entropy $S$
as: $$\begin\{aligned\}
\label\{GibbsEntropy\}
S = - k_B \sum_\nu P_\nu \log P_\nu
\end\{aligned\}$$

where $k_B = 1.8\times10^\{-23\} J/K$ is the Boltzmann's constant. Note
the implicit assumption here that the number of states in the system is
summable. This assumption holds true since we discretize the system's
phase space into discrete microstates.

The Gibbs Entropy formula can be better understood from the perspective
of information theory. According to this $I(P) = \log P_\nu$ is the
*information function*, which defines the amount of information acquired
due to the observation of event $\nu$. The information function has the
following properties:

1.  It is a non-negative quantity $I(P) \ge 0$

2.  Events that always occur do not communicate information $I(1) = 0$

3.  The joint probability communicates as much information as two
    individual events separately $I(P_m P_n) = I(P_m) + I(P_n)$.

Thus, the definition of entropy from
([\[GibbsEntropy\]](#GibbsEntropy)\{reference-type="ref"
reference="GibbsEntropy"\}) might be thought as the expected value of the
amount of information extracted from all the microscopic states in the
macroscopic system. Our task is to determine the value of $P_\nu$ that
maximizes the entropy while maintaining the normalization, thus we must
maximize $S$ subject to a constraint $$\begin\{aligned\}
\sum_\nu P_\nu = 1
\end\{aligned\}$$ A good mathematical strategy to maximize a function
subject to different constraints is by using a Lagrange multiplier,
which defines that the maximization of a function $f(x,y)$ subject to
the constraint $g(x,y) = c$, is given by maximizing $$\begin\{aligned\}
\mathcal\{L\} (x,y,\lambda) = f(x,y) + \lambda g(x,y)
\end\{aligned\}$$ Thus, we seek the maximization of the function:
$$\begin\{aligned\}
\mathcal\{L\}(P_\nu,\lambda_0) = k_B\sum_\nu P_\nu \log P_\nu - \lambda_0\sum_\nu P_\nu 
\end\{aligned\}$$

Maximization occurs when a perturbation $\delta P_\nu$ does not alter
the function (first variation is zero), occurring when:
$$\begin\{aligned\}
0&= \frac\{d\}\{d P_\nu\} \mathcal\{L\}(P_\mu, \lambda_0) \\
&= -k_B\sum_\nu\left(\log P_\nu +1\right) \delta P_\nu - \lambda_0 \sum_\nu\delta P_\nu 
\end\{aligned\}$$

Since $\delta P_\nu$ is arbitrary, the function is maximized if:
$$\begin\{aligned\}
-k_B \log P_\nu - k_B - \lambda_0 &= 0 \\ P_\nu &=\exp\left(-1-\frac\{\lambda_0\}\{k_B\}\right) 
\end\{aligned\}$$

Considering $\Omega$ as the total number of microstates, and the
normalization of the probability, we get the following relation:
$$\begin\{aligned\}
\sum_\nu^\Omega P_\nu &= 1 \\
\sum_\mu^\Omega \exp \left ( -1 - \frac\{\lambda_0\}\{k_B\} \right ) &= 1\\
\Omega \exp \left ( -1 - \frac\{\lambda_0\}\{k_B\} \right ) &= 1 \\
\frac\{1\}\{\Omega\} &= \exp\left(-1 - \frac\{\lambda_0\}\{k_B\}\right)
\end\{aligned\}$$

This leaves the probability distribution: $$\begin\{aligned\}
\label\{ProbMicrocano\}
P_\nu = \frac\{1\}\{\Omega\}
\end\{aligned\}$$

Equation ([\[ProbMicrocano\]](#ProbMicrocano)\{reference-type="ref"
reference="ProbMicrocano"\}) means that the system is equally likely to
be found in any of its accessible states. This is called the principle
of *equal a priori probabilitie*. Additionally we found that in an
isolated system at thermal equilibrium at a given $E$, $V$, $N$, the
system spends equal amount of time in each state over a sufficiently
long period of time, *i.e* the system is *ergodic*. This is always true
provided that the motion of molecules must be sufficiently random. It
implies that the observation time $t\gg \tau$, where $\tau$ is a
microscopic relaxation time for the system (*e.g.* collision time in
gases).

Finally we find that the entropy of the macroscopic system is given by:
$$\begin\{aligned\}
\label\{Entropy\}
S = k_B \log \Omega
\end\{aligned\}$$

which gives complete knowledge of all thermodynamic behavior of the
system, provided it can be evaluated.

    class MicrocanonicalEnsemble[ProbabilityDistribution]($S$: $\mathbb\{R\}$):
      def $\lambda$():
        # Sample Any Accessible State with entropy $S$

### Canonical Ensemble

The canonical ensemble represents the possible states of a closed,
isolated system that is in thermal equilibrium with a heat bath. This
system is thermodynamically characterized by the Helmholtz free energy
$$\begin\{aligned\}
F = F(T,V,N)
\end\{aligned\}$$

We need to find a statistical distribution for the molecular states of
the system that obeys the first and second laws of thermodynamics. In
this case, in order to satisfy the first law, the internal energy $U$
identifies the statistical average of the kinetic and potential energy
of the molecules in the system, defined by $$\begin\{aligned\}
\langle E\rangle
\end\{aligned\}$$ As in the microcanonical ensamble, second law is
satisfied by finding the probability distribution maximizes the system
entropy.

In this ensemble, the total energy $E$ is not uniquely fixed. As a
result, the summation over states of the system includes all microstates
of the system, with each weighted by the appropriate probability.

To illustrate what is meant by this summation, consider the total energy
from the translational microstates of a single point particle in two
dimensions. Therefore, the internal energy is found to satisfy:
$$\begin\{aligned\}
U = \langle E \rangle = \sum_\{n_x=1\}^\infty \sum_\{n_y=1\}^\infty E_\{n_x,n_y\} P_\{n_x,n_y\} 
\end\{aligned\}$$

where $P_\{n_x,n_y\}$ is the probability of finding the system in the
quantum state $n_x,n_y$ (to be explicitly determined).

Begin with our microscopic definition of entropy: $$\begin\{aligned\}
S = - k_B \sum_\mu P_\mu \log P_\mu 
\end\{aligned\}$$

where the sum over $\mu$ implies a sum over all microstates.

In a fixed-temperature ensemble, the total system energy fluctuates
about an average value, such that $\langle E \rangle = U$ (internal
energy). We introduce the two constraints on the probability
distribution: $$\begin\{aligned\}
\sum_\mu P_\mu &= 1 \\
\langle E \rangle &= \sum_\mu E_\mu P_\mu = U 
\end\{aligned\}$$

The probability distribution that maximizes entropy while satisfying
these constraints corresponds to the maximization of: $$\begin\{aligned\}
S - \lambda_0 \sum_\mu P_\mu - \lambda_1 \langle E \rangle = \sum_\mu \left( - k_B P_\mu \log P_\mu - \lambda_0 P_\mu - \lambda_1 E_\mu P_\mu \right) 
\end\{aligned\}$$

The Lagrange multipliers $\lambda_0$ and $\lambda_1$ assume values that
satisfy the constraints. The first variation of this function is set to
zero, giving: $$\begin\{aligned\}
\sum_\mu \left( - k_B \log P_\mu - k_B - \lambda_0 - \lambda_1 E_\mu \right)\delta P_\mu = 0 
\end\{aligned\}$$ Since the $\delta P_\mu$ are arbitrary, each term is set
to zero, thus: $$\begin\{aligned\}
 0 &= -k_B \log P_\mu - k_B - \lambda_0 - \lambda_1 E_\mu  \\
 P_\mu &= \exp \left( -1-\frac\{\lambda_0\}\{k_B\}\right)\exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) 
\end\{aligned\}$$

The first constraint $\sum_\mu P_\mu = 1$ results in: $$\begin\{aligned\}
1 &= \exp \left( -1-\frac\{\lambda_0\}\{k_B\}\right)\sum_\mu \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) \\
\exp \left( 1 + \frac\{\lambda_0\}\{k_B\}\right) &= \sum_\mu \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) 
\end\{aligned\}$$ We define the *partition function* $Z$ as
$$\begin\{aligned\}
    Z &= \exp \left( 1 + \frac\{\lambda_0\}\{k_B\}\right)
\end\{aligned\}$$

The partition function is a powerful mathematical tool for describing a
statistical mechanical system. We will use partition functions to derive
a number of useful quantities.

The probability $P_\mu$ now satisfies: $$\begin\{aligned\}
P_\mu=\frac\{1\}\{Z\} \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) 
\end\{aligned\}$$

where $\lambda_1$ is a currently unspecified Lagrange multiplier.

To satisfy the second constraint
$\sum_\mu E_\mu P_\mu = \langle E\rangle$, we invoke thermodynamic
properties. The differential of the entropy $S$ gives $$\begin\{aligned\}
\left(\delta S\right)_\{V,N\} &= \sum_\mu \left( - k_B \log P_\mu - k_B\right)\delta P_\mu \\ &= \sum_\mu \left(\lambda_1 E_\mu + k_B \log Z - k_B\right)\delta P_\mu
\end\{aligned\}$$

and noting that $\delta 1 = \sum_\mu \delta P_\mu = 0$, we have:
$$\begin\{aligned\}
\left(\delta S\right)_\{V,N\}  = \sum_\mu \lambda_1 E_\mu \delta P_\mu 
\end\{aligned\}$$

The differential of the average energy $\langle E \rangle$ gives:
$$\begin\{aligned\}
\left(\delta \langle E \rangle \right)_\{N,V\} = \sum_\mu E_\mu \delta P_\mu 
\end\{aligned\}$$

From thermodynamics, the Lagrange multiplier $\lambda_1$ is:
$$\begin\{aligned\}
\left( \frac\{\delta S\}\{\delta \langle E \rangle\} \right)_\{V,N\} = \lambda_1 = \frac\{1\}\{T\}
\end\{aligned\}$$

The governing equations for the canonical ensemble are:
$$\begin\{aligned\}
\label\{ProbCanonical\}
P_\mu = \frac\{1\}\{Z\}\exp\left(-\frac\{E_\mu\}\{k_BT\}\right)
\end\{aligned\}$$

where: $$\begin\{aligned\}
\label\{PartCanonical\}
Z = \sum_\mu \exp\left(-\frac\{E_\mu\}\{k_BT\}\right)
\end\{aligned\}$$

Using the definition of the probability distribution and thermodynamics,
we have:

$$\begin\{aligned\}
k_BT \log P_\mu &= -E_\mu - k_BT \log Z \\
k_BT \sum_\mu P_\mu \log P_\mu &= -\sum_\mu P_\mu E_\mu - k_BT \log Z \sum_\mu P_\mu \\
-TS &=-\langle E \rangle - k_B T \log Z 
\end\{aligned\}$$

Since the Helmholtz free energy is given by
$F = \langle E \rangle - TS$, we get:

$$\begin\{aligned\}
F = -k_BT\log Z
\end\{aligned\}$$

The canonical partition function $Z$ provides complete information about
the thermodynamic behavior. If we write the partition function as
$$\begin\{aligned\}
Z = \sum_\mu \exp\left(-\beta E_\nu\right)
\end\{aligned\}$$ where $\beta = 1/\left(k_BT\right)$. The partition
function $Z$ acts as a generating function: $$\begin\{aligned\}
\langle E^m \rangle &= \sum_\nu E_\nu^m P_\nu \\
&= \frac\{1\}\{Z\}\sum_\nu E^m_\nu \exp\left(-\beta E_\nu \right) \\
&= \left(-1\right)^m \frac\{1\}\{Z\} \left(\frac\{\partial^m Z\}\{\partial \beta^m\}\right)_\{V,N\} 
\end\{aligned\}$$

Consider the variance of the energy: $$\begin\{aligned\}
\langle E^2 \rangle - \{\langle E \rangle\}^2 &= \frac\{1\}\{Z\} \left(\frac\{\partial^2 Z\}\{\partial \beta^2\}\right)_\{V,N\} - \frac\{1\}\{Z^2\} \left(\frac\{\partial Z\}\{\partial \beta\}\right)_\{V,N\}^2 \\
&= \left(\frac\{\partial^2 \log Z\}\{\partial \beta^2\}\right)_\{V,N\} \\
&= -\left(\frac\{\partial^2 \beta F\}\{\partial \beta^2\}\right)_\{V,N\} \\
&= k_B T^2 C_V 
\end\{aligned\}$$

connecting energy fluctuation to the heat capacity $C_V$. Bringing this
all together, we can define the canonical ensemble in Physika

``` \{.python language="python"\}
class CanonicalEnsemble[ProbabilityDistribution]
  def Z(N: $\mathbb\{N\}$) : $\mathbb\{R\}$:
    out = 0
    for i in N:
      $\mu$ = sample_microstate()
      out += exp(-$E_\mu$/($k_B$ T))
    out
```

### Grand Canonical Ensemble

The Grand Canonical ensemble represents the possible states of an open
system that is in equilibrium with a reservoir. This system is
thermodynamically characterized by the Landau potential
$$\begin\{aligned\}
-pV = -pV(T,V,\mu)
\end\{aligned\}$$ This thermodynamic system is called the *grand canonical
ensemble* when considered in statistical thermodynamics. We need to find
a statistical distribution for the molecular states of the system that
obeys the first and second laws of thermodynamics. In this case the
internal energy $U$ identifies the statistical average of the kinetic
and potential energy $\langle E\rangle$ of the molecules in the system
(first law), and the probability distribution maximizes the system
entropy (second law).

In this ensemble, the total energy $E$ and the number of particles
$N_i'$ $(i=1,2,...,r)$ are the fluctuating variables.

We begin with our microscopic definition of entropy: $$\begin\{aligned\}
S = -k_B \sum_\nu P_\nu \log P_\nu 
\end\{aligned\}$$

where the sum over $\nu$ implies a sum over all system states and all
particle number $N_i' = 0,1,...$ with $i = 1,2...,r$.

We introduce the $2+r$ constraints on the probability distribution:
$$\begin\{aligned\}
\sum_\nu P_\nu &= 1\\
\langle E\rangle &= \sum_\nu E_\nu P_\nu = U \\
\langle N_i \rangle &= \sum_\nu^r N_i' P_\nu = N_i 
\end\{aligned\}$$

The probability distribution that maximizes entropy while satisfying
these constraints corresponds to the maximization of: $$\begin\{aligned\}
\mathcal\{L\} (P_\nu,\lambda_0,\lambda_1,\Lambda_1,...,\Lambda_r)= \sum_\nu \left( -k_B P_\nu \log P_\nu - \lambda_0 P_\nu - \lambda_1 E_\nu P_\nu - \sum_\{i=1\} ^r\Lambda_iN_i'P_\nu \right) 
\end\{aligned\}$$

The Lagrange multipliers $\lambda_0, \lambda_1,$ and
$\Lambda_i = (i =1,2,...r)$ assume values that satisfy the constraints.

Setting the first variation of this function to zero, we have:
$$\begin\{aligned\}
\sum_\nu\left(-k_B \log P_\nu - k_B - \lambda_0 - \lambda_1 E_\nu - \sum_\{i=1\} ^r\Lambda_iN_i'\right)\delta P_\nu = 0 
\end\{aligned\}$$

Since the $\delta_P$ are arbitrary, each term is set to zero, giving:
$$\begin\{aligned\}
\left(-k_B  \log P_\nu - k_B - \lambda_0 - \lambda_1 E_\nu - \sum_\{i=1\} ^r\Lambda_i N_i'\right) =0 \\
P_\nu = \exp \left(-1-\frac\{\lambda_0\}\{k_B\}\right) \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right) 
\end\{aligned\}$$

The first constraint $\sum_\nu P_\nu = 1$ results in: $$\begin\{aligned\}
1 = \exp \left(-1-\frac\{\lambda_0\}\{k_B\}\right) \sum_\nu  \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right) 
\end\{aligned\}$$

from where we can define the *grand canonical partition function* $\Xi$:
$$\begin\{aligned\}
\label\{PartGrand\}
\Xi &= \exp \left(1+\frac\{\lambda_0\}\{k_B\}\right) \\
&= \sum_\nu  \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right)
\end\{aligned\}$$

The probability $P_\nu$ now satisfies: $$\begin\{aligned\}
\label\{ProbGrand\}
P_\nu = \frac\{1\}\{\Xi\}\exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right)
\end\{aligned\}$$

To satisfy the constraints $$\begin\{aligned\}
\sum_\nu E_\nu P_\nu &= \langle E\rangle \\ \sum_\nu N_i' P_\nu &= \langle N_i \rangle
\end\{aligned\}$$ we invoke thermodynamic properties. The differential of
the entropy $S$ gives: $$\begin\{aligned\}
\left(\delta S\right)_\{V\} &= \sum_\nu \left( - k_B \log P_\nu - k_B\right)\delta P_\nu \\ &= \sum_\nu \left(\lambda_1 E_\nu + \sum_\{i=1\}^r\Lambda_iN_i' - \log \Xi - k_B\right)\delta P_\nu 
\end\{aligned\}$$

and noting that $\delta 1 = \sum_\nu \delta P_\nu = 0$, we have:
$$\begin\{aligned\}
\left(\delta S\right)_\{V\}  = \sum_\nu \left( \lambda_1 E_\nu  + \sum_\{i=1\}^r\Lambda_iN_i'\right) \delta P_\nu 
\end\{aligned\}$$ The differentials of the average energy
$\langle E \rangle$ and $\langle N_i \rangle$ gives: $$\begin\{aligned\}
\left(\delta \langle E \rangle \right)_\{V\} &= \sum_\nu E_\nu \delta P_\nu \hspace\{0.2cm\} \\
\left(\delta \langle N_i \rangle \right)_\{V\} &= \sum_\nu N_i' \delta P_\nu 
\end\{aligned\}$$

From thermodynamics, the Lagrange multiplier $\lambda_1$ is:
$$\begin\{aligned\}
\left( \frac\{\delta S\}\{\delta \langle E \rangle\} \right)_\{V,N\} &= \lambda_1 = \frac\{1\}\{T\} \\ 
\left( \frac\{\delta S\}\{\delta \langle N_i \rangle\} \right)_\{E,V,N_\{j\neq i\}\} &= \Lambda_i = - \frac\{\mu_i\}\{T\} 
\end\{aligned\}$$

The governing equations for the grand canonical ensemble are:
$$\begin\{aligned\}
P_\mu = \frac\{1\}\{\Xi\}&\exp\left(-\frac\{E_\nu\}\{k_BT\}+ \sum_\{i=1\}^r\frac\{\mu_iN_i'\}\{k_BT\} \right) \hspace\{0.5cm\} \\ 
\Xi &= \sum_\nu \exp\left(-\frac\{E_\nu\}\{k_BT\}+ \sum_\{i=1\}^r\frac\{\mu_iN_i'\}\{k_BT\}\right) 
\end\{aligned\}$$

``` \{.python language="python"\}
class GrandCanonicalEnsemble[ProbabilityDistribution]
  def $\Xi$(N: $\mathbb\{N\}$):
    out = 0
    for _ in N:
      $\mu$ = sample_microstate()
      out += exp(-$E_\mu$/(k_B*T) + sum(1, r, $\mu_i N_i'$/($k_B$ T)))
    out
```

Using the probability distribution and thermodynamics, we have:
$$\begin\{aligned\}
k_BT\log P_\nu = -E_\nu + \sum_\{i=1\}^r\mu_iN_i' -k_BT\log\Xi \\
-TS =-\langle E \rangle + \sum_\{i=1\}^r\mu_i \langle N_i \rangle -k_BT\log\Xi \\
-pV = \langle E \rangle -TS - \sum_\{i=1\}^r \mu_i \langle N_i \rangle = -k_BT\log\Xi 
\end\{aligned\}$$

Therefore, the grand canonical partition function $\Xi$ provides
complete knowledge of the thermodynamic behavior of the system.
Furthermore, the grand canonical partition function acts as a generating
function for averages; for example, the moments are: $$\begin\{aligned\}
\langle E^m \rangle = (-1)^m\frac\{1\}\{\Xi\} \left(\frac\{\partial^m \Xi\}\{\partial \beta^m\}\right)_\{V,\mu\} \hspace\{0.2cm\} \text\{and\} \hspace\{0.2cm\} \langle N_i^m \rangle = \frac\{1\}\{\Xi\} \left(\frac\{\partial^m \Xi\}\{\partial \lambda_i^m\}\right)_\{T,V,\mu_\{j\neq i\}\} 
\end\{aligned\}$$ where $\beta= 1/(k_BT)$ and $\lambda_i = \mu_i/(k_BT)$.


# Ising model \{#chap:ising_model\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:stat_thermo\]](#chap:stat_thermo)\{reference-type="ref+label"
reference="chap:stat_thermo"\},
[\[chap:noninteracting\]](#chap:noninteracting)\{reference-type="ref+label"
reference="chap:noninteracting"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Thus far, our analyses have focused on non-interacting systems.
Non-interacting systems require the evaluation of a single-molecule
partition function, which generally is tractable (either exactly or
approximately). However, very few practical problems involve either
non-interacting molecules or approximately non-interacting molecules.
Furthermore, a variety of important physical phenomena are not exhibited
in non-interacting systems. For example, phase transitions are a
hallmark issue in interacting systems. Our goal for this chapter is to
discuss the effect of interactions on thermodynamic behavior.

We introduce the Ising Model, which constitutes the basis for the
analysis of long ranged correlations. Behind its apparently simple
formulation, the Ising model hides a complex mathematical problem,
specially for 2 or 3 dimensional systems, that requires a number of
approximations in order to derive a reasonable descriptive model for the
study of these correlations. Of course, there is always a cost
associated for using these approximations, and we will discuss these
limitations over the course of this chapter. For now, the behavior of
the Ising model in 1D system will be presented, and in the second part,
we will introduce the mean field approximation for the study of 2D
systems.

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/2D_lattice_structure.png)\{width="\\linewidth"\}
:::

Consider a lattice of $N$ sites. The definition of the lattice can
include many different geometries (square lattice, triangular lattice,
etc..) and dimensionality (1 dimension, 2 dimensions, etc\...)
(Fig. [\[lattice\]](#lattice)\{reference-type="ref"
reference="lattice"\}). In the presence of a magnetic field, $h$, the
energy of the system in a particular state, $\nu$, is: $$\begin\{aligned\}
E_\nu &= -\sum_\{i=1\}^N h s_i + (\text\{energy due to interactions between spins\})
\end\{aligned\}$$ According to the expression, each site contains exactly
1 spin associated to its magnetic moment $\pm \mu$. Thus, the net
magnetic moment is defined by the spin state $s_i$, that is either spin
up ($s_i = +1$) or spin down ($s_i = -1$).

A simple model for the interaction energy is: $$\begin\{aligned\}
- J \sum_\{\langle i j \rangle\}s_i s_j
\end\{aligned\}$$

where the sum over $\langle i j \rangle$ implies a summation over all
nearest neighbor pairs in the lattice, with a coupling strength $J$.

The system energy is now defined to be: $$\begin\{aligned\}
\label\{Eq1\}
E_\nu= -h\sum_\{i=1\}^N s_i - J \sum_\{\langle i j \rangle\}s_i s_j
\end\{aligned\}$$

which is the Ising Model. In general, this model can be applied to a
variety of problems (magnetic systems, binary alloys, liquid-gas phase
transition, neural networks - Hopfield model).

Notice that when $J> 0$, it is energetically favorable that the
neighboring spins remain aligned. At low temperatures, this alignment
between spins will lead to a cooperative phenomenon called spontaneous
magnetization.

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/ground states_2D_ising_model.png)\{width="\\linewidth"\}
:::

In general the sign of the coupling constant $J$ dictates whether the
spins prefer to align with their neighbors or anti-align
(Fig. [\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}):

-   Ferromagnetic: the spins prefer to align with their neighbors,
    occurring when $J > 0$.

-   Antiferromagnetic: the spins prefer to anti-align with their
    neighbors, occurring when $J < 0$.

Define the total magnetization $M$ of the system to be:
$$\begin\{aligned\}
\label\{Eq2\}
M = \bigg\langle \sum_\{i=1\}^N s_i \bigg\rangle = \frac\{\partial \log Q\}\{\partial \beta h\}
\end\{aligned\}$$

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/spont_magnetization.png)\{width="\\linewidth"\}
:::

In the absence of an external magnetic field $h$, for $J>0$ we expect
the system to develop a net magnetization $\langle M \rangle$ at
temperatures below $T_c$, defined as the Curie Temperature or Critical
Temperature (Fig.[\[fig2a\]](#fig2a)\{reference-type="ref"
reference="fig2a"\})

### *Non-interacting case ($J = 0$)* \{#non-interacting-case-j-0 .unnumbered\}

The partition function for the Ising model with $J = 0$ is given by
$$\begin\{aligned\}
Z &= \sum_\{s_1=-1,1\}\sum_\{s_2=-1,1\}...\sum_\{s_N=-1,1\} \exp\left(\beta h\sum_\{i=1\}^Ns_i\right)\\
&=\prod_\{j=1\}^N \left(\sum_\{s_j=-1,1\}e^\{\beta h s_j\}\right) =\left(\sum_\{s=-1,1\}e^\{\beta h s\}\right)^N \\
&= \left(e^\{\beta h\} + e^\{-\beta h\}\right)^N \\
&= 2^N \cosh^N\left(\beta h \right) 
\end\{aligned\}$$

which is valid for any lattice type and dimensionality. The total
magnetization $M$ is: $$\begin\{aligned\}
\label\{Eq2\}
M = \frac\{\partial \log Z\}\{\partial \beta h\} = N\tanh \left(\beta h \right) 
\end\{aligned\}$$

The average energy $\langle E \rangle$ and entropy $S$ are given by:
$$\begin\{aligned\}
\langle E \rangle &= -h \bigg\langle \sum_\{i=1\}^Ns_i \bigg\rangle \\
&= -hM \\
&= -hN\tanh \left(\beta h \right) \\
S &= \frac\{E-F\}\{T\} \\
&= \frac\{-hN\tanh \left(\beta h \right) + Nk_BT\log\left[2\cosh \left(\beta h \right)\right]\}\{T\} 
\end\{aligned\}$$

Note, $\langle E \rangle \rightarrow -hN$ and $S\rightarrow 0$ as
$T \rightarrow 0$, and $\langle E \rangle \rightarrow 0$ and
$S \rightarrow Nk_B \log 2$ as $T \rightarrow \infty$.

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/total_magnetization.png)\{width="\\linewidth"\}
:::

The non-interacting Ising model ($J =0$) exhibits a gradual change in
the magnetization $M$ with the external field $h$
(Fig. [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}). Thus,
no phase transition is exhibited in the non-interacting case; there is
never a phase transition for non-interacting molecules. We now turn to
cases where the interactions are turned on, such that $J \neq 0$.

## *Ising model in 1 dimension* \{#ising-model-in-1-dimension .unnumbered\}

Consider a line of spins in $1$ dimension with periodic boundary
conditions such that $s_N$ interacts with $s_1$. For the Ising model in
$1$ dimension, the energy is written as: $$\begin\{aligned\}
\label\{Eq3\}
\beta E = -\tilde\{h\} \sum_\{i=1\}^N s_i -\tilde\{J\}\sum_\{i=1\}^N s_is_\{i+1\}
\end\{aligned\}$$ where $$\begin\{aligned\}
\tilde\{h\} & =\beta h  \\
\tilde\{J\} &= \beta J
\end\{aligned\}$$ Define a transfer function $T(s_i,s_\{i+1\})$ to be
$$\begin\{aligned\}
\label\{Eq4\}
T(s_i,s_\{i+1\}) &= \exp \left[ \tilde\{h\}\left(\frac\{s_i\}\{2\}+\frac\{s_\{i+1\}\}\{2\}\right)+\tilde\{J\}s_is_\{i+1\}\right]
\end\{aligned\}$$

With this definition, we write the partition function as:
$$\begin\{aligned\}
Z = \sum_\{s_1=-1,1\}\sum_\{s_2=-1,1\}...\sum_\{s_N=-1,1\} T(s_1,s_2)T(s_2,s_3)...T(s_N,s_1)
\end\{aligned\}$$

Now construct the *transfer matrix* to be: $$\begin\{aligned\}
\label\{Eq5\}
T = \left[ \begin\{array\}\{cc\} T(1,1) & T(1,-1) \\ T(-1,1) & T(-1,-1) \end\{array\} \right] = \left[ \begin\{array\}\{cc\} e^\{\tilde\{h\}+\tilde\{J\}\} & e^\{-\tilde\{J\}\} \\ e^\{-\tilde\{J\}\} & e^\{-\tilde\{h\}+\tilde\{J\}\} \end\{array\} \right] 
\end\{aligned\}$$

The partition function is then written as: $$\begin\{aligned\}
\label\{Eq6\}
Z = \sum_\{s_1=-1,1\}T^N(s_1,s_1) = Tr\{T^N\}
\end\{aligned\}$$

where $Tr\{T^N\}$ indicates a trace of the matrix $M$.

The trace of a matrix is the sum of its eigenvalues, and the eigenvalues
of $T^N$ are $\lambda_\pm ^N$, where $\lambda_\pm$ are the eigenvalues
of $T$. In our case: $$\begin\{aligned\}
\label\{Eq7\}
\lambda_\pm =e^\{\tilde\{J\}\} \cosh \tilde\{h\} \pm \left(e^\{-2\tilde\{J\}\} + e^\{2\tilde\{J\}\}\sinh^2\tilde\{h\}\right)^\{1/2\}
\end\{aligned\}$$

Therefore, the exact solution for the partition function is:
$$\begin\{aligned\}
Z = \left[e^\{\tilde\{J\}\} \cosh \tilde\{h\} + \left(e^\{-2\tilde\{J\}\} + e^\{2\tilde\{J\}\}\sinh^2\tilde\{h\}\right)^\{1/2\}\right]^N + \\ \left[e^\{\tilde\{J\}\} \cosh \tilde\{h\} \pm \left(e^\{-2\tilde\{J\}\} + e^\{2\tilde\{J\}\}\sinh^2\tilde\{h\}\right)^\{1/2\}\right]^N 
\end\{aligned\}$$

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/Phase_transition_1D (1).png)\{width="\\linewidth"\}
[]\{#fig3a label="fig3a"\}
:::

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/Phase_transition_2D (1).png)\{width="\\linewidth"\}
:::

For $\tilde\{J\} =0$, the partition function $Z=2^N \cosh^N \tilde\{h\}$,
which agrees with our previous result. For $\tilde\{h\}=0$, the partition
function
$Z=2^N\cosh^N\tilde\{J\} + 2^N\sinh^N\tilde\{J\} \approx 2^N\cosh^N\tilde\{J\}$
for large $N$.

Since $\lambda_+ > \lambda_-$, the partition function is
$Z \approx \lambda_+^N$ for $N \gg 1$. The average magnetization is
given by $$\begin\{aligned\}
M = \frac\{\partial \log Z\}\{\partial h\} = N \frac\{\sinh \tilde\{h\}\}\{\sqrt\{e^\{-4\tilde\{J\}\}\}+\sinh^2\tilde\{h\}\}
\end\{aligned\}$$ In the limit $\tilde\{h\} \rightarrow 0^+$, the
magnetization approaches zero. Thus, this exact solution for the 1D
Ising model does not exhibit a phase transition from a disordered state
to an ordered state. The physical justification for this lies in the
fact that forming a disordered phase from an ordered phase occurs
through an energy change $\Delta E = 4J$ in 1D (independent of $N$).
Thus, the net magnetization per particle should vanish for all
temperatures higher than $T\sim J/Nk_B$, which is very small considering
that $N$ is large for macroscopic systems
(Fig. [\[fig3b\]](#fig3b)\{reference-type="ref" reference="fig3b"\}a).

In higher dimensions ($D \geq 2$), the energy cost scales with $N$
(*e.g.* $\Delta E \sim N^\{1/2\}J$ in 2D), thus leading to a finite-$T$
phase transition (Fig. [\[fig3b\]](#fig3b)\{reference-type="ref"
reference="fig3b"\}b).

### Mean-field approximation \{#mean-field-approximation .unnumbered\}

The solution for the Ising model for $D \geq 2$ is not as easily found.
In 1944, Lars Onsager found an exact solution for the 2D Ising model for
$h = 0$ that exhibits a spontaneous magnetization for sufficiently large
$J$. However, there is no exact solution for the Ising model for
$D \geq 3$.

We introduce the mean-field approximation. The idea of this method is to
focus on one particular particle of the system, and assume that the role
of the neighboring particles is to form an average molecular magnetic
field, which acts on the tagged particle
(Fig. [\[fig4\]](#fig4)\{reference-type="ref" reference="fig4"\}). In the
next lecture, we test the limits of this approximation.

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/Mean field.png)\{width="\\linewidth"\}
:::

Define the average magnetization per site $$\begin\{aligned\}
m=\frac\{M\}\{m\}=\frac\{1\}\{N\}\sum_\{i=1\}^N\langle s_i\rangle = \langle s\rangle
\end\{aligned\}$$ by noting that spatial invariance implies
$\langle s_i\rangle = \langle s\rangle$. Define the coordination number
z as the number of nearest neighbors to any given site divided by $2$
(*e.g.* $z = 2$ for 2D square lattice and $z = 3$ for 3D square
lattice). The mean-field approximation replaces the near-neighbor
interactions by the average interaction, thus
$s_j \approx \langle s_j\rangle$, giving the energy approximation:

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/tanh_K_plot.png)\{width="\\linewidth"\}
:::

$$\begin\{aligned\}
\label\{Eq8\}
E &= -h\sum_\{i=1\}^N s_i - J\sum_\{\langle i j\rangle\}s_is_j \\
&\approx -h\sum_\{i=1\}^N s_i - Jzm\sum_\{i=1\}^N s_i
\end\{aligned\}$$

which reduces the problem to an effective non-interacting model.

Therefore, we write the average magnetization per site as:
$$\begin\{aligned\}
\label\{Eq9\}
m = \frac\{\sum_\{s=-1,1\}s \exp\left[ (\beta h + \beta Jzm) s \right]\}\{\sum_\{s=-1,1\} \exp\left[ (\beta h + \beta Jzm) s \right]\} = \tanh (\beta h + \beta Jzm)
\end\{aligned\}$$

which performs the average of the site magnetization using the
mean-field site energy in the Boltzmann weight for the spin probability.

For the case $h = 0$ (no external field), the magnetization satisfies
$$\begin\{aligned\}
m &= \tanh (Km)
\end\{aligned\}$$ where $K = \beta Jz$. This is a self-consistent
mean-field equation. Local order is dictated by the surrounding order
that is itself defined by the local order, leading to a self-consistent
condition that must be met. Two scenarios exist
(Fig. [\[fig5\]](#fig5)\{reference-type="ref" reference="fig5"\}):

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/ising_phase_diagram0.png)\{width="\\linewidth"\}
:::

-   For $K < 1$, there is only one solution for m that corresponds to
    $m = 0$.

-   For $K >1$, there are 3 solutions for $m$ corresponding to $m=0$ and
    $m=\pm m^*$. The solution $m=0$ is unphysical as it is a free energy
    maximum.

Therefore, $K = K_c = 1$ defines a critical temperature $T_c = Jz/k_B$,
and we can write $K = T_c/T$
(Fig. [\[fig6\]](#fig6)\{reference-type="ref" reference="fig6"\}).

Near $T = T_c$, we can expand the self-consistent equation near $m = 0$
to get: $$\begin\{aligned\}
m = \tanh \left( \frac\{T_c\}\{T\}m \right) \approx \frac\{T_c\}\{T\}m - \frac\{1\}\{3\}\left( \frac\{T_c\}\{T\}\right)^3m^3 \rightarrow m \approx \pm \sqrt\{\frac\{3\}\{T_c\}\}(T_c-T)^\{1/2\} 
\end\{aligned\}$$

Generally, the external field $h$ acts as the intensive variable that is
conjugate to the magnetization $M$ (extensive variable). Earlier in the
book, we learned that thermodynamic stability dictates that a derivative
of an intensive variable with respect to its conjugate extensive
variable is a positive quantity, *e.g.*: $$\begin\{aligned\}
\label\{Eq10\}
\left( \frac\{\partial T\}\{\partial S\} \right)_\{V,N\} & > 0\\ 
- \left( \frac\{\partial p\}\{\partial V\} \right)_\{T,N\} &> 0\\
\left( \frac\{\partial \mu_i\}\{\partial N_i\} \right)_\{T,V,N_\{j\neq i\}\} &> 0
\end\{aligned\}$$

Similarly, thermodynamic stability dictates that: $$\begin\{aligned\}
\label\{Eq11\}
\left( \frac\{\partial \beta h\}\{\partial M\} \right)_\{\beta Jz,N\}>0
\end\{aligned\}$$

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/ising_plot_m_bh.png)\{width="\\linewidth"\}
:::

The phase diagram for the mean-field solution is dictated by the
self-consistent mean field equation
(Fig. [\[fig7\]](#fig7)\{reference-type="ref" reference="fig7"\}):
$$\begin\{aligned\}
\label\{Eq12\}
m = \tanh (\beta h + \beta Jzm)
\end\{aligned\}$$

This equation of state can be likened to the vapor-liquid equation of
state $p = p(v)$.

The phase diagram for the mean-field approximation for the Ising model
exhibits similar characteristics as the liquid-vapor phase diagram. The
correspondence between $M-h$ in the magnetic system and $p-V$ in the
liquid-vapor system reflects the universal physical issues that are
addressed with the Ising model and why it can be used to understand a
variety of phenomena (Fig. [\[fig9\]](#fig9)\{reference-type="ref"
reference="fig9"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/ising_model/ising_hysteresis.png)\{width="\\linewidth"\}
:::


# Phase Equilibrium and Thermodynamic Stability \{#chap:phase_equilibrium\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:classical_thermo\]](#chap:classical_thermo)\{reference-type="ref+label"
reference="chap:classical_thermo"\},
[\[chap:stat_thermo\]](#chap:stat_thermo)\{reference-type="ref+label"
reference="chap:stat_thermo"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Having reviewed classical thermodynamics, we now venture into the
concept of thermodynamic stability. We learned that a system reaches
equilibrium when any small variation of the thermodynamic potential is
zero, *i.e.* $\delta U = 0$ for a closed isolated system. However, the
second law imposes an additional condition: *a closed isolated system is
in equilibrium when the entropy has reached its global maximum*. This is
directly related with the concept of thermodynamic stability and
fluctuations, meaning that even when microscopically the system is under
constant change, it will remain stable to small variation from
equilibrium. Using *Maxwell Relations*, we will derive explicit
expressions to analyze the stability of a system. In the last part of
this module, we will use these expression to derive the conditions for
phase equilibrium.

## Maxwell Relations

We learned that macroscopic system can be studied under the perspective
of different ensembles, each one having a thermodynamic potential, and
three control variables. Thus, a closed isolated system is represented
by the control variables $U,V,N$ and the thermodynamic potential $S$, or
similarly, with the control variables $S,V,N$, and the thermodynamic
potential $U$. Also, from the statement that the control variables are
extensive, we derived an explicit expression for internal energy through
the *Euler Equation*.

The functional form of a closed isolated system, together with the use
of *Legendre Transformations*, allow us to derive a number of new
ensembles and their respective thermodynamic potentials. The following
list considers the most used thermodynamic potentials, and their
differential forms:\
**Internal Energy: $U(S,V,N)$** $$\begin\{aligned\}
U = TS - pV + \sum^r_\{i=1\}\mu_i\{N\}_i \rightarrow dU = TdS - pdV + \sum^r_\{i=1\}\mu_i\{dN\}_i \notag
\end\{aligned\}$$ **Enthalpy: $H(S,p,N)$** $$\begin\{aligned\}
H = TS  + \sum^r_\{i=1\}\mu_i\{N\}_i \rightarrow dH = TdS + Vdp + \sum^r_\{i=1\}\mu_i\{dN\}_i \notag
\end\{aligned\}$$ **Helmholtz Free Energy**: $F(T,V,N)$ $$\begin\{aligned\}
F = -pV + \sum^r_\{i=1\}\mu_i\{N\}_i \rightarrow dF = -SdT - pdV    + \sum^r_\{i=1\}\mu_i\{dN\}_i \notag
\end\{aligned\}$$ **Gibbs Free Energy**: $G(T,p,N)$ $$\begin\{aligned\}
G = \sum^r_\{i=1\}\mu_i\{N\}_i \rightarrow dG = -SdT + Vdp  + \sum^r_\{i=1\}\mu_i\{dN\}_i \notag
\end\{aligned\}$$ **Landau Potential**: $\Phi(T,V,\mu)$ $$\begin\{aligned\}
\Phi = -pV \rightarrow d\Phi = -SdT - pdV   - \sum^r_\{i=1\}\{N\}_id\mu_i \notag
\end\{aligned\}$$

The second-order partial derivatives with respect to these potentials do
not depend on the order that you perform the derivative (state
variables). For example, we have
$\left(\frac\{\partial^2 U\}\{\partial V \partial S\}\right) = \left(\frac\{\partial^2 U\}\{\partial S \partial V\}\right)$,
from where we find:

$$\begin\{aligned\}
\label\{T-prelation\}
\{\left(\frac\{\partial T\}\{\partial V\}\right)\}_\{S,N\} =  
-\{\left(\frac\{\partial p\}\{\partial S\}\right)\}_\{V,N\}
\end\{aligned\}$$

Such second-derivative equalities are called Maxwell relations. The
Maxwell relations involving $T$, $S$, $p$ and $V$ are:

$$\begin\{aligned\}
\{\left(\frac\{\partial T\}\{\partial V\}\right)\}_\{S,N\} =  
-\{\left(\frac\{\partial p\}\{\partial V\}\right)\}_\{V,N\} \hspace\{1.0cm\} \{\left(\frac\{\partial T\}\{\partial p\}\right)\}_\{S,N\} =  
\{\left(\frac\{\partial V\}\{\partial S\}\right)\}_\{p,N\} \notag \\
\{\left(\frac\{\partial S\}\{\partial V\}\right)\}_\{T,N\} =  
\{\left(\frac\{\partial p\}\{\partial T\}\right)\}_\{V,N\} \hspace\{1.0cm\} \{\left(\frac\{\partial S\}\{\partial p\}\right)\}_\{T,N\} =  
-\{\left(\frac\{\partial V\}\{\partial T\}\right)\}_\{p,N\} \notag
\end\{aligned\}$$

In addition to these Maxwell relations, there exist partial derivatives
involving the chemical potential of the *i*th species $\mu_i$. For
example, the Maxwell relation:

$$\{\left(\frac\{\partial p\}\{\partial N_i\}\right)\}_\{T,V,N_\{j\neq i\}\} =  
-\{\left(\frac\{\partial \mu_i\}\{\partial V\}\right)\}_\{T,N\}$$

is derived from the second derivative of the Helmholtz free energy
$\left(\frac\{\partial^2 F\}\{\partial N_i \partial V\}\right) = \left(\frac\{\partial^2 F\}\{\partial V \partial N_i\}\right)$.

The Maxwell relations are useful for relating difficult to find
thermodynamic quantities to quantities that are more easily determined.
In particular, changes in entropy are difficult to find, so it's easier
to relate this to a change in $p$ or $V$ with respect to $T$.

## Thermodynamic Stability

We discussed the conditions for thermodynamic equilibrium in a closed,
isolated system with subsystems 1 and 2. Specifically, we found the
following criterion for equilibrium:

-   Thermal equilibrium occurs when $T^\{(1)\} = T^\{(2)\}$.

-   Mechanical equilibrium occurs when $p^\{(1)\} = p^\{(2)\}$.

-   Chemical equilibrium occurs when $\mu_i^\{(1)\} = \mu_i^\{(2)\}$, for
    all species $i=1,2...,r$.

These criterion were derived by finding conditions where
$(\delta S)_\{U,V,N\} = 0$ (first variation of the entropy).

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig1.png)\{width="\\linewidth"\}
:::

According to the second law of thermodynamics, the equilibrium condition
means also that the entropy has reached a maximum. However, the maximum
entropy condition requires that the curvature is negative, *i.e.* the
second variation of the entropy satisfies $(\delta^2 S)_\{U,V,N\} < 0$.
Thus, conditions where $(\delta^2 S)_\{U,V,N\} = 0$, mark the point where
the equilibrium point is unstable (limit of stability).

Consider a closed, isolated system that is at equilibrium. Within the
system are two subsystems that are separated by a rigid, impenetrable
barrier that conducts heat
(Fig. [\[fig:isolatedsystem\]](#fig:isolatedsystem)\{reference-type="ref"
reference="fig:isolatedsystem"\}). A small fluctuation in heat across the
boundary causes a small perturbation in the internal energy of the
subsystems ($\delta U^\{(1)\} = -\delta U^\{(2)\}$). Since the boundary is
rigid and impenetrable, the volumes and particle numbers remain fixed
(*i.e.*
$\delta V^\{(1)\} = \delta V^\{(2)\}=\delta N^\{(1)\} = \delta N^\{(2)\}=0$).
However, there will be a perturbation to the entropy of the system, from
where we can determine whether the system remains stable.

Since the entropy is an extensive quantity ($S^\{(1)\}+S^\{(2)\} = S)$, we
can express a change in the entropy of the system be: $$\begin\{aligned\}
 \label\{deltaS\}
\Delta S &= S^\{(1)\} \left( U^\{(1)\}+\delta U^\{(1)\},V^\{(1)\},N^\{(1)\}\right) +S^\{(2)\} \left( U^\{(2)\}+\delta U^\{(2)\},V^\{(2)\},N^\{(2)\}\right) 
\notag\\  
& \hspace\{0.5cm\} -S^\{(1)\} \left( U^\{(1)\},V^\{(1)\},N^\{(1)\}\right) -S^\{(2)\} \left( U^\{(2)\},V^\{(2)\},N^\{(2)\}\right) 
\notag\\
&=\left[\{\left(\frac\{\partial S\}\{\partial U\}\right)\}_\{V,N\}^\{(1)\}-\{\left(\frac\{\partial S\}\{\partial U\}\right)\}_\{V,N\}^\{(2)\}\right]\delta U^\{(1)\} 
\notag\\  
& \hspace\{0.5cm\} + \frac\{1\}\{2\}\left[\{\left(\frac\{\partial^2 S\}\{\partial^2 U\}\right)\}_\{V,N\}^\{(1)\}+\{\left(\frac\{\partial^2 S\}\{\partial^2 U\}\right)\}_\{V,N\}^\{(2)\}\right]\{\left(\delta U^\{(1)\}\right)\}^2
\end\{aligned\}$$

up to quadratic order in the perturbation $\delta U^\{(1)\}$.

The linear-order term is the first variation of the entropy
$\left(\partial S\right)_\{U,V,N\}$ , and the quadratic-order term is the
second variation $\left(\partial^2 S\right)_\{U,V,N\}$. Since we begin at
equilibrium, $T^\{(1)\}=T^\{(2)\}=T$, and noting that
$\{\left(\frac\{\partial S\}\{\partial U\}\right)\}_\{V,N\}=\frac\{1\}\{T\}$, the
first term at the right of
Eq. [\[deltaS\]](#deltaS)\{reference-type="ref" reference="deltaS"\}. is
zero. For the second term, we use the thermodynamic relations:

$$\{\left(\frac\{\partial^2 S\}\{\partial^2 U\}\right)\}_\{V,N\} = \left(\frac\{\delta (1/T)\}\{\delta U\}\right)_\{V,N\} = -\frac\{1\}\{T^2C_v\}$$

where $C_v$ is the heat capacity
$C_v=\left(\frac\{\partial U\}\{\partial T\}\right)_\{V,N\}=T\left(\frac\{\partial S\}\{\partial T\}\right)_\{V,N\}$

Therefore, we have that change in entropy associated with the
spontaneous perturbation $\delta U^\{(1)\}$ is:

$$\Delta S = -\frac\{1\}\{2T^2\}\left(\frac\{1\}\{C_v^\{(1)\}\}+\frac\{1\}\{C_v^\{(2)\}\}\right)\left(\delta U^\{(1)\}\right)^2$$

For the system to remain stable, we must have a negative change in
entropy \[$\Delta S <0$ or $\left(\delta^2 S\right)_\{U,V,N\}<0$\]. This
implies that: $$\label\{Eq6\}
\frac\{1\}\{T^2\}\left(\frac\{1\}\{C_v^\{(1)\}\}+\frac\{1\}\{C_v^\{(2)\}\}\right)>0$$

The subsystem sizes are arbitrary, thus to ensure stability, both
contributions must independently satisfy this inequality. This leaves
the stability criteria for a thermodynamic system to remain stable with
respect to a spontaneous energy fluctuation:

$$\label\{Eq7\}
\frac\{1\}\{T^2C_v\}>0 \hspace\{1.0cm\} \text\{or\} \hspace\{1.0cm\} C_v>0$$

Now consider a closed, isothermal system at equilibrium, whose two
subsystems are separated by a sliding, impenetrable barrier
(Fig. [\[fig:2\]](#fig:2)\{reference-type="ref" reference="fig:2"\}). A
small fluctuation in the boundary position causes a small perturbation
in the volume of the subsystems ($\delta V^\{(1)\} = -\delta V^\{(2)\}$).
Since the boundary is impenetrable, the particle numbers remain fixed
(*i.e.* $\delta N^\{(1)\} = \delta N^\{(2)\}=0$).

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig2.png)\{width="\\linewidth"\}
:::

The closed, isothermal ensemble is governed by the Helmholtz free energy
$F=F(T,V,N)$. The response of the Helmholtz free energy to this
perturbation determines whether the system remains stable.

Following a similar line of reasoning as before, we write the
fluctuation in the free energy as:

$$\begin\{aligned\}
 \label\{Eq8\}
\Delta F &=\left[\{\left(\frac\{\partial F\}\{\partial V\}\right)\}_\{T,N\}^\{(1)\}-\{\left(\frac\{\partial F\}\{\partial V\}\right)\}_\{T,N\}^\{(2)\}\right]\delta V^\{(1)\} 
\notag\\  
& \hspace\{0.5cm\} + \frac\{1\}\{2\}\left[\{\left(\frac\{\partial^2 F\}\{\partial^2 V\}\right)\}_\{T,N\}^\{(1)\}+\{\left(\frac\{\partial^2 F\}\{\partial^2 V\}\right)\}_\{T,N\}^\{(2)\}\right]\{\left(\delta V^\{(1)\}\right)\}^2
\end\{aligned\}$$

where first and second term on the right side correspond to the first
\[$\left(\delta F\right)_\{T,V,N\}$\] and second
\[$\left(\delta^2 F\right)_\{T,V,N\}$\] variations of the Helmholtz free
energy, respectively.

Since the system begins at equilibrium, the first variation is zero. The
second law of thermodynamics implies that the free energy is minimized
at equilibrium, thus the system is stable if $\Delta F>0$. Using the
thermodynamic relation
$\left(\frac\{\partial^2 F\}\{\partial^2 V\}\right)_\{T,N\} = -\left(\frac\{\delta p\}\{\delta V\}\right)_\{T,N\}$,
we get:

$$\label\{Eq9\}
-\left[\left(\frac\{\partial p\}\{\partial V\}\right)_\{T,N\}^\{(1)\}+\left(\frac\{\partial p\}\{\partial V\}\right)_\{T,N\}^\{(2)\}\right] >0$$

Therefore, we have
$\kappa_T=-\frac\{1\}\{V\}\left(\frac\{\partial V\}\{\partial p\}\right)_\{T,N\}>0$
for stability against spontaneous volume (or density) fluctuations.

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig3.png)\{width="\\linewidth"\}
:::

In general $\left(\frac\{\partial T\}\{\partial S\}\right)_\{V,N\}>0$,
$-\left(\frac\{\partial p\}\{\partial V\}\right)_\{T,N\}>0$ or
$\left(\frac\{\partial \mu_i\}\{\partial N_i\}\right)_\{T,V,N_\{j \neq i\}\}>0$,
represent a set of stability criteria against spontaneous energy ,
volume (or density), or matter flow of species *i* fluctuations,
respectively. These criterion always involve derivatives of intensive
properties with respect to their conjugate extensive properties.
However, these stability concepts, which derive from the second law of
thermodynamics, say nothing about the sign of:

$$\label\{Eq10\}
\left(\frac\{\partial p\}\{\partial T\}\right)_\{V,N\} \hspace\{1.0cm\} \text\{and\} \hspace\{1.0cm\} \left(\frac\{\partial \mu_i\}\{\partial N_k\}\right)_\{T,V,N_\{j \neq k\}\}$$

since the variables are not conjugate to each other
(Fig [\[fig:3\]](#fig:3)\{reference-type="ref" reference="fig:3"\}).

## Phase Equilibrium

In order to describe the phase equilibrium we need a model that property
describes the vapor and the liquid phases. One popular model is the Van
der Waals equation of state given by:

$$\begin\{aligned\}
 \label\{Eq11\}
p = \frac\{NRT\}\{V-Nb\}-\frac\{aN^2\}\{V^2\}= \frac\{RT\}\{v-b\}-\frac\{a\}\{v^2\}
\end\{aligned\}$$

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig4.png)\{width="\\linewidth"\}
:::

where $v=V/N$ is the molar volume.

Although $a$ and $b$ are considered empirical constant, there are
microscopic justifications for the van der Waals constants:

-   The constant $b$ represents the hard-core excluded volume of the
    molecules in the system.

-   The constant $a$ determines the strength of the two-body attractive
    interaction between molecules in the system

For simplicity, we write the dimensionless equation of state:

$$\begin\{aligned\}
\label\{Eq12\}
\tilde\{p\} = \frac\{1\}\{\tilde\{v\} - 1\} - \frac\{\tilde\{a\}\}\{\{\tilde\{v\}\}^2\}
\end\{aligned\}$$

where $\tilde\{p\}=\frac\{pb\}\{RT\}$, $\tilde\{v\}=\frac\{v\}\{b\}$, and
$\tilde\{a\}=\frac\{a\}\{bRT\}$, thus reducing the number of parameters to
just $\tilde\{a\}$ to capture the temperature dependence.

Consider the closed, isothermal system containing a van der Waals fluid
in vapor-liquid equilibrium
(Fig. [\[fig:4\]](#fig:4)\{reference-type="ref" reference="fig:4"\}). Our
goals are to find the limits of stability of each phase, the conditions
where the phases coexist, and the properties of the two phases in
coexistence. The simple example of phase coexistence demonstrates the
essential issues at work in more complex, multi-component, multiphase
systems.

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig5.png)\{width="\\linewidth"\}
:::

From the plot of the pressure as a function of volume for a given
$\tilde\{a\}$ (Fig. [\[fig:5\]](#fig:5)\{reference-type="ref"
reference="fig:5"\}), we can observe a region of large volume which is
the vapor phase, a region of liquid phase with low volumes, and a region
in between. This region in between is in violation of thermodynamic
stability, which requires that
$\kappa_T=-\frac\{1\}\{v\}\left(\frac\{\partial v\}\{\partial p\}\right)_\{T\}>0$.

The **limit of stability** of a single pure phase occurs when:

$$\left(\frac\{\partial \tilde\{p\}\}\{\partial \tilde\{v\}\}\right)_\{\tilde\{a\}\} = -\frac\{1\}\{(\tilde\{v\}-1)^2\}+\frac\{2\tilde\{a\}\}\{\{\tilde\{v\}\}^3\}=0 \notag$$

This gives the cubic equation:
$$-\{\tilde\{v\}\}^3+2\tilde\{a\}\left(\{\tilde\{v\}\}^2-2\tilde\{v\}+1\right)=0 \notag$$

Equation ([\[Eq12\]](#Eq12)\{reference-type="ref" reference="Eq12"\}) has
three solutions:

-   The first solution is less than one ($\tilde\{v\}<1$) and is
    unphysical since the pressure $\tilde\{p\}$ diverges as
    $\tilde\{v\}\rightarrow1_+$.

-   The two solutions greater than $\tilde\{v\}=1$ identify the limits of
    stability of the vapor and liquid phases.

The two solutions are approach each other with decreasing $\tilde\{a\}$,
until that meet at the critical point:

$$\tilde\{v\}_c=3 \hspace\{1.0cm\} \text\{and\} \hspace\{1.0cm\} \tilde\{a\}_c=\frac\{27\}\{8\} \notag$$

In order to avoid this violation of thermodynamic equilibrium, the
system undergoes to a state of **phase coexistence**. In order to find
the properties of this coexisting phase we consider the equilibrium
conditions $\mu^\{(v)\}=\mu^\{(l)\}$ and $p^\{(v)\}=p^\{(l)\}=p_\{coex\}$.

The molar Helmholtz free energy is written as: $$\begin\{aligned\}
\label\{Eq13\}
\tilde\{f\}&=\frac\{F\}\{NRT\}=\int\left(\frac\{\partial \tilde\{f\}\}\{\partial \tilde\{v\}\}\right)_\{\tilde\{a\}\}d\tilde\{v\} + \tilde\{f\}_0 \notag \\ 
&=-\int \tilde\{p\}d\tilde\{v\} + \tilde\{f\}_0
\end\{aligned\}$$

where $\tilde\{f_0\}$ is an unspecified constant of integration (reference
state).

On the other hand, we can derive an expression for the chemical
potential from the Gibbs free energy. The Euler equation for a
one-component system is: $$N\mu = U-ST +pV = F+pV=G \notag$$

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig6.png)\{width="\\linewidth"\}
:::

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig9.png)\{width="\\linewidth"\}
:::

or in dimensionless form, we have: $$\label\{Eq14\}
\tilde\{\mu\} = \frac\{\mu\}\{RT\}= \tilde\{f\}+\tilde\{p\}\tilde\{v\}$$

Since the system is in equilibrium the chemical potential of the vapor
and liquid phases must be equal
($\{\tilde\{\mu\}\}^\{(v)\}=\{\tilde\{\mu\}\}^\{(l)\}$), which according to
([\[Eq13\]](#Eq13)\{reference-type="ref" reference="Eq13"\}) and
([\[Eq14\]](#Eq14)\{reference-type="ref" reference="Eq14"\}) is equivalent
to: $$\label\{Eq31\}
\int_\{\{\tilde\{v\}\}^\{(l)\}\}^\{\{\tilde\{v\}\}^\{(v)\}\}\tilde\{p\}(\tilde\{v\})d\tilde\{v\} = \tilde\{p\}_\{coex\}\left(\{\tilde\{v\}\}^\{(v)\}-\{\tilde\{v\}\}^\{(l)\}\right)$$

As written, this leads us to the Maxwell construction for vapor-liquid
phase coexistence, which provides a graphical method of identifying
phase coexistence.

A second way to view the coexistence condition is to note:
$$\begin\{aligned\}
\label\{Eq33\}
&\tilde\{\mu\}^\{(v)\} = \tilde\{\mu\}^\{(l)\} \rightarrow \tilde\{f\}^\{(v)\} -\frac\{\partial \tilde\{f\}^\{(v)\}\}\{\partial \tilde\{v\}\}\tilde\{v\}^\{(v)\}= \rightarrow \tilde\{f\}^\{(l)\} -\frac\{\partial \tilde\{f\}^\{(l)\}\}\{\partial \tilde\{v\}\}\tilde\{v\}^\{(l)\} \notag\\
&\tilde\{p\}^\{(v)\} = \tilde\{p\}^\{(l)\} \rightarrow \frac\{\partial \tilde\{f\}^\{(v)\}\}\{\partial \tilde\{v\}\} = \frac\{\partial \tilde\{f\}^\{(l)\}\}\{\partial \tilde\{v\}\}
\end\{aligned\}$$ This development gives the common-tangent construction
(Fig. [\[fig:6\]](#fig:6)\{reference-type="ref" reference="fig:6"\},
[\[fig:9\]](#fig:9)\{reference-type="ref" reference="fig:9"\}).

The most direct method of visualizing the coexistence condition is plot
$\tilde\{\mu\}$ versus $\tilde\{p\}$. Numerically calculating the
equilibrium conditions is easiest using the plot of $\tilde\{\mu\}$ versus
$\tilde\{p\}$, since the conditions involve a single point of crossing
(Fig. [\[fig:7\]](#fig:7)\{reference-type="ref" reference="fig:7"\},
[\[fig:8\]](#fig:8)\{reference-type="ref" reference="fig:8"\}).

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig7.png)\{width="\\linewidth"\}
:::

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig8.png)\{width="\\linewidth"\}
:::

::: marginfigure
![image](figures/part2a/statistical_mechanics/phase_equilibrium/Fig10.png)\{width="\\linewidth"\}
:::

The conditions on the $\tilde\{v\}-\tilde\{p\}$ plane that correspond to the
coexistence points form the binodal curves. The limit of stability of
the pure vapor and pure liquid phases form the spinodal curves. These
curve form the phase diagram, which predicts the conditions of
coexistence of the vapor and liquid phases
(Fig. [\[fig:10\]](#fig:10)\{reference-type="ref" reference="fig:10"\}).


# Polymers \{#chap:polymers\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:phase_equilibrium\]](#chap:phase_equilibrium)\{reference-type="ref+label"
reference="chap:phase_equilibrium"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

The term polymer is used to describe molecules which consist of a large
number of repeating units connected by covalent chemical bonds. Some
polymers are linear, like beads on a necklace; others are branched.
Statistical mechanics has a prominent role in predicting the properties
of polymers. For example, stretching the polymer chains lowers their
conformational entropy, and so under no external force conditions, the
chain naturally tend to retract to regain entropy. The entropy is also
lowered when polymers become compact, as when proteins fold or when DNA
becomes encapsulated within virus heads. Additionally the changes on the
end-to end length of polymer molecules due stretching, can be used to
interpret the viscosities of polymer solutions, the scattering of light,
and some of the dynamic properties of polymer solutions.

In order to understand the thermodynamic behavior due to polymer
stretching, a proper characterization of the random changes in length
needs to be considered. For this, we will consider the *random flight*
model. This model has the advantage that in the case of long chains, it
can be approximated as a Gaussian distribution, and so simplifying the
analysis of the statistical mechanics.

## Polymers and proteins

Polymer molecules are chains of covalently bonded monomer units
(Fig. [\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/polymers/fig1.png)\{width="\\linewidth"\}
![image](figures/Physics/statistical_mechanics/polymers/fig2.png)\{width="\\linewidth"\}
:::

Proteins are polymers that are composed of specific sequence of amino
acids that are linked by peptide bonds. Nature has exquisite control
over primary structure of proteins (*i.e.* amino acid sequence),
facilitating the manufacture of proteins that fold into precise
structures with specific biological functions. There are 20 natural
amino acids that are used to construct biological proteins. The diverse
chemical functionality of these amino acids provides the flexibility to
create enzymes and structural proteins that engage in a wide range of
biological processes (Fig. [\[fig8\]](#fig8)\{reference-type="ref"
reference="fig8"\}).

## Physical behavior at different length scales

The chemical structure of a polymer dictates the atomic level physical
behavior. Thermal fluctuations at small length scales result in local
deformations that are randomly distributed about a mean structure with a
finite variance. At larger length scales, the correlation between the
fluctuating chain segments vanishes, and the behavior is that of many
independently fluctuating chain segments.

The Central Limit Theorem states that any sum of many independent
identically distributed random variables will tend to a normal (or
Gaussian) distribution, provided the sum of variables has a finite
variance.

::: marginfigure
![image](figures/Physics/statistical_mechanics/polymers/fig10.png)\{width="\\linewidth"\}
:::

Various models for the small-length-scale physical behavior of a polymer
exist (see Fig.4). Though they differ in their small length scale
behavior, they all tend to a Gaussian distribution at large length
scales. We focus on a discrete random walk (or a freely jointed chain)
to demonstrate the crossover to a Gaussian distribution.

<figure id="fig11">
<p><img src="figures/Physics/statistical_mechanics/polymers/fig11.png"
alt="image" /> Figure : Several ideal chain models. <span id="fig11"
data-label="fig11"></span></p>
</figure>

The Ideal random-walk (or freely jointed) polymer chain is the simplest
idealization of a flexible polymer. The polymer does not interact with
itself; we can think of it as a phantom chain. This framework is
mathematically useful since random walks appear in a range of seemingly
different physical processes (diffusion, heat transfer, quantum
mechanics, etc).

## Entropic elasticity

Consider a walk on a one-dimensional lattice. After $N$ steps, how far
do you travel on average?

$$\begin\{aligned\}
\label\{Eq1\}
X = s_1+s_2+...+s_N=\sum_\{i=1\}^Ns_i
\end\{aligned\}$$

where $s_i = \pm 1$ is the jump vector. The mean-square end displacement
is: $$\begin\{aligned\}
\label\{Eq2\}
\langle X^2\rangle = \sum_\{i=1\}^N\sum_\{j=1\}^N\langle s_is_j\rangle
\end\{aligned\}$$

Since jumps are uncorrelated, we have: $$\begin\{aligned\}
\label\{Eq3\}
\langle s_i s_j \rangle = \delta_\{ij\}
\end\{aligned\}$$

hwere $\delta_\{ij\}$ if $i=j$ and $\delta_\{ij\}=0$ if $i\ne j$. Therefore
$$\begin\{aligned\}
\label\{Eq4\}
\langle X^2\rangle = \sum_\{i=1\}^N\sum_\{j=1\}^N \delta_\{ij\} = \sum_\{i=1\}^N 1 =N
\end\{aligned\}$$ This gives the size of a 1-dimensional polymer with
discrete subunits of unit length.

This is extended to 3 dimensions. Consider a chain of $N$ bond vectors
$\vec\{b\}_i$ ($i =1,2,...,N$) with length
$b (\vec\{b\}_i \cdot\vec\{b\}_i =b^2)$, with end-to-end vector
$\vec\{R\} = \sum_\{i=1\}^N\vec\{b\}_i$. The mean-square end-to-end distance
is: $$\begin\{aligned\}
\label\{Eq5\}
\langle \vec\{R\}^2 \rangle = \sum_\{i=1\}^N\sum_\{j=1\}^N\langle\vec\{b\}_i\vec\{b\}_j\rangle = \sum_\{i=1\}^N\sum_\{j=1\}^N b^2\delta_\{ij\}=b^2N
\end\{aligned\}$$

using similar arguments as the 1-dimensional case.

Decomposing the end-to-end vector into its components
$\vec\{R\} = X\hat\{x\}+ Y\hat\{y\}+Z\hat\{z\}$, we have: $$\begin\{aligned\}
\label\{Eq6\}
\langle X^2 \rangle = \langle Y^2 \rangle = \langle Z^2 \rangle = \frac\{b^2N\}\{3\}
\end\{aligned\}$$

since no direction is preferred.

The central limit theorem states that the limiting behavior of any
statistical distribution tends to a Gaussian distribution in the limit
of large sample size. In our case, the limit of large $N$ leads to a
chain probability distribution that is Gaussian. With this, we construct
the polymer chain distribution function.

Generally, a Gaussian distribution for a variable $X$ with zero mean
($\langle X \rangle = 0$) and variance $\langle X^2 \rangle$ is written
as: $$\begin\{aligned\}
\label\{Eq7\}
p_x(X,N)=\frac\{1\}\{\sqrt\{2\pi\langle X^2\rangle\}\}\exp\left( -\frac\{X^2\}\{2\langle X^2 \rangle\} \right)
\end\{aligned\}$$

This is the limiting behavior of a random walk in one dimension as the
number of steps is very large ($N \gg 1$).

Assuming the probability distribution is decoupled in the 3 directions,
we write the 3-dimensional probability distribution as $$\begin\{aligned\}
\label\{Eq8\}
p(\vec\{R\},N) &= p_x(X,N)p_y(Y,N)p_y(Z,N) \notag\\
&=\left(\frac\{2\pi Nb^2\}\{3\}\right)^\{-3/2\}\exp\left( -\frac\{3\vec\{R^2\}\}\{2Nb^2\} \right)
\end\{aligned\}$$

giving the probability that a chain of length $N$ that begins at the
origin will end at position $\vec\{R\}$.

The chain probability gives the free energy for constraining the chain
ends at a fixed position; specifically, we have: $$\begin\{aligned\}
\label\{Eq9\}
F(\vec\{R\})-F(\vec\{0\}) = -k_BT\log\left[ \frac\{p(\vec\{R\},N)\}\{p(\vec\{0\},N)\} \right] = \frac\{3k_BT\}\{2Nb^2\}\vec\{R\}^2
\end\{aligned\}$$

Thus, the chain behaves as a Hookean spring, *i.e* a Gaussian chain
exhibits a linear response to tension. Pulling a Gaussian chain along
the $z$-axis a distance $Z$ requires a tension $\tau$ given by:
$$\begin\{aligned\}
\label\{Eq10\}
\tau = \frac\{\partial F(Z)\}\{\partial Z\} = \frac\{3k_BT\}\{Nb^2\}Z
\end\{aligned\}$$

::: marginfigure
![image](figures/Physics/statistical_mechanics/polymers/fig12.png)\{width="\\linewidth"\}
:::

This linear response governs the small-scale deformation of a polymer
chain. The response is purely entropic, indicated by the linear scaling
in temperature. This leads to the curious behavior for a stretch elastic
band retracts when heated. The Gaussian chain model does not capture the
behavior once the chain is nearly extended. Full extension reflects the
microscopic details of the model; for example, the nearly inextensible
covalent bonds of the polymer chain. However, many polymer properties
are captured by the Gaussian chain model. This is highly advantageous
because the properties tend to be governed by just a few parameters
(Fig. [\[fig12\]](#fig12)\{reference-type="ref" reference="fig12"\}).

## Freely Jointed Model

Consider one link of a polymer chain shown in the
Fig. [\[fjm\]](#fjm)\{reference-type="ref" reference="fjm"\} which has a
fixed length of $b$. The bond angle is given as $\theta$ and the torsion
angle is give as $\phi$, both being free to to rotate. A force of
magnitude $f$ is applied at its ends. Here, we will develop a model for
its streching.

::: marginfigure
![image](figures/Physics/statistical_mechanics/polymers/fjm.png)\{width="\\linewidth"\}
:::

Since each polymer link has a fixed length $b$, the system has only 2
degrees of freedom. When this single link is pulled with a force,
$\vec\{f\} = f \hat\{z\}$, the energy associated with it is given by
$\varepsilon = -\vec\{r\}.\vec\{f\}$. The partition function $q(f)$ would
be:

$$\begin\{aligned\}
q &= \sum \exp(-\beta E) \\
&= \int_0^\{2\pi\}\int_0^\pi\exp[-\beta(-\vec\{r\}\cdot \vec\{f\})]\sin\theta d\theta d\phi\\
&=\int_0^\{2\pi\}\int_0^\pi\exp(\beta zf)\sin\theta d\theta d\phi\\
&=\frac\{2\pi\}\{b\}\int_\{-b\}^\{+b\}\exp(\beta zf) dz\\
&=\frac\{4\pi\}\{bf\}k_BT\sinh\left(\frac\{fb\}\{k_BT\}\right)
\end\{aligned\}$$

Consider now a polymer made of $N$ such links. Since the links are
distinguishable, we can write down the partition function $Q(f,N)$ as
$$Q = q^N$$

Using the above partition function, we can evaluate the Helmholtz free
energy, $F(f,N)$ as:

$$\begin\{aligned\}
F = -k_BT\log Q 
\end\{aligned\}$$

The Helmholtz free energy can be used to calculate the average
displacement $\langle r \rangle$.

Since
$F = -\langle r\rangle f \rightarrow \langle r \rangle = \frac\{\partial F\}\{\partial f\}$,
we have: $$\begin\{aligned\}
\langle r\rangle  &= k_BT\frac\{\partial \log Q\}\{\partial f\} = \frac\{k_BT\}\{q^N\}Nq^\{N-1\}\frac\{\partial q\}\{\partial f\} \\
&= \frac\{k_BTN\}\{q\}\left[ \frac\{4\pi\}\{f\} \cosh\left( \frac\{fb\}\{k_BT\} \right) - \frac\{4\pi\}\{bf^2\} \sinh\left( \frac\{fb\}\{k_BT\} \right) \right] \\
&= \frac\{N\}\{f\}\left[ bf\coth\left( \frac\{fb\}\{k_BT\} \right) - k_BT \right]
\end\{aligned\}$$

::: marginfigure
![image](figures/Physics/statistical_mechanics/polymers/stretch.jpg)\{width="\\linewidth"\}
:::


# Liquids \{#chap:liquids\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ideal_gas\]](#chap:ideal_gas)\{reference-type="ref+label"
reference="chap:ideal_gas"\},
[\[chap:ising_model\]](#chap:ising_model)\{reference-type="ref+label"
reference="chap:ising_model"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Classical fluids are systems of particles which retain a definite
volume, and are at sufficiently high temperatures that quantum effects
can be neglected. Most liquids at normal temperatures do satisfy this
condition, like liquid air, oil and gasoline, as well as electrolytes,
molten salts and salts dissolved in water, which are classified as
classical charged fluids.

In this chapter, we develop a simple model to describe the behavior of
classical fluids. Unlike ideal gases, whose energy states are purely
described by their kinetic energy, the existence of an interactive
potential in the case of liquids adds more complexity, as we now have to
consider the degree of correlation between fluctuations at different
positions. Thus, to determine the distribution function of the
correlations from different locations plays a central role in the
thermodynamic properties of classical liquids. This can be achieved by
x-ray scattering techniques, as we will review at the end of this
chapter.

## Classical Fluids

A system of molecules in fluid and solid phases includes all of the
particles that comprise the molecules, *i.e.* all protons, neutrons, and
electrons. This molecules are often accurately described in terms of the
principles of classical mechanics. The behavior of electrons surrounding
the nucleus indeed, would most likely be quantum mechanical in nature.
However in most cases, electron motion can be averaged over the quantal
fluctuations, leading to effective interactions between nuclei. For
example, the Lennard-Jones interaction potential is constructed to
include two offsetting physical contributions.

-   For short inter-nuclei spacing $r$, the Pauli exclusion principle
    dictates that the energy diverges exponentially as $r\rightarrow0$.
    This is approximated in the Lennard-Jones potential as $1/r^\{12\}$.

-   For large $r$, the electronic polarizability of the neutral atom
    leads to an attractive interaction that scales as $1/r^6$; such
    forces are called *Van der Waal's forces*.

The Lennard-Jones potential $V_\{LJ\}(r)$ is given by
(Fig. [\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}):
$$\begin\{aligned\}
\label\{Eq1\}
V_\{LJ\}(r)=\nu_0\left[\left(\frac\{\sigma\}\{r\}\right)^\{12\} - \left(\frac\{\sigma\}\{r\}\right)^\{6\}\right]
\end\{aligned\}$$

where $r$ is the inter-nuclei spacing, $\nu_0$ is a potential scaling
term, and $s$ is a length scale of the interactions (related to the
polarizability of the atoms).

Generally, this procedure ignores the coupling between the electron and
nucleus kinetic energies; this approximation is valid in the limit
$m_e/M\rightarrow 0$ ($m_e \equiv$ electron mass and $M \equiv$ nuclear
mass). An important exception where a classical model of a fluid is not
acceptable is low temperature helium

::: marginfigure
![image](figures/Physics/statistical_mechanics/liquids/Fig1.png)\{width="\\linewidth"\}
:::

Upon integrating over the electronic motion, we are left with a set of
effective particles that interact via an interaction potential, thus
defining a classical model for the interactions.

For an $N$ particle system, the classical degrees of freedom include the
coordinate positions $$\begin\{aligned\}
 \{\vec\{r\}\} = \vec\{r\}_1, \vec\{r\}_2,...,\vec\{r\}_N 
\end\{aligned\}$$ and momenta $$\begin\{aligned\}
 \{\vec\{p\}\} = \vec\{p\}_1, \vec\{p\}_2,...,\vec\{p\}_N 
\end\{aligned\}$$ thus, the microscopic state of the system is defined by
the point in phase space $\{ \vec\{p\}, \vec\{r\} \}$. For simplicity, we
will denote these terms respectively by $\vec\{p\}, \vec\{r\}$ when the
context is clear.

The thermodynamic behavior is governed by the partition function

$$\begin\{aligned\}
Q = \sum_\mu \exp\left(-\frac\{E_\mu\}\{k_BT\}\right) 
\end\{aligned\}$$ where the summation over $\mu$ implies a sum over
quantum states. The energy for a point in phase space is the Hamiltonian
$H$, *i.e.*: $$\begin\{aligned\}
\label\{Eq2\}
E_\mu \rightarrow H(\vec\{p\}, \vec\{r\}) = K(\vec\{p\}) + U(\vec\{r\})
\end\{aligned\}$$

where $$\begin\{aligned\}
K(\vec\{p\}) &= \sum_\{i=1\}^N\frac\{\vec\{p\}_i^2\}\{2m\}
\end\{aligned\}$$ is the total kinetic energy, and $U(\vec\{r\})$ is the
effective classical interaction potential. Points in phase space form a
continuum, so the classical partition function must be:
$$\begin\{aligned\}
Q &= A\int d\vec\{r\}_1d\vec\{r\}_2...d\vec\{r\}_N  \int d\vec\{p\}_1d\vec\{p\}_2...d\vec\{p\}_N   \exp\left[-\beta H(\vec\{p\},\vec\{r\})\right] 
\end\{aligned\}$$

where $A$ is a factor to ensure that $Q$ is dimensionless. (Note that we
use the alternative style of writing the integral with integration
factors at the front rather than at the end for clarity. In general, we
will use the mathematical style for integrals with integration factors
at the end but may occasionally use the physics style for clarity of
expression.)

The factor $A$ corrects for the approximation that the phase space is a
continuum, thus $A$ is a universal parameter that does not depend on the
Hamiltonian $H$.

We find $A$ by considering the ideal gas Hamiltonian $H = K$, giving the
partition function: $$\begin\{aligned\}
Q &= AV^N\left[\int_\{-\infty\}^\infty  \int_\{-\infty\}^\infty \int_\{-\infty\}^\infty  \exp \left(-\frac\{\beta\vec\{p\}^ 2\}\{2m\}\right) dp_z dp_y dp_x \right]^N\\
&= AV^N\left(\frac\{2\pi m\}\{\beta\}\right)^\{3N/2\} \\ 
&= \frac\{1\}\{N!\}\frac\{V^N\}\{\Lambda^\{3N\}\} 
\end\{aligned\}$$

where the final expression is taken from the translational partition
function of ideal gases , and considering that the particles are
indistinguishable. $$\begin\{aligned\}
\Lambda &= \sqrt\{\frac\{h^2\}\{2\pi mk_B T\}\}
\end\{aligned\}$$ is the thermal de Broglie wavelength.

Using this equality, we write $$\begin\{aligned\}
A &= \frac\{1\}\{N!\}\frac\{1\}\{h^\{3N\}\},
\end\{aligned\}$$ and the classical partition function is written as:
$$\begin\{aligned\}
Q &= \frac\{1\}\{N!h^\{3N\}\}\int d\vec\{r\}_1d\vec\{r\}_2...d\vec\{r\}_N \int d\vec\{p\}_1d\vec\{p\}_2...d\vec\{p\}_N \exp\left[-\beta H(\vec\{p\},\vec\{r\})\right] \\ 
&= \frac\{1\}\{N!\Lambda^\{3N\}\}\int d\vec\{r\}_1d\vec\{r\}_2...d\vec\{r\}_N \exp[-\beta U(\vec\{r\})]  \\
&= \frac\{1\}\{N!\}\frac\{1\}\{\Lambda^\{3N\}\}Z(T,V,N)  
\end\{aligned\}$$

where we define the configurational partition function $Z(T,V,N)$, thus
leaving only a configurational integral to perform.

The probability of finding the system at locations between $\{\vec\{r\}\}$
and $\{\vec\{r\} + d\vec\{r\}\}$ and momenta between $\{\vec\{p\}\}$ and
$\{\vec\{p\} + d\vec\{p\}\}$ system is: $$\begin\{aligned\}
f(\vec\{p\},\vec\{r\})d\vec\{r\}_1d\vec\{r\}_2...d\vec\{r\}_Nd\vec\{p\}_1d\vec\{p\}_2...d\vec\{p\}_N 
\end\{aligned\}$$

where $f(\vec\{p\},\vec\{r\})$ is the probability distribution for observing
a system at phase space point $\{\vec\{p\},\vec\{r\}\}$.

In this case, the probability distribution has the specific form
$$\begin\{aligned\}
f(\vec\{p\},\vec\{r\}) = \frac\{1\}\{Qh^\{3N\}N!\}\exp[-\beta H(\vec\{p\},\vec\{r\})] 
\end\{aligned\}$$

The Hamiltonian is $$\begin\{aligned\}
H(\vec\{p\},\vec\{r\}) = K(\vec\{p\}) + U(\vec\{r\}),
\end\{aligned\}$$ thus we can represent the phase space distribution as
$$\begin\{aligned\}
f(\vec\{p\},\vec\{r\}) &= \frac\{1\}\{Qh^\{3N\}N!\}e^\{-\beta K(\vec\{p\})\}e^\{-\beta U(\vec\{r\})\} \\
&= \frac\{1\}\{h^\{3N\}N!\}\frac\{e^\{-\beta K(\vec\{p\})\}\}\{\int dp^Ne^\{-\beta K(\{ \vec\{p\}\})\}\}\frac\{e^\{\beta U(\vec\{r\})\}\}\{\int dr^Ne^\{\beta U(\vec\{r\})\}\} \\
&= \frac\{1\}\{h^\{3N\}N!\}\Phi(p^N)P(r^N)
\end\{aligned\}$$

where $\Phi(p^N)$ and $P(r^N)$ define the probability distribution for
observing system at momentum space point $p^N$ and configuration space
point $r^N$, respectively.

We integrate the last expression over positions and momenta of particles
2 to N to get: $$\begin\{aligned\}
\int d\vec\{r\}^N \int d\vec\{p\}_j \prod_\{j=2\}^N f(\vec\{p\}, \vec\{r\})d\vec\{p\}_1 &= \frac\{1\}\{h^\{3N\}N!\} d\vec\{p\}_1 \int d\vec\{p\}_j \prod_\{j=2\}^N \Phi(p^N) \\ 
&=\frac\{1\}\{h^\{3N\}N!\} \phi(p_1) d\vec\{p\}_1 \prod_\{j=2\}^N \int d\vec\{p\}_j \phi(p_j) \\
&= \frac\{1\}\{h^\{3N\}N!\} \frac\{e^\{-\beta K(\vec\{p\}_1)\}\}\{\int d\vec\{p\}_1 e^\{-\beta K( \vec\{p\}_1)\}\} d\vec\{p\}_1 \\
&=(2\pi m k_B T)^\{3/2\} e^\{-\frac\{\vec\{p\}_1^2\}\{2mk_BT\}\}d\vec\{p\}_1
\end\{aligned\}$$

which is the Maxwell Distribution, that gives the probability that
particle 1 has momentum between $\vec\{p\}_1$ and
$\vec\{p\}_1 + d\vec\{p\}_1$.

We can rearrange the last expression, which originally considers the
differential over the three coordinates of $\vec\{p\}_1$, into a more
compact representation in spherical coordinates, such that
$d\vec\{p\}_1 = dp_\{x_1\}dp_\{y_1\}dp_\{z_1\} = p_1^2dp_1$. Additionally, if we
define the expression in terms of the velocity, we obtain a simpler form
of the Maxwell Distribution, giving the probability of finding a
particle with speed between $v$ and $v + dv$, given by:
$$\begin\{aligned\}
\label\{Eq3\}
f(v)dv = 4\pi m^3 (2\pi mk_BT)^\{3/2\}\exp\left(-\frac\{m\}\{2k_BT\}v^2\right)v^2dv
\end\{aligned\}$$

The most probable speed is $\nu^*= \sqrt\{2k_BT/m\}$.

## *Equipartition Theorem*

The equipartition theorem dictates that each thermally active degree of
freedom is partitioned a fixed energy.

Assume the Hamiltonian can be diagonalized into $m$ degree of freedom
using a general coordinate system $\{q\} = q_1,q_2,...,q_m$, *e.g.* the
phonon modes in a crystal, giving the Hamiltonian:

$$\begin\{aligned\}
\label\{Eq4\}
H = \frac\{1\}\{2\}\sum_\{i=1\}^na_ip_i^2 + \frac\{1\}\{2\}\sum_\{j=1\}^mb_jq_j^2
\end\{aligned\}$$

Using the partition function, we can write the average:
$$\begin\{aligned\}
\langle a_i p_i^2\rangle = \frac\{\int_\{-\infty\}^\infty a_ip_i^2e^\{-\frac\{1\}\{2\}\beta a_ip_i^2\}dp_i\}\{\int_\{-\infty\}^\infty e^\{-\frac\{1\}\{2\}\beta a_ip_i^2\}dp_i\} = \frac\{1\}\{2\}k_BT 
\end\{aligned\}$$

with a similar expression giving
$\langle b_jq_j^2\rangle = \frac\{1\}\{2\}k_BT$.

Thus, the average energy is
$\langle E \rangle = \langle H \rangle = \frac\{1\}\{2\}nk_BT + \frac\{1\}\{2\}mk_BT$.
For a monoatomic gas, $m=0$ and $n=3N$ ($N$ particles and 3 directions),
giving $\langle E\rangle = \frac\{3\}\{2\}Nk_BT$. For vibration in a solid,
$m=3N$ and $n=3N$, giving $\langle E \rangle = 3Nk_BT$.

## Pair distribution function

::: marginfigure
![image](figures/Physics/statistical_mechanics/liquids/Fig2a.png)\{width="\\linewidth"\}
[]\{#fig2a label="fig2a"\}
:::

::: marginfigure
![image](figures/Physics/statistical_mechanics/liquids/Fig2b.png)\{width="\\linewidth"\}
:::

The configurational distribution $P(r^N)$, cannot be factored into
single partition functions, as we did with $\Phi(p^N)$ above, because
the potential energy $U(\vec\{r\})$, couples together all of the
coordinates. However, we can analyze the distribution function for a
small number of particles by integrating over all coordinates except
those corresponding to the particles of interest. For example, define
the reduced probability distribution for a pair of particles:
$$\begin\{aligned\}
p^\{2,N\}(\vec\{r\}_1,\vec\{r\}_2) = \frac\{1\}\{Z\}\int d\vec\{r\}_3\int d\vec\{r\}_4\dotsc \int
d\vec\{r\}_N\exp[-\beta U(\vec\{r\})] 
\end\{aligned\}$$

giving the joint probability of finding particle 1 at $\vec\{r\}_1$ and
particle 2 at $\vec\{r\}_2$.

Since we don't care about the particle identity, the joint probability
$$\begin\{aligned\}
\rho^\{(2,N)\}(\vec\{r\}_1,\vec\{r\}_2) &= N(N-1)p^\{(2,N)\}(\vec\{r\}_1,\vec\{r\}_2),
\end\{aligned\}$$ giving the probability for finding any one particle at
$\vec\{r\}_1$ and any other particle at $\vec\{r\}_2$. Generally, we can
write the $n$-particle joint probability as: $$\begin\{aligned\}
\rho^\{(n,N)\}(\vec\{r\}_1,\vec\{r\}_2,\dotsc,\vec\{r\}_n) = \frac\{N!\}\{(N-n)!\}\frac\{1\}\{Z\}\int\prod_\{i=n+1\}^Nd\vec\{r\}_i\exp [-\beta U(\vec\{r\})] 
\end\{aligned\}$$

For an isotropic fluid,

$$\begin\{aligned\}
 \rho^\{(1,N)\}(\vec\{r\}_1) = \frac\{N\}\{V\} = \rho
\end\{aligned\}$$

For a non-interacting system (ideal gas): $$\begin\{aligned\}
\rho^\{(2,N)\}(\vec\{r\}_1,\vec\{r\}_2) &= \rho^\{(1,N)\}(\vec\{r\}_1)\rho^\{(1,N)\}(\vec\{r\}_2)\\
&= \frac\{N(N-1)\}\{V^2\} \approx \frac\{N^2\}\{V^2\} = \rho^2 
\end\{aligned\}$$

To measure correlations, define the pair distribution function:
$$\begin\{aligned\}
\label\{Eq5\}
g(\vec\{r_1\},\vec\{r_2\}) = \frac\{\rho^\{(2,N)\}(\vec\{r\}_1,\vec\{r\}_2)\}\{\rho^2\}
\end\{aligned\}$$

The fractional deviation from ideal gas approximation is:
$$\begin\{aligned\}
\label\{Eq6\}
h(\vec\{r\}_1,\vec\{r\}_2)=g(\vec\{r\}_1,\vec\{r\}_2) - 1
\end\{aligned\}$$

For an isotropic fluid the pair distribution function only depends on
$|\vec\{r\}_1 -\vec\{r\}_2| = r$, thus $g(\vec\{r\}_1,\vec\{r\}_2) = g(r)$ and
$h(\vec\{r\}_1,\vec\{r\}_2) = h(r)$.

The pair distribution function gives $\rho g(r)$, the average density of
particles at a distance $r$ given that a tagged particle is at the
origin (Fig. [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}).

## *Measurement of $g(r)$ by diffraction*

Consider a collection of $N$ particles in a fluid. To probe distances
$\sim 1\AA$, irradiate the sample with X-rays or neutrons; our
discussion will assume X-ray scattering. A beam with wavelength
$\lambda$ illuminates the sample. The incident beam is coherent, *i.e.*
all photons are in-phase (Fig. [\[fig3\]](#fig3)\{reference-type="ref"
reference="fig3"\}). Due to a difference between the refractive index of
the particles and their surroundings, each particle acts as a scattering
center

::: marginfigure
![image](figures/Physics/statistical_mechanics/liquids/Light scattering.png)\{width="\\linewidth"\}
:::

Compare the pathlengths for scattering from 2 different particles
(Fig. [\[fig4\]](#fig4)\{reference-type="ref" reference="fig4"\}). The
position of the $i$th and $j$th particles are located at $\vec\{r\}_i$ and
$\vec\{r\}_j$, respectively. The difference in the pathlength of the 2
photons in Fig. [\[fig4\]](#fig4)\{reference-type="ref" reference="fig4"\}
is given by the difference between lengths $B$ and $A$, since these are
the only segments that differ between the two photons.

Define the incident wavevector: $$\begin\{aligned\}
\label\{Eq7\}
\vec\{q\}_0 = \frac\{2\pi n\}\{\lambda\}\vec\{u\}_0
\end\{aligned\}$$

where $n$ is the index of refraction of the solution, and $\vec\{u\}_0$ is
a unit vector that defines the direction of the incident light.
Similarly, define the scattered wavevector:

$$\begin\{aligned\}
\label\{Eq8\}
\vec\{q\}_s = \frac\{2\pi n\}\{\lambda\}\vec\{u\}_s
\end\{aligned\}$$

The fixed detector orientation relative to the source is defined by the
angle $\theta$.

Using these definitions, the two lengths $B$ and $A$ are given:
$$\begin\{aligned\}
\label\{Eq9\}
B = (\vec\{r\}_i - \vec\{r\}_j)\cdot \vec\{u\}_s
\end\{aligned\}$$

::: marginfigure
:::

$$\begin\{aligned\}
\label\{Eq10\}
A = (\vec\{r\}_i - \vec\{r\}_j)\cdot \vec\{u\}_0
\end\{aligned\}$$

The pathlength difference is given by: $$\begin\{aligned\}
\label\{Eq11\}
B - A = (\vec\{r\}_i - \vec\{r\}_j)\cdot (\vec\{u\}_s - \vec\{u\}_0)
\end\{aligned\}$$

This results in a phase difference given by: $$\begin\{aligned\}
\label\{Eq12\}
\phi_\{ij\} = \frac\{2\pi n\}\{\lambda\}(B - A) = \frac\{2\pi n\}\{\lambda\}(\vec\{u\}_s - \vec\{u\}_0) \cdot (\vec\{r\}_i - \vec\{r\}_j)
\end\{aligned\}$$

Define the scattering vector $\vec\{q\}$ as:

$$\begin\{aligned\}
\label\{Eq13\}
\vec\{q\} = \frac\{2\pi n\}\{\lambda\}(\vec\{u\}_s - \vec\{u\}_0)
\end\{aligned\}$$

whose magnitude is: $$\begin\{aligned\}
\label\{Eq14\}
q = |\vec\{q\}| = \frac\{4\pi n\}\{\lambda\}\sin\left(\frac\{\theta\}\{2\}\right)
\end\{aligned\}$$

We have: $$\begin\{aligned\}
\label\{15\}
\phi_\{ij\} = \vec\{q\}\cdot (\vec\{r\}_i - \vec\{r\}_j)
\end\{aligned\}$$

The electric field of light scattered by the $i$th particle relative to
the phase of light scattered by the $k$th particle is: $$\begin\{aligned\}
\label\{Eq16\}
E_i = E_0C \cos(2\pi\nu t - \phi _\{ik\})
\end\{aligned\}$$

where $n$ is the frequency, $E_0$ is the amplitude of the incident
electric field, and $C$ incorporates a number of experiment-specific
parameters (*e.g.* distance to detector, wavelength, polarizability).
The intensity of the scattered light is proportional to the total
electric field squared. The average scattering intensity $I_s$ is given
by the intensity averaged over a complete oscillation period and is
given by: $$\begin\{aligned\}
\label\{Eq17\}
I_s &= 2I_0C^2\nu\int_0^\{1/\nu\}dt\left[ \sum_\{i=1\}^N\cos(2\pi\nu t - \phi_\{ik\}) \right]^2 \\
&= 2I_0C^2\nu\int_0^\{1/\nu\}dt \sum_\{i=1\}^N \sum_\{j=1\}^N\cos(2\pi\nu t - \phi_\{ik\})\cos(2\pi\nu t - \phi_\{jk\}) \\
&= 2I_0C^2\nu\int_0^\{1/\nu\}dt \sum_\{i=1\}^N \sum_\{j=1\}^N[\cos(4\pi\nu t - \phi_\{ik\}- \phi_\{jk\}) + \cos(\phi_\{ik\}- \phi_\{jk\})]
\end\{aligned\}$$

which makes use of the property:

$$\begin\{aligned\}
\label\{Eq18\}
\cos A \cos B = \frac\{1\}\{2\}\cos(A + B) + \frac\{1\}\{2\}\cos (A- B)
\end\{aligned\}$$

Noting that: $$\begin\{aligned\}
\label\{Eq19\}
\int_0^\{1/\nu\}\cos(4\pi\nu t - \phi_\{ik\} - \phi_\{jk\}) dt =0
\end\{aligned\}$$

and, $$\begin\{aligned\}
\label\{Eq20\}
\phi_\{ik\} - \phi_\{jk\} = \phi_\{ij\}
\end\{aligned\}$$

we have, $$\begin\{aligned\}
\label\{Eq21\}
I_s &= I_0C^2\sum_\{i=1\}^N\sum_\{j=1\}^N\cos\phi_\{ij\} \\
 &= I_0C^2\sum_\{i=1\}^N\sum_\{j=1\}^N\cos[\vec\{q\}\cdot(\vec\{r\}_i - \vec\{r\}_j)]
\end\{aligned\}$$

Measuring $I_s$ at zero scattering angle ($\theta = 0$) gives the
unknown parameters $I_0$ and $A$. Therefore,

$$\begin\{aligned\}
\label\{Eq22\}
\frac\{I_s(\vec\{q\})\}\{I_s(\vec\{q\}=\vec\{0\})\} = \frac\{1\}\{N^2\}\sum_\{i=1\}^N\sum_\{j=1\}^N\cos[\vec\{q\}\cdot(\vec\{r\}_i - \vec\{r\}_j)]
\end\{aligned\}$$

Define the structure factor $S(\vec\{k\})$ to be: $$\begin\{aligned\}
\label\{Eq23\}
S(\vec\{k\}) = \frac\{1\}\{N^2\}\sum_\{i=1\}^N\sum_\{j=1\}^N \bigg\langle \exp\left[ i\vec\{k\}\cdot (\vec\{r\}_i - \vec\{r\}_j) \right] \bigg\rangle
\end\{aligned\}$$

we can write:

$$\begin\{aligned\}
\label\{Eq24\}
\frac\{I_s(\vec\{q\})\}\{I_s(\vec\{q\}=\vec\{0\})\} = Re[S(\vec\{q\})]
\end\{aligned\}$$

Rotational invariance gives $S(\vec\{k\}) = S(-\vec\{k\})$, thus:

$$\begin\{aligned\}
\label\{Eq25\}
\frac\{I_s(\vec\{q\})\}\{I_s(\vec\{q\}=\vec\{0\})\} = S(\vec\{q\})
\end\{aligned\}$$

Expand the structure factor into $i = j$ and $i \neq j$. This gives:
$$\begin\{aligned\}
S(\vec\{k\}) & = \frac\{1\}\{N^2\}N + \frac\{1\}\{N^2\}N(N-1)\bigg\langle \exp\left[ i\vec\{k\}\cdot (\vec\{r\}_1 - \vec\{r\}_2) \right] \bigg\rangle \\
& = \frac\{1\}\{N\} + \frac\{1\}\{N^2\}\frac\{N(N-1)\int\prod_\{i=1\}^Nd\vec\{r\}_ie^\{i\vec\{k\}\cdot (\vec\{r\}_1 - \vec\{r\}_2)\}e^\{-\beta U\}\}\{\int\prod_\{i=1\}^Nd\vec\{r\}_ie^\{-\beta U\}\}\\
&= \frac\{1\}\{N\} + \frac\{1\}\{N^2\} \int d\vec\{r\}_1d\vec\{r\}_2\rho^\{(2,N)\}(\vec\{r\}_1,\vec\{r\}_2)e^\{i\vec\{k\}\cdot (\vec\{r\}_1 - \vec\{r\}_2)\} \\
&= \frac\{1\}\{N\} + \frac\{1\}\{N^2\} \int d\vec\{r\}_1d\vec\{r\}_\{12\}\rho^2g(r_\{12\})e^\{i\vec\{k\}\cdot\vec\{r\}_\{12\}\} \\
&= \frac\{1\}\{N\} + \frac\{1\}\{N\}\rho \int d\vec\{r\}g(r)e^\{i\vec\{k\}\cdot\vec\{r\}\} \\
&= N^\{-1\} + N^\{-1\}\rho\tilde\{g\}(\vec\{k\}) 
\end\{aligned\}$$

where $\tilde\{g\}(\vec\{k\})$ is the Fourier transform of $g(\vec\{k\})$.
This demonstrates that the results of a scattering experiment give a
direct measure of the pair distribution of the particles in the fluid.


# Adsorption \{#chap:adsorption\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:quantum_mechanics\]](#chap:quantum_mechanics)\{reference-type="ref+label"
reference="chap:quantum_mechanics"\},
[\[chap:stat_thermo\]](#chap:stat_thermo)\{reference-type="ref+label"
reference="chap:stat_thermo"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Adsorption occurs when atoms, ions, or molecules from a gas, liquid, or
dissolved solid become bound to a surface due to favorable interactions
(*e.g.* Van der Waal's forces), creating a film of the adsorbate on the
surface of the adsorbent. The term should not be confused with
*absorption*, in which a fluid (the absorbate) permeates or is dissolved
by a liquid or solid (the absorbent). Adsorption considers a
surface-based process, while the later involves the whole volume of the
material. The thermodynamic description of adsorption varies according
to the characteristics of the adsorbent. In this chapter we begin with a
study of non-interacting adsorption, ignoring the energy of interaction
between the adsorbed components. We consider two cases: mobile and
localized adsorption. We end by discuss phenomena arising from the
interaction between adsorbed molecules

::: marginfigure
![image](figures/Physics/statistical_mechanics/adsorption/CO_adsorbing_on_Ni.png)\{width="\\linewidth"\}
:::

## Non-interacting adsorption

We first address adsorption on a lattice where there is negligible
interaction between the adsorbed molecules.

Consider the following two scenarios for the surface adsorption
(Fig. [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}):

-   *Localized adsorption* occurs when the binding potential exhibits
    large spatial variations causing the adsorbate to be highly
    localized.

-   *Mobile adsorption* occurs when the surface potential is nearly
    homogeneous and the adsorbate is highly mobile.

::: marginfigure
![image](figures/Physics/statistical_mechanics/adsorption/2_adsorption (1).png)\{width="\\linewidth"\}
:::

In localized adsorption, each adsorbed molecule is trapped in a
potential well located at a specific lattice site. To a first
approximation, we can expand the potential in the $x$, $y$, and $z$
directions to quadratic order in the displacement. Thus, the
single-molecule partition function of an adsorbed molecule is:
$$\begin\{aligned\}
\label\{Eq1\}
q=q_xq_yq_ze^\{-\beta V_0\} \hspace\{0.4cm\} \text\{with\} \hspace\{0.4cm\} q_i = \frac\{e^\{-\theta _\{vib,i\}/(2T)\}\}\{1-e^\{-\theta _\{vib,i\}/(2T)\}\}
\end\{aligned\}$$

where $q_i(i =x,y,z)$ is a 1-dimensional harmonic oscillator partition
function, $$\begin\{aligned\}
\theta_\{vib,i\} = \frac\{\hslash \nu_i\}\{k_B\}
\end\{aligned\}$$ is the vibrational temperature in the $i$ direction, and
$V_0$ is the potential at the well bottom.

Since each adsorbed molecule occupies a lattice site with a distinct
spatial location, they are distinguishable. For a lattice with $M$ sites
containing N adsorbed molecules, the degeneracy of configurations is
$$\begin\{aligned\}
\Omega = \frac\{M!\}\{N!(M-N)!\}
\end\{aligned\}$$ and the canonical partition function for localized
adsorption is: $$\begin\{aligned\}
Q_\{localized\}(N,M,T) =\frac\{M!\}\{N!(M-N)!\}q^N
\end\{aligned\}$$

The chemical potential of the adsorbate is given by: $$\begin\{aligned\}
\frac\{\mu\}\{k_BT\} &= -\left( \frac\{\partial \log Q_\{localized\}\}\{\partial N\}\right)_\{M,T\} \\
&= -\log q + \log N -\log(M-N) \\
&= \log \left[ \frac\{\theta\}\{(1-\theta)q\} \right] \hspace\{0.3cm\} \text\{where\} \hspace\{0.3cm\} \theta = \frac\{N\}\{M\}
\end\{aligned\}$$

Assuming adsorption from the gas phase, equilibrium dictates equality of
the chemical potentials between the adsorbate on the surface and in the
gas phase. For an ideal gas, the chemical potential can be written as
$$\begin\{aligned\}
\frac\{\mu\}\{k_BT\} &= \frac\{\mu_0(T)\}\{k_BT\} + \log p
\end\{aligned\}$$ This leads to the *Langmuir adsorption isotherm*:
$$\begin\{aligned\}
\label\{Eq4\}
\theta(p,T) = \frac\{\chi(T)p\}\{1+\chi(T)p\} \hspace\{0.3cm\} \text\{where\} \hspace\{0.3cm\} \chi(T) = q(T)e^\{\beta\mu_0(T)\} 
\end\{aligned\}$$

that describes the surface coverage $\theta$ versus the temperature and
pressure of the gas-phase adsorbent for localized adsorption on a solid
adsorbent. The coverage tends to unity when $p \rightarrow \infty$
(total site coverage).

In the case of mobile adsorption, the single-molecule partition function
will have a $z$ direction contribution and an $xy$ contribution that
reflects surface mobility. This is constructed by assuming a 2D ideal
gas with a separate z component partition function, *i.e.*:

$$\begin\{aligned\}
\label\{Eq5\}
q = \frac\{A\}\{\Lambda^2\}q_z^\{-\beta V_0\} \hspace\{0.3cm\} \text\{where\} \hspace\{0.3cm\} \Lambda = \sqrt\{\frac\{h^2\}\{2\pi m k_BT\}\}
\end\{aligned\}$$

and $A$ is the surface area of the adsorbent.

The mobile adsorbed molecules are indistinguishable, since they are not
restricted to a localized adsorption site. The canonical partition
function is therefore given by: $$\begin\{aligned\}
\label\{Eq6\}
Q_\{mobile\}(N,A,T) = \frac\{1\}\{N!\}q^N
\end\{aligned\}$$ This gives the chemical potential:

$$\begin\{aligned\}
\label\{Eq7\}
\frac\{\mu\}\{k_BT\} = -\left( \frac\{\partial \log Q_\{mobile\}\}\{\partial N\}\right)_\{M,T\} = -\log q + \log N 
\end\{aligned\}$$

For adsorption from an ideal gas, the surface coverage is found to be:

$$\begin\{aligned\}
\label\{Eq8\}
\log\left(\frac\{N\}\{q\} \right) = \log\left(pe^\{\beta \mu_0\} \right) \rightarrow \frac\{N\}\{A\} = \chi'(T)p
\end\{aligned\}$$

where $$\begin\{aligned\}
\chi'(T) &= \frac\{1\}\{\Lambda^2\}q_ze^\{-\beta V_0\}e^\{-\beta \mu_0\}
\end\{aligned\}$$

All things the same, this leads to $N_\{mobile\} > N_\{localized\}$ at all
pressures $p>0$ (Fig. [\[fig3\]](#fig3)\{reference-type="ref"
reference="fig3"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/adsorption/Fig3.png)\{width="\\linewidth"\}
:::

## Multi-component adsorption

This is easily extended to a system with $s$ component (localized
adsorption).

Define $q_i$ as the single-molecule partition function of the s
component, and $N_i$ as the number of adsorbed molecules of the $i$th
adsorbate (note, $N_\{tot\} = \sum_\{i=1\}^s N_i\leq N$). The degeneracy of
arranging $N_1,N_2,...,N_s$ adsorbed molecules on $M$ sites is
$$\begin\{aligned\}
\Omega = \frac\{M!\}\{N_1!N_2!...N_s!(M - N_\{tot\})!\}
\end\{aligned\}$$ The canonical partition function is given by:

$$\begin\{aligned\}
\label\{Eq9\}
Q(N,M,T) = \frac\{M!\}\{N_1!N_2!...N_s!(M - N_\{tot\})!\} q_1^\{N_1\}q_2^\{N_2\}...q_s^\{N_s\}
\end\{aligned\}$$

The chemical potential of the $i$th species is:

$$\begin\{aligned\}
\frac\{\mu_i\}\{k_BT\} &= -\left( \frac\{\partial \log Q\}\{\partial N_i\}\right)_\{N_\{j \neq i\},M,T\} \notag\\
&= -\log q -\log(M-N_\{tot\}) + log N_i \notag\\
&= \log \left[ \frac\{N_i\}\{(M-N_\{tot\})q_i\} \right] \notag\\
&= \log \left[ \frac\{\theta_i\}\{(1-\sum_\{j=1\}^s \theta_j)q_i\} \right] \notag
\end\{aligned\}$$

For equilibrium with an ideal gas phase, the chemical potential of the
$i$th species is written as $$\begin\{aligned\}
\frac\{\mu_i\}\{k_BT\} &= \frac\{\mu_\{i,0\}(T)\}\{k_BT\} + \log p_i
\end\{aligned\}$$ where $p_i = py_i$ is the partial pressure of the $i$th
species ($y_i$ is the mole fraction in the gas phase).

Defining $$\begin\{aligned\}
\chi_i(T) &= q_ie^\{\beta \mu_\{i,0\}\}
\end\{aligned\}$$, we have $$\begin\{aligned\}
p_i\chi_i = \frac\{\theta_i\}\{1 - \sum_\{j=1\}^s \theta_j\} \hspace\{0.3cm\} \text\{where\} \hspace\{0.3cm\} \theta_i = \frac\{p_i\chi_i\}\{1 + \sum_\{j=1\}^s p_j\chi_j\}
\end\{aligned\}$$

The two extremes of the multi-component isotherm are as follows: For
$p_i\chi_i \rightarrow 0$, the coverage behaves as $$\begin\{aligned\}
\theta_i \approx p_i\chi_i
\end\{aligned\}$$ which is the Henry's law regime. For
$p_i\chi_i \rightarrow \infty$ while $p_j\chi_j$ remains finite
($i \neq j$), the coverage saturates to $$\begin\{aligned\}
\theta_i \rightarrow 1
\end\{aligned\}$$ Which is the complete coverage regime.

Therefore, binding affinity for a particular species can be outcompeted
by species with lower affinity by raising the partial pressure of the
other components

## Adsorption with interactions between adsorbed molecules

Our previous analysis neglects any interactions between adsorbed
molecules, which is an adequate description for low surface coverage and
lattice structure with large spacing. However, surface interactions
leads to correlated surface coverage, analogous to the spatial
correlations that we studied with the Ising model.

Let's adopt a simple model for the adsorption energy on a lattice:

$$\begin\{aligned\}
\label\{Eq11\}
\beta E = f_\{bind\}\sum_\{i=1\}^M n_i - K \sum_\{\langle i j \rangle\} n_in_j
\end\{aligned\}$$

where $n_i$ is the site occupancy ($n_i = 0$ if unoccupied and $n_i = 1$
is occupied), $K$ is a coupling constant to describe interactions
between neighboring adsorbed molecules, and $f_\{bind\}$ is the free
energy of binding the adsorbed molecule (note,
$f_\{bind\} = -k_BT \log q$).

The grand canonical partition function is written as: $$\begin\{aligned\}
\Xi = \sum_\{n_1,n_2,...n_M = 0,1\} \exp \left( \beta K \sum_\{\langle i j \rangle\} n_in_j - \beta f_\{bind\}\sum_\{i=1\}^M n_i + \beta \mu\sum_\{i=1\}^M n_i \right)
\end\{aligned\}$$

Rather than analyze this further, let's convert this into a form that
demonstrates that this is exactly the Ising model that we have already
studied extensively. We relate the occupancy number $n_i$ to the spin
$s_i$ using $n_i = \frac\{s_i+1\}\{2\}$. which gives $n_i=1$ when $s_i=1$
and $n_0=0$ when $s_i=-1$. With this we write: $$\begin\{aligned\}
&\beta K \sum_\{\langle i j \rangle\} n_in_j - \beta f_\{bind\}\sum_\{i=1\}^M n_i + \beta \mu\sum_\{i=1\}^M n_i = \\
&\frac\{\beta K\}\{4\} \sum_\{\langle i j \rangle\} s_is_j + \beta \left(Kz + \frac\{\mu\}\{2\} - \frac\{f_\{bind\}\}\{2\}\right)\sum_\{i=1\}^M s_i + \beta M \left(\frac\{Kz\}\{4\}+\frac\{\mu\}\{2\} - \frac\{f_\{bind\}\}\{2\}\right) 
\end\{aligned\}$$

where $z$ is the coordination number of the lattice giving the number of
near neighbors divided by 2 to avoid double counting (*e.g.* $z = 2$ for
a 2D square lattice).

Therefore, we can exactly map our current problem onto the Ising model
with the parameters $J = K/4$ and $h = Kz+\mu/2- f_\{bind\}/2$, thus
leveraging the work that we have already done.

Owing to the exact mapping between our current problem and the Ising
model, identical physical phenomena arise in both systems

-   Adsorption with interactions exhibits a phase transition under some
    conditions (sufficiently large coupling constant $K$ and lattice
    that is not 1 dimensional).

-   Adsorption exhibits hysteresis in the surface coverage versus gas
    pressure associated with this phase transition.

-   The surface coverage is not completely uncorrelated; rather, the
    adsorbed molecules exhibit correlated fluctuations due to favorable
    interactions.

-   The critical point in the adsorption isotherm is associated with a
    divergence in the correlation length of these spatial fluctuations.

## Exercises

1.  Within the mean-field approach, write the expression for the
    Hamiltonian H = $\beta E$ for the total system of N sites on a 2D
    triangular lattice in terms of the various spin variables, external
    fields and coupling constants (without substituting actual values)
    and average occupation, where the average occupation of kind 'k' can
    be denoted as $\langle \sigma^\{k\}\rangle$.

2.  Express the partition function $Q$ for this system of N sites using
    the defined Hamiltonian in terms of the the various spin variables,
    external fields, coupling constants and average occupations.

3.  What is the expression for each of the average magnetization
    $\langle \sigma^\{k\}\rangle$ in terms of the various spin variables,
    external fields, coupling constants and themselves.

4.  Is this system capable of phase transition?


# Free Energy Perturbation \{#chap:fep\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:newtons_laws\]](#chap:newtons_laws)\{reference-type="ref+label"
reference="chap:newtons_laws"\},
[\[chap:electrostatics\]](#chap:electrostatics)\{reference-type="ref+label"
reference="chap:electrostatics"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Free energy perturbation techniques allow for estimates of the
differences in free energy between different states of a physical
system. Free energy perturbation systems can be critically important for
computing quantities like the binding free energy of a molecule to a
protein.

## Theoretical Foundations

Suppose that we have two states of interest, state $0$ and state $1$.
Let $A$ be the Helmholtz free energy. We can estimate the free energy
state difference between state $0$ and $1$ by only drawing samples from
state $0$ and then computing the potential energies for states $1$ and
$0$, and then compute the estimated free energy difference using the
update formula

$$\begin\{aligned\}
\Delta A &= A_1 - A_0 \\
&= -k_B T \ln \left \langle \exp \left (  -\frac\{U_1 - U_0\}\{k_B T\} \right ) \right \rangle
\end\{aligned\}$$ Here $U_1$ and $U_0$ are the potential energies of
states $1$ and $0$ respectively. The challenge in using this equation
though is that often state $1$ and state $0$ are far enough apart that
computing the estimate directly from state $0$ can be prohibitively
challenging. For this reason, the path from $0$ to $1$ is typically
broken into a series of steps controlled by a parameter traditionally
called $\lambda$.

## Targeted Approaches

A new class of techniques seeks to make computations easier by learning
a transformation that maps states $0$ and $1$ into overlapping regions.
This learned transformation is typically a normalizing flow.


# Electrolytes \{#chap:electrolytes\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:greens_identities\]](#chap:greens_identities)\{reference-type="ref+label"
reference="chap:greens_identities"\},
[\[chap:maxwells_equations\]](#chap:maxwells_equations)\{reference-type="ref+label"
reference="chap:maxwells_equations"\},
[\[chap:phase_equilibrium\]](#chap:phase_equilibrium)\{reference-type="ref+label"
reference="chap:phase_equilibrium"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Any molecule that can dissolve in polar liquids, dissociate into ions,
and carry an electrical current is an *electrolyte*. Molecules that
dissociate completely are called strong electrolytes, and those that
dissociate only partly are called weak electrolytes. Such ions can
regulate the chemical and physical equilibrium of other charged objects.
For example, DNA is a molecular chain of negative phosphate charges.
Because its charges repel each other, DNA is relatively expanded in
water. When $NaCl$ is added to an aqueous solution of DNA, the salt
dissociates into positive ions ($Na^+$) and negative ions ($Cl^-$). The
positive ions seek the DNA's negative charges, surrounding and shielding
them. This shielding weakens the intra-DNA charge repulsion, causing the
DNA to become more compact. Two biological cells each having a net
negative surface charge will also repel each other.

In this chapter we explore how ions shield charged objects. We start by
a review of electrostatics where we derive the expression for the
electrostatic energy of a charged sphere and cylinder. This expression
will provide the starting point for the analysis of more complex
situations, such as *electrostatic screening* due to the large
concentration of ions close to a surface with opposite charge.

## Electrostatic potential

Electrostatic interactions in an aqueous environment dramatically
influence the thermodynamic behavior. Polyelectrolytes (charged
polymers) are soluble in water, in contrast to most hydrocarbon-based
polymers. Most biopolymers are polyelectrolytes, and in many instances,
their biological function hinges on the role of electrostatic
interactions (Fig. [\[fig1\]](#fig1)\{reference-type="ref"
reference="fig1"\}).

::: marginfigure
![image](figures/part2a/statistical_mechanics/electrolytes/fig1.png)\{width="\\linewidth"\}
:::

The force on a point particle with charge q in an electric field
$\vec\{E\}$ is given by $\vec\{f\} = q\vec\{E\}$. The potential energy can
then be written as $q\Psi$, where $\Psi$ is defined to be the
electrostatic potential, giving $\vec\{E\}=-\nabla \Psi$.

The electric field satisfies *Gauss' law*, which is written as:

$$\begin\{aligned\}
\label\{Eq1\}
\int_S \vec\{E\}\cdot \vec\{n\} dA = \frac\{4\pi\}\{\varepsilon\}\int_V \rho dV
\end\{aligned\}$$

where $\varepsilon$ is the dielectric constant of the medium (for
example, $\varepsilon = 7.083\times 10^\{-10\} C^2N^\{-1\}m^\{-2\}$, for water
at room temperature), and $\rho$ is the local charge density.

We apply the *divergence theorem* to this to arrive at:
$$\begin\{aligned\}
\label\{Eq2\}
\int_V \vec\{\nabla\}\cdot \vec\{E\}dV = \frac\{4\pi\}\{\varepsilon\}\int_V \rho dV
\end\{aligned\}$$

Therefore, the electrostatic potential $\Psi$ is governed by Poisson's
align: $$\begin\{aligned\}
\label\{Eq3\}
\nabla^2\Psi = -\frac\{4\pi\rho(\vec\{r\})\}\{\varepsilon\}
\end\{aligned\}$$

## Electrostatic potential around a charged sphere

Consider a sphere with a radius a and total charge $q$. Due to symmetry,
the electrostatic potential around the sphere is radial, *ı.e.*
$\Psi = \Psi(r)$. Poisson's equation outside of the sphere is written
as: $$\begin\{aligned\}
\label\{Eq4\}
\frac\{1\}\{r^2\}\frac\{d\}\{dr\}r^2\frac\{d\Psi\}\{dr\} = 0 \rightarrow \Psi = \frac\{A_1\}\{r\} + A_2
\end\{aligned\}$$

where $A_1$ and $A_2$ are constant of integration (to be determined).

Assume as $r\rightarrow \infty$, $\Psi\rightarrow 0$; thus, $A_2 = 0$.

The electric field in the radial direction is
$\hat\{e\}_r\cdot \vec\{E\} = \frac\{A1\}\{r_2\}$, and, using Gauss' law, we can
write: $$\begin\{aligned\}
\label\{Eq5\}
\frac\{A_1\}\{a^2\}4\pi a^2 = \frac\{4\pi q\}\{\varepsilon\} \rightarrow A_1 = \frac\{q\}\{\varepsilon\} \rightarrow \Psi = \frac\{q\}\{\varepsilon\}\frac\{1\}\{r\}
\end\{aligned\}$$

This potential is valid outside the radius $a$ ($r > a$). For a point
particle ($a\rightarrow 0$), this is valid throughout space.

The electrostatic potential provides the necessary information to find
the energy. The energy of interaction between two ions with charge $e$
(charge of an electron) separated by a distance $r$ is:
$$\begin\{aligned\}
\label\{Eq6\}
E_\{coulomb\} = \frac\{e^2\}\{\varepsilon\}\frac\{1\}\{r\}
\end\{aligned\}$$

where $\varepsilon$ is the dielectric constant of the medium (for
example, $\varepsilon = 7.083 \times 10^\{-10\}C^2N^\{-1\}m^\{-2\}$ for water
at room temperature) and the electron charge
$e = 1.6022\times 10^\{-19\}C$.

Define the Bjerrum length: $$\begin\{aligned\}
\label\{Eq7\}
l_B = \frac\{e^2\}\{k_BT\varepsilon\}
\end\{aligned\}$$

which gives the distance that two electrons are separated when their
energy is equal to $k_BT$ ($l_B = 7\AA$ for water at room temperature).

The energy of interaction for two monovalent ions in a dielectric medium
is written as: $$\begin\{aligned\}
\label\{Eq8\}
E_\{coulomb\}= k_BT\frac\{l_B\}\{r\}
\end\{aligned\}$$

## *Electrostatic potential around a charged cylinder* \{#electrostatic-potential-around-a-charged-cylinder .unnumbered\}

Consider a negatively charged cylinder of radius $a$ with a surface
charge such that the average distance between the surface charges is
$b$. The electrostatic potential in an uncharged medium ($\rho = 0$) is
given by: $$\begin\{aligned\}
\label\{Eq9\}
\nabla^2\Psi = \frac\{1\}\{r\}\frac\{\partial\}\{\partial r\}r\frac\{\partial\}\{\partial r\}\Psi  = 0 \rightarrow \Psi = A_1\log r + A_2
\end\{aligned\}$$

where we express the Laplacian in cylindrical coordinates and assume
$\Psi = \Psi(r)$ only.

We find the boundary condition on the cylinder surface using Gauss' law
Using the divergence theorem, we write: $$\begin\{aligned\}
\label\{Eq10\}
\int_S \vec\{E\}\cdot \vec\{n\}dA = \frac\{4\pi\}\{\varepsilon\}\int_V \rho dV
\end\{aligned\}$$

This gives the boundary condition on the cylinder surface:
$$\begin\{aligned\}
\label\{Eq11\}
\left( \hat\{e\}_r \cdot\vec\{E\} \right) 2\pi ab=-\frac\{4\pi e\}\{\varepsilon\}
\end\{aligned\}$$

This boundary condition on the cylinder surface is rewritten as:
$$\begin\{aligned\}
\label\{Eq12\}
\frac\{\partial \Psi\}\{\partial r\}\bigg|_\{r=a\} = \frac\{2e\}\{ab\varepsilon\}
\end\{aligned\}$$

giving $A_1=\frac\{2e\}\{b\varepsilon\}$.

For simplicity, we set $\Psi\rightarrow 0$ as $r\rightarrow a$, fixing
$A_2 = -A_1 \log a$. This gives the solution for the electrostatic
potential. $$\begin\{aligned\}
\label\{Eq13\}
\Psi(r) = \frac\{2k_BTq\}\{e\}\log\left(r/a\right)
\end\{aligned\}$$

where $q = \l_B/b$ and $l_B = e^2/(k_BT\varepsilon)$.

Unlike the point charge, the electrostatic potential outside of a
charged cylinder diverges logarithmically, which has dramatic
consequences on the counter-ion distribution.

## Screened electrostatic interaction in salt (Debye-Hückel theory)

When salt is added to the solution, the nature of the electrostatic
interactions is dramatically altered. A charged molecule or surface
attracts the mobile ions that have opposite charge, or *counter-ions*,
to the surface due to the strength of electrostatic interactions. At the
same time, the charged surface repels the mobile ions of the same sign,
called *co-ions*. For example a polyanion (negative polyelectrolyte) has
a flurry of positively charged counter-ions swarming around the polymer
(Fig [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}). The
interactions between charges are dramatically reduced due to the
*screening* of the electostatic interactions
(Fig [\[fig3\]](#fig3)\{reference-type="ref" reference="fig3"\}).

::: marginfigure
![image](figures/part2a/statistical_mechanics/electrolytes/fig2.png)\{width="\\linewidth"\}
:::

Consider the interaction between two isolated ions in a monovalentsalt
solution with concentration $n_s$ (for example, $NaCl$ that dissociates
into $Na^+$ and $Cl^-$ ions). The electrostatic potential $\Psi$ around
the tagged ion is governed by the Poisson's equation. $$\begin\{aligned\}
\label\{Eq14\}
\nabla^2\Psi = -\frac\{4\pi\rho(\vec\{r\})\}\{\varepsilon\}
\end\{aligned\}$$

where $\rho$ is the local charge density in the solution.

::: marginfigure
![image](figures/part2a/statistical_mechanics/electrolytes/fig3.png)\{width="\\linewidth"\}
:::

The energy of an ion with charge $q = ze$ (*e.g.* $q=-e$ for $Cl^-$)
located at $\vec\{r\}$ in the electrostatic potential $\Psi$ is given by
$E_\Psi = ze\Psi(\vec\{r\})$. As a charged colloidal particle is immersed
on the electrolyte solution, a gradient concentration of co-ions and
conter-ions is developed from the surface. Assume the local
concentration of counter-ions is given by a simple Boltzmann weight of
the ions in the electrostatic potential $\Psi$. Thus, $$\begin\{aligned\}
\label\{Eq15\}
\rho (\vec\{r\}) = n_s e \left[ \exp\left(-\frac\{e\Psi(\vec\{r\})\}\{k_BT\}\right) -\exp\left(+\frac\{e\Psi(\vec\{r\})\}\{k_BT\}\right)\right]
\end\{aligned\}$$

where the left term is due to local concentration of $Na^+$, and the
right is due to $Cl^-$, and the position coordinate $\vec\{r\} = 0$ at the
surface of the charged colloidal particle
(Fig. [\[Poisson-Boltzmann\]](#Poisson-Boltzmann)\{reference-type="ref"
reference="Poisson-Boltzmann"\})

::: marginfigure
[]\{#Poisson-Boltzmann label="Poisson-Boltzmann"\}
![image](figures/part2a/statistical_mechanics/electrolytes/Poisson-Boltzmann.png)\{width="\\linewidth"\}
:::

Therefore, the electrostatic potential is governed by: $$\begin\{aligned\}
\label\{Eq16\}
\nabla^2\Psi = \frac\{8\pi n_s e\}\{\varepsilon\}\sinh\left(\frac\{e\Psi\}\{k_BT\} \right)
\end\{aligned\}$$

This is the Poisson-Boltzmann equation. This nonlinear equation does not
have a simple, closed-form solution. Implicit in this theory is the
neglect of local concentration fluctuations, akin to a mean-field
approximation.

For sufficiently large lengths where $\Psi \rightarrow 0$, we can expand
to lowest order. $$\begin\{aligned\}
\label\{Eq17\}
\nabla^2\Psi = \frac\{8\pi n_s e^2\}\{\varepsilon k_B T\}\Psi
\end\{aligned\}$$

We define the Debye length (Fig [\[fig4\]](#fig4)\{reference-type="ref"
reference="fig4"\}): $$\begin\{aligned\}
\label\{Eq18\}
l_D = \sqrt\{\frac\{\varepsilon k_BT\}\{8\pi e^2n_s\}\}
\end\{aligned\}$$

and assume that $\Psi = \Psi(r)$ only.

We now have: $$\begin\{aligned\}
\label\{Eq19\}
\frac\{1\}\{r^2\}\frac\{\partial \}\{\partial r^2\}r^2\frac\{\partial\}\{\partial r\}\Psi = \frac\{1\}\{l_D^2\}\Psi
\end\{aligned\}$$

In the limit of large length separation $r$ and dilute solution of
counter-ions, we can write the electrostatic energy between two ions in
a salt solution as: $$\begin\{aligned\}
\label\{Eq20\}
E_\{DH\} = \frac\{e^2\}\{4\pi \varepsilon\}\frac\{e^\{-r/l_D\}\}\{r\} = k_BTl_B \frac\{e^\{-r/l_D\}\}\{r\}
\end\{aligned\}$$

This is the Debye-Hückel theory of charge screening. The Debye-Hückel
theory adds a simple correction to the bare electrostatic potential to
account for counter-ion correlations.

::: marginfigure
![image](figures/part2a/statistical_mechanics/electrolytes/fig4.png)\{width="\\linewidth"\}
:::

In the absence of salt, the electrostatic interaction energy is
extremely long range, scaling as $E_\{coulomb\} \sim 1/r$. Salt screening
leads to a short-range electrostatic interaction energy that decays
exponentially with distance of separation.

## Charge behavior at short range (Manning condensation)

Neglecting the nonlinear terms in the Poisson-Boltzmann equation leads
to a qualitatively incorrect behavior for distances very close to a
charged object, *e.g.* the surface of a polyelectrolyte. For a dilute
solution, the average distance between counter-ions is
$r_0 \sim n^\{-1/3\}_s$. At $r < r_0$, the mean-field approximation breaks
down, and we must address the interactions explicitly.

Consider the polyelectrolyte chain to be a charged cylinder with radius
a (Fig. [\[fig5\]](#fig5)\{reference-type="ref" reference="fig5"\}), which
we assume to be very small ($a\rightarrow 0$ in our analysis). The
charged cylinder has negative charges smeared over its surface such that
the average distance between negative charges is $\eta$. We want to find
the behavior of an isolated ion interacting with the cylinder at
distances less than the average inter-ion spacing $r_0$.

::: marginfigure
![image](figures/part2a/statistical_mechanics/electrolytes/fig5.png)\{width="\\linewidth"\}
:::

Earlier in this chapter, we found the electrostatic potential around a
negatively charged cylinder in a dielectric medium to be:

$$\begin\{aligned\}
\label\{Eq21\}
\Psi(r) = \frac\{2k_BTq\}\{e\}\log\left(r/a\right)
\end\{aligned\}$$

where $q = l_B/\eta$. The electrostatic energy between the cylinder and
a positively charged monovalent ion is given by $E_\{cyl\}(r) = e\Psi(r)$.

The partition function for an isolated ion interacting with the charged
cylinder within the distance r0 for small cylinder radius
($a \rightarrow 0$) scales as: $$\begin\{aligned\}
\label\{Eq22\}
Q&\sim \int_0^\{r_0\}  r \exp\left( -\frac\{E_\{cyl\}(r)\}\{k_BT\} \right) dr\\
&\sim \int_0^\{r_0\}r\exp(-2q\log(r)) dr\\
&\sim\int_0^\{r_0\}r^\{1-2q\}dr \\
&\sim \frac\{1\}\{2-2q\}r^\{2-2q\}\bigg|_0^\{r_0\}
\end\{aligned\}$$

The partition function diverges if q \> 1.

To reconcile the divergence of the free energy, the mobile ions condense
on the cylinder surface to neutralize the surface charges such that
effectively $q_\{eff\} = l_B/\eta_\{eff\} = 1$. At short length scales,
Manning condensation leads to a reduced surface charge. At long length
scales, the interaction is a screened electrostatic coulomb potential.


# Basics of Statistical Thermodynamics \{#chap:stat_thermo_basics\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:schrodinger\]](#chap:schrodinger)\{reference-type="ref+label"
reference="chap:schrodinger"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

[TODO: This is mainly notes from Shankar. This should likely be merged
in with one of the other stat thermo chapters]\{style="color: red"\}

Let $P$ be the pressure and $V$ the volume of some gas. Let $S$ be the
entropy, defined as follows where $T$ is the temperature.

$$\begin\{aligned\}
    dS = \frac\{\delta Q\}\{T\}
\end\{aligned\}$$

Here $\delta Q$ is used since $Q$ isn't a state variable. We can express
the energy of the system as $$\begin\{aligned\}
    dU &= \delta Q - P dV
\end\{aligned\}$$


# Critical Behavior in the Ising model \{#chap:ising_critical\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ising_model\]](#chap:ising_model)\{reference-type="ref+label"
reference="chap:ising_model"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In [\[chap:ising_model\]](#chap:ising_model)\{reference-type="ref+label"
reference="chap:ising_model"\}, we discussed the phase behavior of a
magnetic system by analyzing the exact 1D and mean-field approximation
of the Ising model. We saw that no phase transition occurs for the 1D
Ising model and the model exhibits a true phase transition for D $\geq$
2, which was argued by physical arguments and through the mean-field
approximation.

The $h-M$ phase diagram in a magnetic system was found to behave akin to
the $p-V$ phase diagram in vapor-liquid systems. In both systems, a
critical point exists that marks the onset of phase coexistence in the
system, i.e. for $T > T_c$, the system does not exhibit a phase
transition, and for $T < T_c$, phase transitions exist. Now, we discuss
the physical behavior near the critical point for both a magnetic system
and a liquid-vapor system.

## Critical point in vapor-liquid and magnetic systems

The fluctuations of the energy, density, and magnetization for a system
are found using various ensemble manipulations.

From the canonical ensemble, the variance of the energy is:
$$\begin\{aligned\}
\label\{Eq1\}
\langle (\delta E)^2\rangle &= \langle (E - \langle E \rangle)^2\rangle =  \langle E^\{2\} \rangle - \langle E \rangle^\{2\} \\
&= \frac\{1\}\{Q\}\left(\frac\{\partial ^\{2\}Q\}\{\partial \beta^\{2\}\}\right)\bigg|_\{V,N\} - \frac\{1\}\{Q^\{2\}\}\left(\frac\{\partial Q\}\{\partial \beta\}\right)^\{2\}\bigg|_\{V,N\}  \\
 &= \left(\frac\{\partial ^\{2\}\ln Q\}\{\partial \beta^\{2\}\}\right)\bigg|_\{V,N\} = \left(\frac\{\partial ^\{2\}\beta F\}\{\partial \beta^\{2\}\}\right)\bigg|_\{V,N\} = kT^\{2\}C_\{V\}
\end\{aligned\}$$

Thus connecting the energy fluctuations to $C_\{V\}$. The standard
deviation
$\sigma_\{E\} = \sqrt\{\langle E^\{2\} \rangle - \langle E \rangle^\{2\}\}$ for
energy fluctuations results in the characteristic magnitude of the
energy fluctuation per molecule in terms of the heat capcity per
molecule $c_\{V\} = \frac\{C_\{V\}\}\{N\}$ as: $$\begin\{aligned\}
\label\{Eq2\}
\frac\{\sigma_\{E\}\}\{N\} = \frac\{1\}\{N\}\sqrt\{NkT^\{2\}c_\{V\}\} = \sqrt\{\frac\{kT^\{2\}c_\{V\}\}\{N\}\} 
\end\{aligned\}$$

This calculation verifies that the average energy per molecule is a well
defined quantity with negligible fluctuations, provided that the heat
capacity does not diverge.

Similarly, the grand canonical ensemble for a one-component system
yields the molecule number variance to be:

$$\begin\{aligned\}
\label\{Eq3\}
\langle N^\{2\} \rangle - \langle N \rangle^\{2\} &= \left(\frac\{\partial ^\{2\}\ln \Xi\}\{\partial (\beta\mu)^\{2\}\}\right)\bigg|_\{\beta,V\} \\
&= \left(\frac\{\partial ^\{2\} (\beta pV)\}\{\partial (\beta\mu)^\{2\}\}\right)\bigg|_\{\beta,V\} \\
&= kT\left(\frac\{\partial \langle N \rangle\}\{\partial \mu\}\right)\bigg|_\{T,V\}
\end\{aligned\}$$

And noting the Gibbs-Duhem equation for a 1 component system
($d\mu=-sdT+vdp$) and $v = \frac\{V\}\{\langle N \rangle\}$, the variance in
$N$ can be written as: $$\begin\{aligned\}
\label\{Eq4\}
\langle N^\{2\} \rangle - \langle N \rangle^\{2\} = -kT\frac\{V\}\{v^\{3\}\} \left(\frac\{\partial v\}\{\partial p\}\right)\bigg|_\{T\}
\end\{aligned\}$$

Using the definition of molar density
$\rho=\frac\{1\}\{v\}=\frac\{\langle N \rangle\}\{V\}$ $$\begin\{aligned\}
\label\{Eq5\}
\langle \rho^\{2\} \rangle - \langle \rho \rangle^\{2\} = -\frac\{kT\}\{Vv^\{3\}\} \left(\frac\{\partial v\}\{\partial p\}\right)\bigg|_\{T\}
\end\{aligned\}$$

The condition for thermodynamic stability dictates that
$-\left(\frac\{\partial v\}\{\partial p\}\right)\bigg|_\{T\} > 0$, and if V
$\rightarrow\infty$, the density fluctuations in a stable system are
negligible. However, the condition
$\left(\frac\{\partial p\}\{\partial v\}\right)\bigg|_\{T\} \rightarrow 0$
dictates that the density fluctuations in the system diverge. Thus, the
limit of stability for a 1-component system, marked by
$\left(\frac\{\partial p\}\{\partial v\}\right)\bigg|_\{T\} = 0$, exhibits
wild density fluctuations.

As a system approaches the limit of stability for $T < T_c$, density
fluctuations become substantial, leading to the eventual formation of
the 2 coexisting phases (spinodal decomposition)
(Fig. [\[p-V\]](#p-V)\{reference-type="ref" reference="p-V"\}). As the
system approaches the critical point, the density fluctuations dominate
the physical behavior.

::: marginfigure
![image](figures/part2a/statistical_mechanics/critical_behavior_ising_model/fig1.png)\{width="\\linewidth"\}
:::

Rayleigh Scattering occurs when the length scale of density fluctuations
is comparable to the wavelength of light, where the local mismatch in
the index of refraction leads to light scattering
(Fig. [\[rayleigh\]](#rayleigh)\{reference-type="ref"
reference="rayleigh"\}).

The fluctuation in the magnetization for the Ising model is governed by:
$$\begin\{aligned\}
\label\{Eq6\}
\langle M^\{2\} \rangle - \langle M \rangle^\{2\} &= \left(\frac\{\partial ^\{2\}\log Q\}\{\partial (\beta h)^\{2\}\}\right)\bigg|_\{\beta J,N\} \\
&= \left(\frac\{\partial \langle M \rangle\}\{\partial (\beta h)\}\right)\bigg|_\{\beta J,N\} \\ 
&= N\left(\frac\{\partial m\}\{\partial (\beta h)\}\right)\bigg|_\{\beta J\}
\end\{aligned\}$$

::: marginfigure
![image](figures/part2a/statistical_mechanics/critical_behavior_ising_model/fig2.jpg)\{width="\\linewidth"\}
:::

As in the vapor-liquid system, the Ising model exhibits wild
fluctuations at the limit of metastability, marked by the condition:
$$\begin\{aligned\}
\frac\{\partial (\beta h)\}\{\partial M\}\bigg|_\{\beta J,N\} = 0 
\end\{aligned\}$$ In this case, the fluctuations are in the magnetization
rather than the density.

If we are far away from the critical point, the standard deviation in
the magnetization per site is:

$$\begin\{aligned\}
\label\{Eq7\}
\frac\{\sigma_\{M\}\}\{N\} = \sqrt\{\frac\{1\}\{N\}\left(\frac\{\partial m\}\{\partial (\beta h)\}\right)\bigg|_\{\beta J\}\}
\end\{aligned\}$$

Thus, suggesting that the magnetization is well-defined in the
thermodynamic limit $N \rightarrow \infty$. As we approach the critical
point, the role of fluctuations dominates the system thermodynamics, as
we will proceed to discuss.

## Critical behavior for the Ising model

We discuss the critical behavior for the Ising model from 3 different
perspectives.

-   Using Monte Carlo simulation, demonstrate the physical behavior of
    the Ising model as $T \rightarrow T_c$

-   Mean-field approximation using our treatment from the previous
    lecture

-   Introduce basic concepts in renormalization group theory

To illustrate these concepts, focus on the 2-dimensional Ising model on
a square lattice. The analytical solution for the 2D Ising model
[@Onsager1944] shows that the critical temperature is $T_c = 2.269 J/k$
and the magnetization scales as $M \sim N(T-T_c)^\{\frac\{1\}\{8\}\}$. This
result acts as a useful basis for comparison for the methods that we
discuss today and next time.

## Monte Carlo Simulation

Exact analytical solutions for many problems are frequently both
difficult to find and unwieldy to use if they can be solved. Computer
simulations are particularly useful for quantifying behavior when
analytical results cannot be found and for visualizing physical
phenomena to aid understanding. Here, we introduce Monte Carlo
simulation as a method to visualize the physical behavior near the
critical point.

Consider the Ising model on a 2-dimensional square lattice. The
governing equations for the canonical ensemble are: $$\begin\{aligned\}
P_\{\mu\} = \frac\{1\}\{Q\}\exp(-\beta E_\{\mu\}) 
\end\{aligned\}$$

where: $$\begin\{aligned\}
Q = \sum_\{\mu\}\exp(-\beta E_\{\mu\}) 
\end\{aligned\}$$

$\mu$ represents a microscopic configuration of the system (e.g. a set
of spin values s$_i$ for the Ising model).

Rather than explicitly solving the partition function, we employ a
computer simulation technique called Monte Carlo simulation. First, we
choose a random starting configuration. Then, we cycle many times
through the following Metropolis algorithm:

1.  Starting from state $\mu$, randomly create a new trial state $\mu '$

2.  The probability $\alpha$ of choosing this new state is given by:
    $$\begin\{aligned\}
        \alpha = \frac\{P_\{\mu '\}\}\{P_\{\mu\}\} = \frac\{\frac\{1\}\{Q\}\exp(-\beta E_\{\mu '\})\}\{\frac\{1\}\{Q\}\exp(-\beta E_\{\mu\})\} = \exp(-\beta (E_\{\mu '\} - E_\{\mu\}))
        
    \end\{aligned\}$$

3.  If $E_\{\mu '\} < E_\{\mu\}$, always change the state from $\mu$ to
    $\mu '$

4.  If $E_\{\mu '\} > E_\{\mu\}$, select a random number $A$ between 0 and
    1, and only change the state from $\mu$ to $\mu '$ if $A < \alpha$

In order to properly explore phase space, the system need to undergo
many random changes. Since the distribution is not known a priori, it is
necessary to establish metrics for whether you have run the simulation
sufficiently long. For the Ising model, the simplest method for choosing
a trial state is to randomly choose a spin and flip it. Then, calculate
the energy change, and determine if you select the new move.

::: marginfigure
![image](figures/part2a/statistical_mechanics/critical_behavior_ising_model/fig3.png)\{width="\\linewidth"\}
:::

There are many caveats to Monte Carlo simulations. Care has to be taken
when determining whether you have fully sampled the distribution.
Adaptive methods for randomly selecting trial states that do not result
in too many instances of rejections or too small of moves must be
established. Calculating certain averages can be difficult when the
simulation needs to explore rare regions of phase space to accurately
evaluate the average (e.g. the Helmholtz free energy). Therefore, one
must ensure the moves are able to get out of deep energy wells.
Importantly, it must be ensured that the simulation are not biased to
any region of phase space (for satisfying the ergodic hypothesis).

Monte Carlo simulation is generally very adaptable to different models
and is very simple to implement. An example is shown in
Fig. [\[mc\]](#mc)\{reference-type="ref" reference="mc"\}.

## Mean-field approximation

In an earlier chapter, we showed that the equation of state for the
mean-field approximation to the Ising model is: $$\begin\{aligned\}
\label\{Eq8\}
m = \tanh(\beta h + \beta J zm) = \tanh(\bar\{h\} + Km) 
\end\{aligned\}$$

The mean-field critical point is at $\bar\{h\}$ = 0 and $K_c = 1$, thus we
can write $K = T_c/T$, where $T_c = 2J/k$ for a 2D square lattice. Just
below $T = T_c$ and $h = 0$, on expanding $m$ near $m=0$, we get:
$$\begin\{aligned\}
\label\{Eq9\}
m = \tanh \left( \frac\{T_c\}\{T\}m \right) \approx \frac\{T_c\}\{T\}m - \frac\{1\}\{3\}\left( \frac\{T_c\}\{T\}\right)^3m^3 \rightarrow m \approx \pm \sqrt\{\frac\{3\}\{T_c\}\}(T_c-T)^\{1/2\}
\end\{aligned\}$$

To find the fluctuations in the magnetization near $T = T_c$, we find:
$$\begin\{aligned\}
\label\{Eq10\}
\frac\{\partial m\}\{\partial \bar\{h\}\} &= [1 - \tanh^\{2\} (\bar\{h\} + Km)][1 + K\frac\{\partial m\}\{\partial \bar\{h\}\}]  \\
&=  \frac\{1-m^\{2\}\}\{1-K+Km^\{2\}\}
\end\{aligned\}$$

For $T > T_c$, we have $m=0$. Therefore: $$\begin\{aligned\}
\frac\{\partial m\}\{\partial \bar\{h\}\} =  \frac\{1\}\{1-K\} \sim \frac\{1\}\{T-T_c\} 
\end\{aligned\}$$ And for $T < T_c$, we have: $$\begin\{aligned\}
\frac\{\partial m\}\{\partial \bar\{h\}\} =  \frac\{1-3+3/K\}\{1-K+K(3-3/K)\}  \sim \frac\{1\}\{K-1\} \sim \frac\{1\}\{T_c-T\} 
\end\{aligned\}$$

Thus, the mean-field approximation to the Ising model predicts the
fluctuations in the magnetization to diverge near the critical
temperature as: $$\begin\{aligned\}
\langle M^\{2\} \rangle - \langle M \rangle^\{2\} \sim \frac\{N\}\{|T-T_c|\}
\end\{aligned\}$$

For $T = T_c$ ($K = 1$) and $h \neq 0$, expand $m$ about $m = 0$ to get:
$$\begin\{aligned\}
m \simeq (\bar\{h\}+m) - 1/3(\bar\{h\}+m)^\{3\} \rightarrow m \simeq 3^\{1/3\}\bar\{h\}^\{1/3\} - \bar\{h\} \sim h^\{1/3\}
\end\{aligned\}$$

The deviation of the mean-field approximation from the exact result lies
in the neglect of correlations near the critical point, thus the
mean-field approximation is best far from the critical point.

For a vapor-liquid system governed by a mean-field theory (e.g. van der
Waal's equation of state), the critical behavior is: $$\begin\{aligned\}
\rho_c - \rho &\sim (T_c-T)^\{1/2\},\,\,\,\langle \rho^\{2\} \rangle - \langle \rho \rangle^\{2\} \sim \frac\{1\}\{|T-T_c|\} \sim \kappa_T  \\
\rho - \rho_c &\sim (p-p_c)^\{1/3\}
\end\{aligned\}$$

Which shows a direct correspondence between $m-\rho$ and $h-p$. The
critical behavior of the Ising model and of other related thermodynamic
systems reflects the underlying impact of correlations on the
thermodynamic behavior. The mean-field approximation does not properly
account for these correlations, so we employ a technique called
*renormalization group theory* to account for spatial correlations which
will be discussed in future chapters.


# Correlations and Renormalization Group Theory \{#chap:renorm_group\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ising_model\]](#chap:ising_model)\{reference-type="ref+label"
reference="chap:ising_model"\},
[\[chap:ising_critical\]](#chap:ising_critical)\{reference-type="ref+label"
reference="chap:ising_critical"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

In this chapter we discuss the effect of range of correlations on phase
transitions. This correlations can be experimentally measured by
scattering experiments, as we will review in subsequent chapters. We
also introduce the renormalization group theory. This method has the
particularity that it accounts for large length scale fluctuations and
is essential for the study of phase transitions. We start with the
one-dimensional Ising model, as several of the concepts of the
Renormalization Group theory can be illustrated for this case. Then we
end with the study of two-dimensional systems.

## Correlations in a system

We showed earlier that the critical point is associated with a
divergence in the fluctuations in the magnetization, energy, or number
of particles. We call these variables, *order parameters*, *i.e*
fluctuating variables whose average values provide the information about
the order or broken symmetry of the system. From Monte Carlo
simulations, we observe that these fluctuations are highly correlated as
we approach the critical point.

It is useful to introduce the concept of *range of correlations*, which
consider the distance over which fluctuations in one region
$(s_i - \langle s_i \rangle)$ of space, are correlated or affected by
those in another region $(s_j - \langle s_j \rangle)$. Mathematically:

$$\begin\{aligned\}
\label\{Eq1\}
c_\{ij\} &= \langle (s_i - \langle s_i \rangle)(s_j - \langle s_j \rangle)\rangle \\
&=\langle s_i s_j \rangle -2 \langle s_i \rangle \langle s_j \rangle + \langle s_i \rangle \langle s_j \rangle \\
&= \langle s_i s_j \rangle - \langle s_i\rangle\langle s_j\rangle 
\end\{aligned\}$$

where $c_\{ij\}$ is called the *correlation function*.

If two sites are separated over a larger distance than the range
predicted by $c_\{ij\}$, these two points are uncorrelated of each other.
In terms of the magnetic model, two correlated spins are likely to be
aligned in the same direction. Thus, we can anticipate that the
spontaneous magnetization phenomena is associated with *long range
correlations*.

In fact, there is a direct relationship between the correlation function
and the fluctuations in magnetization, as we explain bellow:

$$\begin\{aligned\}
\left(\frac\{\partial \langle M\rangle\}\{\partial \beta h\}\right)_\{\beta,N\} &= \langle (\delta M)^2\rangle \\
&= \langle (M - \langle M \rangle)(M - \langle M \rangle)\rangle \\
&= \Bigg\langle \left(\sum_\{i=1\}^N \left[ s_i - \langle s_i\rangle \right] \right)\left(\sum_\{j=1\}^N \left[ s_j - \langle s_j\rangle \right]\right)\Bigg\rangle \\
&=\sum_\{i=1\}^N\sum_\{j=1\}^N \left[\langle s_i s_j \rangle - \langle s_i\rangle\langle s_j\rangle\right] \\
&=N\sum_\{j=1\}^N c_\{1,j\} 
\end\{aligned\}$$

where in the last part, we consider the fact that the lattice sites are
equivalent.

Thus, the divergence of fluctuations at the critical point is directly
related with long range of correlations between neighboring spins. When
this occurs, the distance of these correlations becomes macroscopic, so
even very distant spins will tend to align, forming the big blobs
observed in the Monte Carlo simulation.

In the mean-field approximation, we assumed an average magnetization for
the neighboring spins, $\langle s_i \rangle = m$ ($c_\{i j\} = 0$ for
$i \neq j$), thus neglecting spatial correlations. This explains the
discrepancies of our approximation with respect to the exact solution.
Quantifying the effective interaction between distal spins is a powerful
way to think about the physical effect of correlations.

## Renormalization Group Theory

In order to conceptualize the correlations in the Ising model, it is
extremely useful to understand how the correlations lead to effective
long-range interactions. *Renormalization Group Theory* is an analytical
method of coarse graining an interacting system to address how physical
interactions are impacted by spatial correlations correlations.

The basic procedure of Renormalization Group Theory is:

-   Begin with the original theory with all of relevant degrees of
    freedom (e.g. the spins $s_i$ in the Ising model).

-   Separate degrees of freedom into one set that will be summed over
    first and one that will summed over second.

-   Perform the summation over the set of degrees of freedom.

-   Define a new theory based on the remaining degrees of freedom that
    has coarse-grained (or renormalized) parameters.

-   Identify the renormalization group equations that pass the theory to
    the next level, successively solve these equations

## Renormalization Group Theory of the 1D Ising model

Consider the 1D Ising model with $h = 0$, defined according to the
system energy: $$\begin\{aligned\}
\beta E=-J\sum_\{i=1\}^N s_i s_\{i+1\}
\end\{aligned\}$$ The partition function is defined by: $$\begin\{aligned\}
Q(J,N) = \sum_\{s_1,s_2,...,s_N=-1,1\}\exp[J(...+s_1s_2+s_2s_3+s_3s_4++s_4s_5+...)]
\end\{aligned\}$$ Although this has an exact solution, we will employ
Renormalization Group Theory to address the physics of this model
(Fig. [\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/correlations_and_renormalization_group_theory/renormalized_ising_1d.png)\{width="\\linewidth"\}
:::

Arrange the partition into terms that isolate the even spins, *i.e.*:
$$\begin\{aligned\}
Q(J,N) = \sum_\{s_1,s_2,...,s_N=-1,1\}\exp[J(s_1s_2+s_2s_3)]\exp[J(s_3s_4+s_4s_5)]...
\end\{aligned\}$$ Perform the sum over all even spins, arriving at the
expression: $$\begin\{aligned\}
Q(J,N) = \sum_\{s_1,s_3,...,s_N=-1,1\}\left[e^\{J(s_1+s_3)\} + e^\{-J(s_1+s_3)\}\right]\cdot \\ 
\left[e^\{J(s_3+s_5)\}+e^\{-J(s_3+s_5)\}\right]...
\end\{aligned\}$$

Now, recast the partial-summed theory to look the same as the original
theory with $N/2$ spins and (perhaps) a new coupling constant $J'$. This
is accomplished if we can find a function $f (J)$ and coupling constant
$J'$ that satisfies $$\begin\{aligned\}
e^\{J(s+s')\}+e^\{-J(s+s')\} = f(J)e^\{J'ss'\}
\end\{aligned\}$$ for any values of $s, s' = -1,1$. If this transformation
is possible, the partition function becomes:

$$\begin\{aligned\}
\label\{Eq2\}
Q(J,N) &= \sum_\{s_1,s_2,...,s_N=-1,1\} f(J) e^\{J's_1s_3\}f(J) e^\{J's_3s_5\}... \\
&= [f(J)]^\{N/2\}Q(J',N/2)]
\end\{aligned\}$$

where $Q(J',N/2)$ has the same form as the original theory.

To find $f(J)$ and $J'$, consider two scenarios:

-   If $s = s' = \pm 1$, the coarse-graining criterion is:

    $$\begin\{aligned\}
    \label\{Eq3\}
    e^\{2J\}+e^\{-2J\}=f(J)e^\{J'\}
    \end\{aligned\}$$

-   If $s = -s' = \pm 1$, the coarse-graining criterion is:
    $$\begin\{aligned\}
    \label\{Eq4\}
    2=f(J)e^\{-J'\}
    \end\{aligned\}$$

These two criteria are easily solved to get: $$\begin\{aligned\}
J' &= \frac\{1\}\{2\} \log [\cosh(2J)]\\
f(J) &= 2\cosh^\{1/2\}(2J) 
\end\{aligned\}$$

and we define $g(J) = N^\{-1\} \log Q$, resulting in the expression:
$$\begin\{aligned\}
g(J')=2g(J)-\log\left[ 2\sqrt\{\cosh(2J)\} \right] 
\end\{aligned\}$$

These are the *renormalization group equations*. The inverses of these
equations are:

$$\begin\{aligned\}
J&=\frac\{1\}\{2\}\cosh^\{-1\}\left( e^\{2J'\} \right) \hspace\{0.2cm\} \\
g(J) &= \frac\{1\}\{2\} g(J') + \frac\{1\}\{2\} \log 2 + \frac\{J'\}\{2\} 
\end\{aligned\}$$

Successive application of the RG equations always decreases the coupling
constant $J$, except for the initial value $J \rightarrow \infty$
(Fig. [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}). The
loss of coupling implies the lack of long-range order associated with a
phase transition. Conditions where $J' = J$ is called a fixed point of
the theory, which identifies a state whose physical behavior is
length-scale invariant (*self-similar*). The 1D Ising model has a stable
fixed point at $J = 0$ and an unstable fixed point at
$J \rightarrow \infty$.

::: marginfigure
![image](figures/Physics/statistical_mechanics/correlations_and_renormalization_group_theory/rg_equation_coupling.png)\{width="\\linewidth"\}
:::

## Renormalization Group Theory of the 2D Ising model

The 2D Ising model can also be addressed using Renormalization Group
theory. We follow a similar procedure as in the 1D case, though several
approximations need to be made.

The partition function organized into terms such that:

$$\begin\{aligned\}
Q = \sum_\{s_1,s_2,...,s_N=-1,1\} \dotsc \exp[Js_5(s_1+s_2+s_3+s_4)]  \\ 
\times \exp[Js_6(s_2+s_3+s_7+s_8)] \dotsc
\end\{aligned\}$$

where $s_5$ and $s_6$ are among the spins that we will first sum over
(Fig [\[fig3\]](#fig3)\{reference-type="ref" reference="fig3"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/correlations_and_renormalization_group_theory/renormalization_2d_coupling.png)\{width="\\linewidth"\}
:::

Sum over half the spins to get: $$\begin\{aligned\}
Q = \sum_\{\text\{remaining \}s_i\} \dotsc \left[ e^\{J(s_1+s_2+s_3+s_4)\} + e^\{-J(s_1+s_2+s_3+s_4)\} \right] \\
\times \left[ e^\{J(s_2+s_3+s_7+s_8)\} + e^\{-J(s_2+s_3+s_7+s_8)\} \right] \dotsc
\end\{aligned\}$$

It would be desirable to find a set of renormalization group equations
based on: $$\begin\{aligned\}
\label\{Eq5\}
e^\{J(s_1+s_2+s_3+s_4)\} + e^\{-J(s_1+s_2+s_3+s_4)\} = f(J) e^\{J'(s_1s_2+s_1s_4+s_2s_3+s_3s_4)\}
\end\{aligned\}$$

however this will prove to be impossible.

Since there exist unique scenarios for $s_1$, $s_2$, $s_3$, $s_4$, such
that: $$\begin\{aligned\}
s_1 &=s_2=s_3=s_4 = \pm 1 \\
s_1 &=s_2=s_3=-s_4 = \pm 1 \\
s_1 &=s_2=-s_3=-s_4 = \pm 1 \\
s_1 &=-s_2=-s_3=-s_4 = \pm 1 
\end\{aligned\}$$

the model is overspecified (4 equations, 2 unknowns), and the
coarse-grained theory cannot have the same form as the original.

The simplest possibility is: $$\begin\{aligned\}
&e^\{J(s_1+s_2+s_3+s_4)\} + e^\{-J(s_1+s_2+s_3+s_4)\} = \\
 &f(J)\exp \left[ \frac\{1\}\{2\}J_1(s_1s_2+s_2s_3+s_3s_4+s_4s_1) + J_2(s_1s_3+s_2s_4) + J_3s_1s_2s_3s_4 \right] 
\end\{aligned\}$$

Since there are 4 equations and 4 unknowns, these can be solved to find:

$$\begin\{aligned\}
\label\{Eq6\}
J_1 &= \frac\{1\}\{4\}\log[\cosh(4J)] \\
J_2 &= \frac\{1\}\{8\}\log[\cosh(4J)] \\
J_3 &= \frac\{1\}\{8\}\log[\cosh(4J)] - \frac\{1\}\{2\}\log[\cosh(4J)] \\
f(J) &= 2[\cosh(2J)]^\{1/2\}[\cosh(4J)]^\{1/8\}
\end\{aligned\}$$

Inserting this into the partition function, we have:

$$\begin\{aligned\}
\label\{Eq7\}
Q(J,N) = [f(J)]^\{N/2\} \sum_\{N/2 spins\}\exp\left[ J_1\sum_\{ij\}\{'\}s_i s_j + \right. \\
 \left. J_2\sum_\{lm\}\{''\}s_l s_m + J_3\sum_\{pqrt\}\{'''\}s_p s_q s_r s_t \right] 
\end\{aligned\}$$

where $\sum\{'\}$ is over nearest neighbors, $\sum\{''\}$ is over
next-nearest neighbors, and $\sum\{'''\}$ is over four spins in a square.

This demonstrates that coarse-graining introduces new interactions into
the theory that did not exist in the original theory. However, we cannot
use this structure, since the next renormalization step will introduce
even more complex interactions. Therefore, we make the approximation:

$$\begin\{aligned\}
\label\{Eq8\}
J_1\sum_\{ij\}\{'\}s_is_j+J_2\sum_\{lm\}\{''\}s_ls_m \approx J'(J_1,J_2)\sum_\{ij\}\{'\}s_is_j
\end\{aligned\}$$

and neglect the 4-spin interaction ($J_3 \approx 0$).

This approximation gives:

$$\begin\{aligned\}
\label\{Eq9\}
Q(J,N) = [f(J)]^\{N/2\} Q[J'(J_1,J_2),N/2]
\end\{aligned\}$$

and the free energy is found from
$g(J)=\frac\{1\}\{2\}\log f(J)+\frac\{1\}\{2\}g(J')$.

To estimate $J'$, consider the case where all the spins are aligned.
Since we have $N/2$, there are $N$ near neighbors and $N$ next-near
neighbors, thus $J_1\sum_\{i j\}\{'\}s_is_j = NJ_1$ and
$J_2\sum_\{lm\}\{''\}s_ls_m = NJ_2$. Therefore, we estimate that
$J'\approx J_1+J_2$.

Using our RG equations for $J_1$ and $J_2$, we have: $$\begin\{aligned\}
J'=\frac\{3\}\{8\}\log[\cosh(4J)] 
\end\{aligned\}$$

Unlike the 1D Ising model, the RG equation for the 2D system has a
non-trivial fixed point $J_c = 0.50698$ that satisfies
$J_c = \frac\{3\}\{8\}\log[\cosh(4J_c)]$. The critical point $J_c$ dictates
conditions where the correlations diverge at all length scales (i.e. the
system is self-similar) and marks the onset of a phase transition
(Fig. [\[fig4\]](#fig4)\{reference-type="ref" reference="fig4"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/correlations_and_renormalization_group_theory/coupling_constant_values.png)\{width="\\linewidth"\}
:::

## Coarse-Grained Physical Models

Renormalization Group theory gives a systematic method for coarse
graining a physical model. In many instances, the insights that can be
gained by coarsegraining can provide fundamental insight into the
dominant physics. For example, coarse-graining concepts have been
pivotal in polymer physics (Fig. [\[fig5\]](#fig5)\{reference-type="ref"
reference="fig5"\}, [\[fig6\]](#fig6)\{reference-type="ref"
reference="fig6"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/correlations_and_renormalization_group_theory/Fig5.png)\{width="\\linewidth"\}
:::

::: marginfigure
![image](figures/Physics/statistical_mechanics/correlations_and_renormalization_group_theory/Fig6.png)\{width="\\linewidth"\}
:::
