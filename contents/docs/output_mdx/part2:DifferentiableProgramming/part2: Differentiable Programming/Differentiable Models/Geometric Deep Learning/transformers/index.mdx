# Transformers \{#chap:transformer\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Recurrent neural networks provide powerful tools for working with
series, but also have a powerful efficiency disadvantage. Because inputs
have to be processed step by step, there are limits to how far RNNs can
be parallelized on a modern processor. Transfomers offer an alternative
interpretation of sequence handling that allow for more efficient
processing.

![Transformers use the attention mechanism to estimate the importance of
each input in a
sequence.](figures/Differentiable Models/transformers/Transformers-Attention Mechanism.png)\{#fig:transformer\}

## Tokenizers and Vocabularies

Transformers assume that input comes as a sequence $x_1,\dotsc,x_L$. In
many applications, we can assume that each input is an integer
$$\begin\{aligned\}
    x_i \in \{1,\dotsc,N\}
\end\{aligned\}$$ where $N$ is a large but finite number. These integers
are assumed to index into a vocabulary set $V$, where $i$ represents
some element of the vocabulary. Tokenization is the process of
transforming an arbitrary input sequence into a sequence of integers
drawn from the given vocabulary. An embedding matrix is typically used
to associate to each index in the vocabulary an associated high
dimensional embedding vector $$\begin\{aligned\}
    W: \mathbb\{R\}^\{d \times N\}
\end\{aligned\}$$

Position encoding is a technique for representing the position $i$ of
input $x_i$ in the original sequence to the model. The
`PositionEncoding` layer provides a way to represent the position of a
token by a continuous value.

``` \{.python language="python"\}
class PositionEncoding(L: $\mathbb\{N\}$, M: $\mathbb\{N\}$, $\ell$ : $\mathbb\{N\}$=1e4):
  def $\lambda$() $\to$ $\mathbb\{R\}$[L, M]:
    pos = [1..L]
    dim = [1..M]
    phase = pos / ($\ell$ ** (dim/M))   
    where(d % 2 == 0, sin(phase), cos(phase))
```

## Transformer Equations

The attention mechanism consists of three terms. First there are a set
of query vectors $q_i \in \mathbb\{R\}^p$ for $i = 1,\dotsc,m$, then a set
of key vectors $k_j \in \mathbb\{R\}^p$ for $j = 1,\dotsc,n$, and then a
set of value vectors $v_j \in \mathbb\{R\}^r$. Here $r$ and $p$ are the
sizes of low dimensional learned embeddings. The key $k_j$ and value
$v_j$ are conceptually attached to the same input $j$. For query $q_i$
we write the attention mechanism as

$$\begin\{aligned\}
    \textrm\{Attn\}(q_i, \{k_j\}, \{v_j\}) &= \sum_\{j=1\}^n \alpha_\{ij\} v_j \\
    \alpha_\{ij\} &= \frac\{\exp(q_i^Tk_j)\}\{\sum_\{j'=1\}^n\exp(q_i^Tk_\{j'\})\}
\end\{aligned\}$$ For self-attention networks, we write $$\begin\{aligned\}
    q = h_Q(f),\qquad k=h_K(f),\qquad v=h_V(f)
\end\{aligned\}$$ where $h_Q$, $h_K$, and $h_V$ are embedding neural
networks.

One of the most powerful properties of the attention mechanism is
permutation invariance. Shuffling the order of the inputs does not
change the output from the system. Let's look at the pseudocode for the
attention mechanism.

``` \{.python language="python"\}
class Attention(m : $\mathbb\{N\}$, n: $\mathbb\{N\}$, p: $\mathbb\{N\}$):
  def $\lambda$(query: $\mathbb\{R\}$[m, p], key: $\mathbb\{R\}$[n, p], value: $\mathbb\{R\}$[n, p]): $\mathbb\{R\}$[m, p]:
    softmax(query*key$^T$/ sqrt(p)) * value

class AttentionHead(D: $\mathbb\{N\}$, Q: $\mathbb\{N\}$, K: $\mathbb\{N\}$):
    q = Linear(D, Q)
    k = Linear(D, K)
    v = Linear(D, K)

    def $\lambda$(query: $\mathbb\{R\}$[m, D], key: $\mathbb\{R\}$[n, D], value: $\mathbb\{R\}$[n, D]):
      Attention(q(query), k(key), v(value))
```

In practice, we will often find it useful empirically to run multiple
attention heads in parallel, much as we want to learn multiple
convolutional filters in parallel.

``` \{.python language="python"\}
class MultiHeadAttention(H: $\mathbb\{N\}$, D: $\mathbb\{N\}$, Q: $\mathbb\{N\}$, K: $\mathbb\{N\}$):
    heads = [AttentionHead(D, Q, K) for _ in H]
    linear = Linear(H * K, D)

    def $\lambda$(query: $\mathbb\{R\}$[p], key: $\mathbb\{R\}$[p, n], value: $\mathbb\{R\}$[r, d]):
      linear(concat([h(query, key, value) for h in heads]))  
```

We introduce a simple feed forward layer for use in more complex
architectures.

``` \{.python language="python"\}
def FeedForward(I: $\mathbb\{N\}$, F: $\mathbb\{N\}$):
  def $\lambda$(X):
    Linear(I, F) $\circ$ ReLU() $\circ$ Linear(F, I)(X)
```

There are several variants of the transformer that have been considered
in the literature. In this chapter, we consider the originally proposed
sequence-to-sequence encoder-decoder architecture. The encoder encodes
the input sequence into a vector representation and the decoder undoes
the encoding to make the prediction for the output sequence.

``` \{.python language="python"\}
class EncoderLayer(M: $\mathbb\{N\}$, H: $\mathbb\{N\}$, F: $\mathbb\{N\}$, dropout: $\mathbb\{R\}$):
    Q = K = max(M // H, 1)
    attention = Residual(
        MultiHeadAttention(H, M, Q, K), M, dropout)
    feed_forward = Residual(
        FeedForward(M, F), M, dropout)

    def $\lambda$(src: $\mathbb\{R\}$[A, B]):
      src = attention(src, src, src)
      feed_forward(src)
```

The `Encoder` chains together a sequence of encoding layers.

``` \{.python language="python"\}
class Encoder(L: $\mathbb\{N\}$, M: $\mathbb\{N\}$, H: $\mathbb\{N\}$, F: $\mathbb\{N\}$, dropout: $\mathbb\{R\}$):
    layers = [
        EncoderLayer(M, H, F, dropout) for _ in L]

    def $\lambda$(src: $\mathbb\{R\}$[L, D]):
        src += PositionEncoding(L, D)
        for layer in layers:
          src = layer(src)
        src
```

The decoder layer plays a similar role to the encoder layer, but with
the critical different that the decoder attempts to match the embedding
produced by the encoder to the target sequence.

``` \{.python language="python"\}

        
class DecoderLayer(M: $\mathbb\{N\}$, H: $\mathbb\{N\}$, F: $\mathbb\{N\}$, dropout: $\mathbb\{R\}$):
    Q = K = max(M // H, 1)
    attention_1 = Residual(
        MultiHeadAttention(H, M, Q, K), M, dropout)
    attention_2 = Residual(
        MultiHeadAttention(H, M, Q, K), M, dropout)
    feed_forward = Residual(
        Feedforward(M, F), M, dropout )

    def $\lambda$(tgt: $\mathbb\{R\}$[L, D], memory: Tensor) $\to$ Tensor:
        tgt = attention_1(tgt, tgt, tgt)
        tgt = attention_2(tgt, memory, memory)
        feed_forward(tgt)
```

The `Decoder` chains together a series of decoder layers.

``` \{.python language="python"\}
class Decoder(L: $\mathbb\{N\}$, M: $\mathbb\{N\}$, H: $\mathbb\{N\}$, F: $\mathbb\{N\}$, dropout: $\mathbb\{R\}$):
    layers = [
        DecoderLayer(M, H, F, dropout)
        for _ in L]
    linear = Linear(M, M)

    def $\lambda$(tgt: $\mathbb\{R\}$[L, D], memory: Tensor) $\to$ Tensor:
        tgt += PositionEncoding(L, D)
        for layer in layers:
          tgt = layer(tgt, memory)
        softmax(linear(tgt), dim=-1)
```

The `Transformer` itself simply chains a decoder and an encoder to
prdouce the full sequence-to-sequence architecture.

``` \{.python language="python"\}
class Transformer(NE: $\mathbb\{N\}$, ND: $\mathbb\{N\}$, M: $\mathbb\{N\}$, H: $\mathbb\{N\}$, F: $\mathbb\{N\}$, dropout: $\mathbb\{R\}$):
    encoder = Encoder(NE, M, H, F, dropout)
    decoder = Decoder(ND, M, H, F, dropout)

    def $\lambda$(src: $\mathbb\{R\}$[L, D], tgt: $\mathbb\{R\}$[L, D]) $\to$ Tensor:
        decoder(tgt, encoder(src))
```

## Masked Language Modeling

Masked language modeling is a type of pretraining in which a random
portion of the input data is removed and the model is trained to fill in
the missing data. The major advantage of this pretraining is it can be
done in a fully self supervised fashion. This and similar pretraining
methods have proven highly influential in recent years.
