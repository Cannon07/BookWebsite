# Linear Systems and Matrix Algebra \{#chap:linear_systems\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:vectors\]](#chap:vectors)\{reference-type="ref+label"
reference="chap:vectors"\},
[\[chap:optimization\]](#chap:optimization)\{reference-type="ref+label"
reference="chap:optimization"\}\
**Difficulty Level:** \*\

------------------------------------------------------------------------

This chapter will introduce you to matrix notation and will introduce
some common types of matrices including identity matrices, diagonal
matrices, symmetric matrices, triangular matrices and tridiagonal
matrices. We will then cover how to represent a system of linear
algebraic equations in matrix form and how such systems can be solved
through matrix inversion.

## Motivation

Linear system of equations need to be solved in numerous applications
including parabolic interpolation where the minima at every iteration is
determined by fitting a parabola through three points and finding the
minima. The general form of a parabola is $f(x) = ax^2+bx+c$, and the
minima is obtained by equating the derivative $f'(x)$ to zero, which
implies that the extrema occurs at $\dfrac\{-b\}\{2a\}$. Using the
constraints that $x_1$, $x_2$ and $x_3$ lie on a parabola, we can write
the following system of equations to solve for $a$, $b$ and $c$ to
obtain the parabola and thereby the minima.

$$\begin\{aligned\}
ax_1^2 + bx_1 + c &= f(x_1)\\
ax_2^2 + bx_2 + c &= f(x_2)\\
ax_3^2 + bx_3 + c &= f(x_3)
\end\{aligned\}$$ $$\underbrace\{
    \begin\{bmatrix\}
    x_1^2 & x_1 & 1 \\
    x_2^2 & x_2 & 1 \\
    x_3^2 & x_3 & 1 \\
    \end\{bmatrix\}\}_\{\text\{Known\}\}
\underbrace\{
    \begin\{bmatrix\}
    a\\
    b\\
    c
    \end\{bmatrix\}\}_\{\text\{Unknown\}\} = 
\underbrace\{
    \begin\{bmatrix\}
    f(x_1)\\
    f(x_2)\\
    f(x_3)
    \end\{bmatrix\}\}_\{\text\{Known\}\}$$

## Matrix Algebra Overview

A *matrix* consists of a rectangular array of elements represented by a
single symbol. As depicted in Figure
[\[fig:1\]](#fig:1)\{reference-type="ref" reference="fig:1"\}, $A$ is the
shorthand notation for the matrix and $a_\{ij\}$ designates an individual
*element* of the matrix.

::: marginfigure
![image](figures/part1b/linear_systems/matrixindices_0.png)\{width="2.7in"\}
:::

A horizontal set of elements is called a *row* and a vertical set is
called a *column*. The first subscript i always designates the number of
the row in which the element lies. The second subscript j designates the
column. For example, element $a_\{23\}$ is in row 2 and column 3.

The matrix in the Figure [\[fig:1\]](#fig:1)\{reference-type="ref"
reference="fig:1"\} has *m* rows and *n* columns and is said to have a
dimension of *m* by *n* (or *m* $\times$ *n*). It is referred to as an
*m* by *n* matrix.

## Common Types of Matrices

### Row and Column Matrices

Matrices with row dimension $m$ = 1 are called *row vectors*. Similarly,
matrices with column dimension $n$ = 1 are called *column vectors*. A
representation of a row vector is given below. $$\begin\{aligned\}
    b = [b_1 \hspace\{10pt \}b_2  \hspace\{10pt\} \cdots  \hspace\{10pt\} b_n]
\end\{aligned\}$$

### Square Matrices

Matrices where $m$ = $n$ are called *square matrices*. The diagonal
consisting of the elements $a_\{11\}$, $a_\{22\}$, and $a_\{33\}$ is termed
the *principal* or *main diagonal* of the matrix.

::: margintable
$$A=
    \begin\{bmatrix\}
    a_\{11\} & a_\{12\} & a_\{13\} \\
    a_\{21\} & a_\{22\} & a_\{23\} \\
    a_\{31\} & a_\{32\} & a_\{33\}
    \end\{bmatrix\}$$
:::

Square matrices are particularly important when solving sets of
simultaneous linear equations. For such systems, the number of equations
(corresponding to rows) and the number of unknowns (corresponding to
columns) must be equal for a unique solution to be possible.

There are a number of special forms of square matrices that are
important and should be noted:

### Symmetric Matrix

::: margintable
$$S=
    \begin\{bmatrix\}
    a_\{11\} & a_\{12\} & a_\{13\} \\
    a_\{12\} & a_\{22\} & a_\{23\} \\
    a_\{13\} & a_\{23\} & a_\{33\}
    \end\{bmatrix\}$$
:::

A *symmetric matrix* is one where the rows equal the columns - that is,
$a_\{ij\}$= $a_\{ji\}$ for all $i$'s and $j$'s.

### Diagonal Matrix

::: margintable
$$D=
    \begin\{bmatrix\}
    a_\{11\} &  &  \\
    & a_\{22\} &  \\
    &  & a_\{33\}
    \end\{bmatrix\}$$
:::

A *diagonal matrix* is a square matrix where all elements off the main
diagonal are equal to zero. Note that where large blocks of elements are
zero, they are left blank.

### Identity Matrix

An *identity matrix* is a diagonal matrix where all elements on the main
diagonal are equal to 1. The identity matrix has properties similar to
unity. That is,

::: margintable
$$I=
    \begin\{bmatrix\}
    1 &  &  \\
    & 1 &  \\
    &  & 1
    \end\{bmatrix\}$$
:::

$$\begin\{aligned\}
AI = IA = A
\end\{aligned\}$$

### Upper Triangular Matrix

An *upper triangular matrix* is one where all the elements below the
main diagonal are zero.

::: margintable
$$U=
    \begin\{bmatrix\}
    a_\{11\} & a_\{12\} & a_\{13\} \\
    & a_\{22\} & a_\{23\} \\
    &  & a_\{33\}
    \end\{bmatrix\}$$
:::

### Lower Triangular Matrix

::: margintable
$$L=
    \begin\{bmatrix\}
    a_\{11\} &  &  \\
    a_\{21\} & a_\{22\} &  \\
    a_\{31\} & a_\{32\} & a_\{33\}
    \end\{bmatrix\}$$
:::

A *lower triangular matrix* is one where all elements above the main
diagonal are zero.

### Banded Matrix

::: margintable
$$B=
    \begin\{bmatrix\}
    a_\{11\} & a_\{12\} & &  \\
    a_\{21\} & a_\{22\} & a_\{23\} & \\
    & a_\{32\} & a_\{33\} & a_\{34\} \\
    &  & a_\{43\} & a_\{44\} \\
    \end\{bmatrix\}$$
:::

A *banded matrix* has all elements equal to zero, with the exception of
a band centered on the main diagonal. The matrix given to the right has
a bandwidth of 3 and is given a special name - the *tridiagonal matrix*.

### Transpose Matrix

::: margintable
$$A'=
    \begin\{bmatrix\}
    a_\{11\} & a_\{21\} & a_\{31\} \\
    a_\{12\} & a_\{22\} & a_\{32\} \\
    a_\{13\} & a_\{23\} & a_\{33\}
    \end\{bmatrix\}$$
:::

The *transpose* of a matrix involves transforming its rows into columns
and its columns into rows. In other words, the element $a_\{ij\}$ of the
transpose is equal to the $a_\{ji\}$ element of the original matrix. The
transpose of \[A\] is given to the right.

### Permutation Matrix

::: margintable
$$P=
    \begin\{bmatrix\}
    0 & 0  & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 0
    \end\{bmatrix\}$$
:::

A *permutation* matrix (also called a *transposition* matrix) is an
identity matrix with rows and columns interchanged. For example, a
permutation matrix that is constructed by switching the first and third
rows and columns of a 3 $\times$ 3 identity matrix is displayed to the
right. Left multiplying a matrix $A$ by this matrix, as in $PA$, will
switch the corresponding rows of $A$. Right multiplying, as in $AP$,
will switch the corresponding columns.

## Matrix Operating Rules

### Matrix Addition

Addition of two matrices, say, $A$ and $B$, is accomplished by adding
corresponding terms in each matrix. The elements of the resulting matrix
$C$ are computed as\
$$\begin\{aligned\}
c_\{ij\} = a_\{ij\} + b_\{ij\}
\end\{aligned\}$$ for $i$ = 1, 2, . . . ,$m$ and $j$ = 1, 2, . . . ,$n$.
The addition of matrices satisfies the *commutative* and *associative*
properties:

::: margintable
**Matrix Addition Identities:**

     [A] + [B] = [B] + [A] 
    ([A] + [B]) + [C] = [A] + ([B] + [C])
:::

### Matrix Multiplication

The multiplication of a matrix $A$ by a scalar $g$ is obtained by
multiplying every element of $A$ by $g$.

::: marginfigure
![image](figures/part1b/linear_systems/matrixmultiplication_0.png)\{width="2.7in"\}
:::

The product of two matrices is represented as $C = AB$, where the
elements of $C$ are defined as $$\begin\{aligned\}
c_\{ij\} = \sum_\{k=1\}^\{n\}a_\{ik\}b_\{kj\}
\end\{aligned\}$$

where *n* = the column dimension of $A$ and the row dimension of $B$.
Figure [\[fig:2\]](#fig:2)\{reference-type="ref" reference="fig:2"\}
depicts how the rows and columns line up in matrix multiplication.
According to this definition, matrix multiplication can be performed
only if the first matrix has as many columns as the number of rows in
the second matrix.

::: margintable
**Matrix Multiplication Identities:**

    ([A][B])[C] = [A]([B][C])
    [A]([B] + [C]) = [A][B] + [A][C]
:::

If the dimensions of the matrices are suitable, matrix multiplication is
*associative* and *distributive*. However, multiplication is not
generally *commutative*. That is, the order of matrix multiplication is
important.

::: margintable
    [A][B] $\neq$ [B][A]
:::

Although multiplication is possible, matrix division is not a defined
operation. However, if a matrix $A$ is square and nonsingular, there is
another matrix $A^\{-1\}$, called the inverse of $A$, for which

$$\begin\{aligned\}
AA^\{-1\} = A^\{-1\}A = I
\end\{aligned\}$$

The inverse of a 2 $\times$ 2 matrix can be represented by
$$\begin\{aligned\}
A^\{-1\} = \dfrac\{1\}\{a_\{11\}a_\{22\}-a_\{12\}a_\{21\}\}   \begin\{bmatrix\}
a_\{22\} & -a_\{12\} \\
-a_\{21\} & a_\{11\} 
\end\{bmatrix\}
\end\{aligned\}$$

::: margintable
        Example:
        >> A = [3 5 1;5 8 1;-7 -2 4]
        A =
        3     5     1
        5     8     1
        -7    -2     4
:::

::: margintable
        >> det(A)
        ans =   
        
:::

::: margintable
        >> transpose(A) 
        ans =
        3     5    -7
        5     8    -2
        1     1     4
:::

::: margintable
        >> inv(A)   
        ans =   
        2.6154   -1.6923   -0.2308
        -2.0769    1.4615    0.1538
        3.5385   -2.2308   -0.0769
:::

Let's consider a matrix $A$ as given to the right and calculate its
*Determinant*, *Transpose* and *Inverse*.

## Representing Linear Equations as Matrices

It can be observed that matrices provide a concise notation for
representing simultaneous linear equations. For example, a 3 $\times$ 3
set of linear equations, $$\begin\{aligned\}
a_\{11\}x_1 + a_\{12\}x_2 + a_\{13\}x_3 = b_1\\
a_\{21\}x_1 + a_\{22\}x_2 + a_\{23\}x_3 = b_2\\
a_\{31\}x_1 + a_\{32\}x_2 + a_\{33\}x_3 = b_3
\end\{aligned\}$$

can be expressed as $$\begin\{aligned\}
Ax = b
\end\{aligned\}$$

where $A$ is the matrix of coefficients, $b$ is the column vector of
constants and $x$ is the column vector of unknowns.

A formal way to obtain a solution using matrix algebra is to multiply
each side of the equation by the inverse of $A$ to yield
$$\begin\{aligned\}
x = A^\{-1\}b
\end\{aligned\}$$

::: margintable
**Matrix Representation of Linear Equations:**\

$$\underbrace\{
    \begin\{bmatrix\}
    a_\{11\} & a_\{12\} & a_\{13\} \\
    a_\{12\} & a_\{22\} & a_\{23\} \\
    a_\{13\} & a_\{23\} & a_\{33\}
    \end\{bmatrix\}\}_\{\text\{A\}\}
    \underbrace\{
    \begin\{bmatrix\}
            x_\{1\}\\
            x_\{2\}\\
            x_\{3\}
    \end\{bmatrix\}\}_\{\text\{x\}\} = 
    \underbrace\{
    \begin\{bmatrix\}
    b_\{1\}\\
    b_\{2\}\\
    b_\{3\}
    \end\{bmatrix\}\}_\{\text\{b\}\}s$$
:::

It should be noted that the above equation is true only for systems with
equal number of equations and unknowns, that is *m* = *n*. Systems with
more equations (rows) than unknowns (columns), *m* \> *n*, are said to
be *over-determined*. A typical example is least-squares regression
where an equation with *n* coefficients is fit to *m* data points.
Conversely, systems with less equations than unknowns, *m* \< *n*, are
said to be *under-determined*. A typical example of under-determined
systems is numerical optimization.


# LU Factorization \{#chap:lu_factorization\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:linear_systems\]](#chap:linear_systems)\{reference-type="ref+label"
reference="chap:linear_systems"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter, we will learn about the LU factorization technique for
matrices and its mathematical underpinnings. We will explain how these
methods can be used to solve linear algebraic equations.

## LU Factorization \{#lu-factorization\}

Consider a system of three equations defined as $Ax = b$ where $A$ is
the coefficient matrix. This can be rearranged to get $$\begin\{aligned\}
 \label\{eq:Eq1\}
Ax - b = 0
\end\{aligned\}$$ Suppose that Eq.$\ref\{eq:Eq1\}$ could be expressed as an
upper triangular system, then $$\begin\{aligned\}
\begin\{bmatrix\}
u_\{11\} & u_\{12\} & u_\{13\} \\ 
0 & u_\{22\} & u_\{23\} \\ 
0 & 0 & u_\{33\}
\end\{bmatrix\}
\left \{
\begin\{tabular\}\{c\}
$x_1$ \\
$x_2$ \\
$x_3$
\end\{tabular\}
\right \}
= \left \{
\begin\{tabular\}\{c\}
$d_1$ \\
$d_2$ \\
$d_3$
\end\{tabular\}
\right \}
\end\{aligned\}$$ $$\begin\{aligned\}
 \label\{eq:Eq2\}
Ux - d = 0
\end\{aligned\}$$ Consider a lower diagonal matrix with 1's on the
diagonal such that when premultiplied with
Eq.[\[eq:Eq2\]](#eq:Eq2)\{reference-type="ref" reference="eq:Eq2"\}, we
get Eq.[\[eq:Eq1\]](#eq:Eq1)\{reference-type="ref" reference="eq:Eq1"\}.
$$\begin\{aligned\}
L =
\begin\{bmatrix\}
1 & 0 & 0 \\ 
l_\{21\} & 1 & 0 \\ 
l_\{31\} & l_\{32\} & 1
\end\{bmatrix\}
\end\{aligned\}$$ $$\begin\{aligned\}
 \label\{eq:Eq3\}
L(Ux - d) = Ax - b
\end\{aligned\}$$ Therefore from rules of matrix multiplication
$$\begin\{aligned\}
\begin\{gathered\}
LU = A\\
Ld = b
\end\{gathered\}
\end\{aligned\}$$

::: marginfigure
![image](figures/part1b/lu_factorization/lufactorization_0.png)
:::

As shown in Figure [\[fig:1\]](#fig:1)\{reference-type="ref"
reference="fig:1"\} a two step strategy is formed to obtain solutions:

1.  *LU factorization step*. $A$ is decomposed into lower $L$ and upper
    $U$ triangular matrices.

2.  *Substitution step*. $L$ and $b$ are used to evaluate intermediate
    matrix $d$ which is then used with $U$ to evaluate $x$ as in
    Eq.[\[eq:Eq2\]](#eq:Eq2)\{reference-type="ref" reference="eq:Eq2"\}.

``` \{.python language="python"\}
def lu_factorization(A: $\mathbb\{R\}^\{N \times N\}$):
  L = eye(N)
  U = A
  for i = 0:N-1
    for j = i+1:N
      ratio = U[j,i]/U[i,i]
      U[j,i:N] = U[j,i:N]-ratio*U[i,i:N]
      L[j,i] = ratio
  i=U\(L\b)
  
```

::: marginfigure
Output:

    L = 1.0000         0         0
       -0.6250    1.0000         0
             0   -0.4364    1.0000
             
    U = 8.0000   -5.0000         0
             0    6.8750   -3.0000
             0         0    9.6909%

    i = 0.8630
        0.3809
       -0.1689
:::
