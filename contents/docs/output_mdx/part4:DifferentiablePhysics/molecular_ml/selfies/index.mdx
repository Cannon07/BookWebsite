# SELFIES Representations \{#chap:selfies\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:graphconvs\]](#chap:graphconvs)\{reference-type="ref+label"
reference="chap:graphconvs"\},
[\[chap:equivariant_networks\]](#chap:equivariant_networks)\{reference-type="ref+label"
reference="chap:equivariant_networks"\},
[\[chap:molecular_dynamics\]](#chap:molecular_dynamics)\{reference-type="ref+label"
reference="chap:molecular_dynamics"\},
[\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

\"I will talk about SELFIES, a 100% robust molecular string
representation. The main feature is that every combination of letters
from the alphabet gives a meaningful molecule. The main application for
SELFIES are (deep) generative models, for the design of new functional
molecules. In contrast to other representations, here every output of
the model is syntactically and semantically correct thus no posterior
filtering or adaptations of the models are necessary. I will show how
this allowed, for example, the direct application of curiosity-based
exploration as well as DeepDreaming techniques. Finally, I will
highlight two works by other groups that have shown that SELFIES
outperform all other representations in string2string and img2string
translation tasks. Thus in some sense, it indicates that SELFIES is
easier to \"understand\" for machines -- which raises several exciting
questions.\"

References [@thiede2020curiosity], [@shen2021deep], [@krenn2020self]
