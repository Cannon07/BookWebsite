# Clifford Group Equivariant Networks \{#chap:deep_onet\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

## Introduction

Most people have focused on $SE(2)$ or $SE(3)$ or $SO(3)$. What kind of
algebraic tools can we use to enable neural networks to learn geometry?

Consider a graph structured equivariant structure. We can categorize
such networks a few categories

-   Regular representations: We use a group convolution. This is most
    famously done by the convolutional neural network. We perform an
    integral over the group, but this is challenging for Lie Group such
    as continuous groups.

-   Scalarization methods: Operate with invariant features unaffected by
    transformations. Or update vector-valued information using scalar
    multiplication. This limits expressibility

-   Steerable networks: Take tensor products of features.
    Representations of group acting on these is Wigner-D matrices. You
    have to decompose into irreps using Clebsch-Gordon coefficients.

This chapter looks at the Clifford algebra, or the geometric algebra.
These objects consist of scalar, vectors, bivectors, and trivectors more
generally called multivectors. We allow neural networks to operate on
multivectors.

## The Clifford Algebra

We first start with bilinear forms. We need a notion of distance
provided by the bilinear form $$\begin\{aligned\}
    \langle \cdot, \cdot \rangle \mapsto F
\end\{aligned\}$$ Can use inner product to compute angles and distances.

This introduces symmetry from the orthogonal group (the group of linear
transformations preserving this bilinear form is the orthogonal group).
For example, consider reflections and rotations.

An Euclidean inner product is given by $$\begin\{aligned\}
    v^TUw
\end\{aligned\}$$ where $U$ is a PST matrix (?). This framework works for
non-Euclidean metrics as well. The orthogonal group of the Lorentz group
is $O(1, 3)$. However the same definition holds.

The Clifford algebra is also called the geometric algebra. An algebra is
a vector space equipped with a product.

$$\begin\{aligned\}
    uv \in R, u \in R, v \in R
\end\{aligned\}$$ To give this expression meaning, we have to connect this
product to the geometry. Inner products give scalars. We want an inner
product not restricted to scalars. Here are axioms $$\begin\{aligned\}
    v^2 &= \langle v, v \rangle \\
    (u + v)^2 &= uv + vu + uv + vu 
\end\{aligned\}$$ The last condition holds if and only if
$$\begin\{aligned\}
    uv + vu = 2\langle u, v\rangle
\end\{aligned\}$$ We still need to define the space in which this
individual product lives. $$\begin\{aligned\}
    uv &= \frac\{uv + vu\}\{2\} + \frac\{uv-vu\}\{2\}  \\
    &= \langle u, v \rangle + u \wedge v
\end\{aligned\}$$ Here $\wedge$ is the wedge product, an anticommutative
product. It computes the area of the parallelogram the two vectors
define.

![image](figures/Differentiable Physics/Neural Diff Eq/Clifford Algebra/wedge_product1.png)

Can we find $w$ such that $$\begin\{aligned\}
    v \cdot w &= 0
\end\{aligned\}$$ and that the area with $v$ is the same as $u \wedge v$
$$\begin\{aligned\}
    wv &= u \wedge v = \frac\{1\}\{2\}(uv - vu) \\
    w &= \frac\{u - vuv^\{-1\}\}\{2\}
\end\{aligned\}$$

### The Algebra Basis

Start with $\mathbb\{R\}^3$ with basis $e_1$, $e_2$, $e_3$. Then we have
$$\begin\{aligned\}
    e_1 e_2 &= \langle e_1, e_2 \rangle + e_1 \wedge e_2 \\
    &= e_1 \wedge e_2
\end\{aligned\}$$ since the basis vectors are orthogonal. So in this case,
it equals the wedge product. This new object is called a basis plate. We
then create the $$\begin\{aligned\}
    \mathrm\{Cl\}(\mathbb\{R\}^3, \langle \cdot, \cdot \rangle )
\end\{aligned\}$$

![image](figures/Differentiable Physics/Neural Diff Eq/Clifford Algebra/basis1.png)

For orthogonal basis vectors we have $$\begin\{aligned\}
    e_1e_2 &= -e_2 e_1
\end\{aligned\}$$

Any multivector can be written in a basis of scalars, vectors,
bivectors, and trivectors. $$\begin\{aligned\}
    x &= x_0 1 + x_1 e_1 + x_2 e_2 + x_3 e_3 + x_\{12\} e_\{12\} + x_\{13\}e_\{13\} + x_\{23\} e_\{23\} + x_\{123\} e_\{123\}
\end\{aligned\}$$ A vector in the algebra is an element of the algebra
with non-vector parts of this basis set to 0.

### The Geometric Product

Let us take $$\begin\{aligned\}
    a, b \in \mathrm\{Cl\}(\mathbb\{R\}^2, q),. q(e_i) = \langle e_i, e_1 \rangle = 1 
\end\{aligned\}$$ Let us take the product $$\begin\{aligned\}
    ab &= (a_1 e_1 + a_2 e_2)(b_1 e_1 + b_2 e_2) \\
    & a_1e_1(b_1 e_1 + b_2 e_2) + a_2 e_2 (b_1 e_1 + b_2 e_2) \\
    &= a_1 b_1 e_1^2 + a_1 b_2 e_1 e_2 + a_2 b_1 e_2 e_1 + a_2 b_2 e_2^2 \\
    &= (a_1 b_1 + a_2 b_2) + \cdots
\end\{aligned\}$$

### Clifford Algebra Representations

The Clifford group is given by $$\begin\{aligned\}
\Gamma^\{[X]\}
\end\{aligned\}$$ This is a subset of the Clifford algebra. Its action
$\rho(w)$ is the twisted conjugation. We consider reflections in
particular. Every orthogonal transformation decomposes into multiple
reflections in the Clifford group. This action satisfies
$$\begin\{aligned\}
    \rho(w)(x_1 + x_2) &= \rho(w)(x_1) + \rho(w)(x_2) \\
    \rho(w)(x_1 x_2) &= \rho(w)(x_1) \rho(w)(x_2) \\
    \rho(w)(\phi x) &= \phi \rho(w)(x) \\
    \langle \rho(w)(x_1), \rho(w)(x_2) \rangle &= \langle x_1, x_2 \rangle
\end\{aligned\}$$ This $\rho$ is a representation of the orthogonal group.

### Polynomials and Grade Projections

Using the properties of $\rho(w)$ we can find two general operations.
First consider polynomials, $$\begin\{aligned\}
F: \mathrm\{Cl\}(V, q)^\ell \to \mathrm\{Cl\}(V, q)
\end\{aligned\}$$ Then we have $$\begin\{aligned\}
    \rho(w)(F(x_1, \dotsc, x_\ell)) &= F(\rho(w)(x_1),\dotsc, \rho(w)(x_\ell))
\end\{aligned\}$$ As an example consider $$\begin\{aligned\}
    F(x_1, x_2) &= \phi_0 + \phi_1 x_1 + \phi_2 x_2 + (\phi_\{11\}x_1^2 + \phi_\{12\} x_\{12\} + \phi_\{21\} x_\{21\} + \phi_\{22\} x_2^2)
\end\{aligned\}$$ Let $$\begin\{aligned\}
    (_)^m: \mathrm\{Cl\}(V, q) \to \mathrm\{Cl\}(V, q)
\end\{aligned\}$$ denote the projection onto the grade $m$ par
$$\begin\{aligned\}
    (_)^1(\phi_0 + \phi_1 x_1 + \phi_2 x_2 + (\phi_\{11\}x_1^2 + \phi_\{12\} x_\{12\} + \phi_\{21\} x_\{21\} + \phi_\{22\} x_2^2)) &= \phi_1 x_1 + \phi_2 x_2
\end\{aligned\}$$ These grade projections are equivariant

## Equivariant Layers

### Linear Layers

A polynomial up to the first order is just a linear map. Let us have
$$\begin\{aligned\}
    x_1, \dotsc, x_\ell
\end\{aligned\}$$ View these as channels. We can linearly combine
$$\begin\{aligned\}
    T_\{\phi, j\} &= \sum_\{i\} \phi_\{ji\}x_i
\end\{aligned\}$$ where $\phi_\{ji\} \in \mathbb\{R\}$.

### Parameterized Geometric Product

When taking a product of two Clifford alebra elements, you can decompose
in terms of grades $$\begin\{aligned\}
P_\phi(x_1, x_2)^\{(k)\} &= \sum_\{i\} \sum_j \phi_\{ijk\} (x_1^\{(i)\} x_2^\{(j)\})^\{(k)\} \dotsc
\end\{aligned\}$$ Each of these grade outputs can be viewed as a different
geometric product. Here again we have $$\begin\{aligned\}
    \phi_\{ijk\} \in \mathbb\{R\}
\end\{aligned\}$$

## Network Architecture

You can combine linear layers, parameterized geometric products. You can
also use gated nonlinearities as used elsewhere in the equivariant
literature. This architecture can be tested on the $n$-body dataset. In
the three dimensional case, this system is the same as the SEGNN.

This architecture can also be applied for $O(1, 3)$ jet-tagging for
top-tagging at CERN. Top-tagging is the problem of taking collision data
(momenta, energy) of hundreds of particles and giving a binary
classification of whether a top quark was produced. These particles are
moving at relativistic speeds, so we must respect Lorentz equivariance.
By using the infrastructure above and swapping the metric for the
$O(1, 3)$ metric, you can achieve a Lorentz-equivariant network. This
achieves performance similiar to LorentzNet.

## Conclusion

A powerful advantage of this framework is avoiding the need for the
group convolution. It also achieves state of the art results for
different groups. The models have computational overhead similar to
other steerable networks though.
