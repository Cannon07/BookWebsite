# Multi-dimensional Optimization \{#chap:multidimensional\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:newton_raphson\]](#chap:newton_raphson)\{reference-type="ref+label"
reference="chap:newton_raphson"\}\
**Difficulty Level:** \*\

------------------------------------------------------------------------

In this chapter, we will learn how to use contour and surface plots to
visualize two-dimensional functions. We will then cover how to find
local-optima of multi-dimensional functions with gradient descent.

## Multidimensional Optimization

Aside from one-dimensional functions, optimization also deals with
multidimensional functions. Recall from the previous chapters that our
visual image of a one-dimensional search was like a roller coaster. For
two-dimensional cases, the image becomes that of mountains and valleys.

Techniques for multidimensional unconstrained optimization can be
classified in a number of ways. For purposes of the present discussion,
we will divide them depending on whether they require derivative
evaluation. Those that require derivatives are called gradient, or
descent (or ascent), methods. The approaches that do not require
derivative evaluation are called non-gradient, or direct, methods. We
will be dealing with one of the popular ways of solving
multi-dimensional problems using gradients.

## Visualizing two-dimensional functions

::: marginfigure
![image](figures/part1b/multidimensional_optimization/contour0.png)\{width="2.7in"\}
:::

Before delving into optimization, let's look at two popular approaches
of visualizing two-dimensional functions namely contour plots and
surface plots.

A contour plot is a graphical technique for representing a 3-dimensional
surface by plotting constant $f(x,y)$ slices, called contours, on a
2-dimensional format. That is, given a value for $f(x,y)$, lines are
drawn for connecting the ($x,y$) coordinates where that $f(x,y)$ value
occurs.

Surface plots are diagrams of three-dimensional data. Rather than
showing the individual data points, surface plots show a functional
relationship between a designated dependent variable $f(x,y)$, and two
independent variables $x$ and $y$. The plot is a companion plot to the
contour plot.

::: marginfigure
![image](figures/part1b/multidimensional_optimization/surface.png)\{width="2.7in"\}
:::

To make this easier to understand, let's look at the surface and contour
plots of a function $f(x,y) = \sin(x) + \cos(y)$.

## Gradient-Descent Method

Let's consider a two-dimensional function $f(x,y)$ depending on the
variables $x$ and $y$. It can be observed that most of the
two-dimensional functions can be viewed as comprising of mountains and
valleys which comprise of local optima.

So, let's use the mountain-valley analogy to look at the approach of
gradient descent. Suppose that you are lost while trekking, and you have
to reach a lake which is at the lowest point of the mountain (a.k.a
valley). A twist is that you are blindfolded and you have zero
visibility to see where you are headed. The best way is to check the
ground near you and observe where the land tends to descend. This will
give an idea in what direction you should take your first step. If you
follow the descending path, it is very likely you would reach the lake.
This is the complete intuition behind gradient descent.

Mathematically, this can be interpreted as determining the slope along
each dimension and taking a small step in that direction to go down the
slope. The gradient of the two-dimensional function is evaluated as
given below: $$\begin\{aligned\}
\nabla_x &= \frac\{\partial f(x,y)\}\{\partial x\}\\
\nabla_y &= \frac\{\partial f(x,y)\}\{\partial y\}
\end\{aligned\}$$

where $\nabla_x$ is the gradient along x-direction and $\nabla_y$ is the
gradient along y-direction. The update rule of gradient descent (i.e
taking a step in the gradient direction) is given by

$$\begin\{aligned\}
x_\{i+1\} &= x_\{i\} - \alpha \nabla_x\\
y_\{i+1\} &= y_\{i\} - \alpha \nabla_y
\end\{aligned\}$$

where $\alpha$ is the length of the step we want to take down the hill.
We can generalize this discussion into a general gradient descent
algorithm for $N$ dimensional function.

``` \{.python language="python"\}
def gradient_descent(loss: $\mathbb\{R\}[N] \to \mathbb\{R\}$, guess: $\mathbb\{R\}[N]$, $\alpha$ : $\mathbb\{R\}$, $\epsilon$: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}$[N]:
  change = $\infty$
  while change > $\epsilon$:
    gradient = $\nabla$loss(guess)
    new = guess - $\alpha$*gradient
    change = $\|$new - guess$\|$ /$\|$guess$\|$
    if change < $\epsilon$:
      break
    guess = new
  guess
```

A simple version of gradient descent is shown above. The relative
approximation error between the current guess and the previous guess is
chosen for termination. Alternatively, the absolute value of the
gradient can also be chosen. This implies that when the gradient is
close to zero, we are near an optima and hence can terminate.

### Step-Size and Initial Guess

The step-size variable $\alpha$ controls how large of a step we take
downhill during each iteration. If we take too large of a step, we may
step over the minimum. However, if we take small steps, it will require
many iterations to arrive at the minimum.

Another important observation to keep note of is that gradient descent
converges to a local optima. This means, in a function with multiple
valleys i.e multiple optima, the initial guess (i.e. where we start
going down the hill) matters a lot.

Let's look at solving a two-dimensional optimization problem using
gradient descent. Suppose a set of measurements are given of the
position of a moving particle in one dimension. Let's try to predict the
time evolution of the particle by assuming a linear model.

Let's try to fit a linear model to the observed data in the fashion
below

$$\begin\{aligned\}
p(t) &= at + b
\end\{aligned\}$$

where $a$ is the velocity of the particle, $b$ is the initial position
and $p(t)$ is the position at any given time $t$.

Let's define our loss function to be the least-square error between our
model and the data. Let $(t_i,p_i)$ be our data points where $t_i$ is
the $i$th time and $p_i$ is the $i$th position. It is given by

$$\begin\{aligned\}
L(a,b) &= \sum_\{i=1\}^\{n\}(p_i-(at_i+b))^2
\end\{aligned\}$$

Now, the objective is reduced to find the parameters $a$ and $b$ which
minimize the above cost function. To do this, we require the computation
of the gradient of the cost function with respect to our parameters. We
can use automatic differentiation in general, but for this simple case,
we can directly compute gradients to be $$\begin\{aligned\}
\frac\{\partial L\}\{\partial a\} &= -2\sum(p_i-at_i-b)x_i\\
\frac\{\partial L\}\{\partial b\} &= -2\sum(p_i-at_i-b)
\end\{aligned\}$$

Using these calculations, we iteratively take steps on the cost function
along the gradient direction to obtain the optimal parameters $a$ and
$b$.

``` \{.python language="python"\}
def L(a: $\mathbb\{R\}$, b : $\mathbb\{R\}$, x: $\mathbb\{R\}$[N], y: $\mathbb\{R\}$[N]) $\to$ $\mathbb\{R\}$:
  cost = 0
  for i: cost += (y[i] - a*x[i] - b)$^2$
  return cost/N

a = Normal(0.8, 1)
b = Normal(2.2, 1)
def regression(x: $\mathbb\{R\}$[N], y:$\mathbb\{R\}$[N]) $\to$ $\mathbb\{R\}$:   
  return gradient_descent(L(x=x, y=y), [a, b], $\alpha$=1e-7, $\epsilon$=1e-8)
```

## Newton Method

The principle of using the Newton method for single dimensional
optimization can be extended to multi-dimensional functions. Giving a
recap, in the Newton method for optimization of one-dimensional
functions, the update rule is given by:

$$\begin\{aligned\}
x_\{i+1\} = x_i - \dfrac\{f'(x_i)\}\{f''(x_i)\}
\end\{aligned\}$$

This simple update step can be now extended to multi-dimensional
functions with continuous second derivatives. Let's take a two
dimensional function $f(x,y)$. Now, we have an initial guess of our
optima at $x_0$ and $y_0$. The updates of these variables are obtained
by independently computing the gradients along the two-dimensions. These
update equations are given by:

$$\begin\{aligned\}
X_\{i+1\}  = X_\{i\} - [Hf(X_i)]^\{-1\} \nabla f(X_i)
\end\{aligned\}$$

where $X_\{i\}$ is the guess vector at $i$th iteration and $H$ is the
hessian of the function given by

$$\begin\{aligned\}
Hf(x,y) = 
\begin\{bmatrix\}
\frac\{\partial^2 f\}\{ \partial x^2\} && \frac\{\partial f\}\{\partial x \partial y\}\\ \\
\frac\{\partial f\}\{\partial y \partial x\} && \frac\{\partial^2 f\}\{\partial y^2\}
\end\{bmatrix\}
ko
\end\{aligned\}$$

The partial derivatives in this case are evaluated at the current
estimate. Similar approach of computing partial derivatives and
iteratively updating the estimates can be done to extend it to higher
dimensions.

The intuition for the Hessian comes from the taylor series expansion of
multi-dimensional functions. Let's look at the taylor series expansion
of $f(x+\delta x,y + \delta y)$ centered at $(x,y)$. This is given by
$$\begin\{aligned\}
f(x+\delta x,y + \delta y) &= f(x,y) + \frac\{\partial f\}\{\partial x\}\delta x + \frac\{\partial f\}\{\partial y\}\delta y + \frac\{1\}\{2!\}(\frac\{\partial^2 f\}\{\partial x^2\} \delta x^2 + 2\frac\{\partial f\}\{\partial x \partial y\}\delta x\delta y  + \frac\{\partial^2 f\}\{\partial y^2\} \delta y^2) + \dots
\end\{aligned\}$$

This equation can be re-written as $$\begin\{aligned\}
f(X + \delta X) &= f(X) + \nabla f(X)\cdot \delta X + \delta X\cdot H f(X)\cdot\delta X^T  + \dots
\end\{aligned\}$$

where $\nabla f(X)$ is the first order partial derivatives of $f(x,y)$
and $Hf(X)$ is the hessian, which is the second order partial
derivatives of $f(x,y)$. Now revisiting the Newton method's update rule,
it can be seen that the multi-dimensional update rule is the same as the
following one-dimensional update rule $$\begin\{aligned\}
x_\{i+1\} = x_i - \dfrac\{f'(x_i)\}\{f''(x_i)\} 
\end\{aligned\}$$

with $f'(x)$ corresponding to $\nabla f(X)$ and $f''(x)$ corresponding
to $Hf(X)$.

## Exercises

1.  Use gradient descent to find an approximate local minima of the
    function $f(x,y) = \sin(x) + \cos(y)$

2.  Use the Newton method to find an approximate local minima of
    $f(x, y) = \sin(x) + \cos(y)$


# Basic Root Finding \{#chap:newton_raphson\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:math_basics\]](#chap:math_basics)\{reference-type="ref+label"
reference="chap:math_basics"\}\
**Difficulty Level:** \*\

------------------------------------------------------------------------

One of the foundational problems of numerical analysis is finding the
roots of functions. We start with one dimensional functions, and in
later chapters will generalize to higher dimensional systems.

In particular, we explain how to solve and implement a root finding
problem with the Newton-Raphson method and how to appreciate the concept
of quadratic convergence. We will then cover numeric methods for
estimating roots and how to manipulate and determine the roots of
polynomials.

## Open Methods

::: marginfigure
![image](figures/part1b/newton_raphson/openmethod0.png)\{width="2.4in"\}
:::

In the bracketing methods of root-finding, the root is located within an
interval prescribed by a lower and an upper bound. In contrast, open
methods require only a single starting value or two starting values that
do not necessarily bracket the root. In *bracketing methods*, the
estimate of the root gets better with every iteration. This cannot be
ensured in the case of *open methods*. However, when the open methods
converge, they converge much faster than the bracketing methods. The
distinction between bracketing methods and open methods is shown in
Figure [\[fig:1\]](#fig:1)\{reference-type="ref" reference="fig:1"\} by
depicting Bisection (bracketing method) and Newton-Raphson (open
method).

## Newton-Raphson Method

::: marginfigure
![image](figures/part1b/newton_raphson/newtonraphson0.png)\{width="2.4in"\}
:::

The most widely used of all root locating formulas amongst the open
methods is the *Newton-Raphson method*. The method starts with an
initial guess of the root at $x_i$ and the tangent to the function is
drawn at the point $[x_i,f(x_i)]$. The point where this tangent crosses
the x-axis usually represents an improved estimate of the root. This can
be observed from Figure [\[fig:2\]](#fig:2)\{reference-type="ref"
reference="fig:2"\}. Using the definition of the derivative (slope of
tangent), this can be derived as $$\begin\{aligned\}
f'(x_i) &= \dfrac\{f(x_i) - 0\}\{x_i - x_\{i+1\}\} \\
x_\{i+1\} &= x_i - \dfrac\{f(x_i)\}\{f'(x_i)\}
\end\{aligned\}$$

The above equation is called the *Newton-Raphson formula* which iterates
over the root search. In root-finding problems in many variables, the
equation becomes:
$$\vec\{x\}_\{i+1\} = \vec\{x\}_i - \mathcal\{J\}(x_i)^\{-1\} \mathcal\{F\}(\vec\{x_i\})$$
where the product of the Jacobian times the function value at the point
$\vec\{x\}_i$

Although the Newton-Raphson method is often very efficient, there are
situations where it performs poorly. For example, let's try to evaluate
the root of $f(x) = x^\{10\} - 1$ using Newton-Raphson method with an
initial guess of x= 0.5. The Newton-Raphson formula for this function is
given by $$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{x_i^\{10\} - 1\}\{10x_i^9\}
 
\end\{aligned\}$$ This can be used to compute the next root estimate. The
root estimates with the iterations are given in the table.

::: margintable
   **i**   **$x_i$**   **$|\epsilon_a|$ in %**
  ------- ----------- -------------------------
     0        0.5     
     1       51.65             99.032
     2      46.485             11.111
     3      41.8365            11.111
     4      37.6585            11.111
     .                
     .                
     .                
     .                
    40     1.002316             2.130
    41     1.000024             0.229
    42         1                0.002
:::

Since, the first guess is near a region where the slope is near zero,
flings the solution far away from the initial guess to a new value
($x = 51.65$) where $f(x)$ has an extremely high value. The solution
then plods along for over 40 iterations until converging on the root
with adequate accuracy.

There are some additional cases where the Newton-Raphson exhibits poor
performance. Some of these are depicted in Figure
[\[fig:3\]](#fig:3)\{reference-type="ref" reference="fig:3"\}. In (a), if
the root is near the inflection point (i.e. $f'(x) = 0$), the iterations
beginning at $x_0$ progressively diverge from the root. In (b), the
tendency of the Newton-Raphson technique to oscillate around a local
maximum or minimum is demonstrated. (c) shows how an initial guess that
is close to one root can jump to a location several roots away. This is
observed when near-zero slopes are encountered. This scenario can be
observed well in (d), where the slope is zero and the next root estimate
never hits the x-axis. Thus, there is no general convergence criterion
for Newton-Raphson. Its convergence depends on the nature of the
function and on the accuracy of the initial guess. The only remedy is to
have an initial guess that is sufficiently close to the root.

::: marginfigure
![image](figures/part1b/newton_raphson/newtonraphsonpoor0.png)\{width="2in"\}
:::

Let's go back to the example function that was used to demonstrate the
other root finding methods and re-solve it using the Newton-Raphson
method. Let's implement the same relative convergence criterion. We
start with the function we seek to optimize

``` \{.python language="python"\}
def f(x: $\mathbb\{R\}$):
  return $x^3$ - 3.7*x$^2$ + 4.54*x - 1.848    
```

Next we implement Newton-Raphson

``` \{.python language="python"\}
def newton_raphson(f: $\mathbb\{R\} \to \mathbb\{R\}$, start: $\mathbb\{R\}$ = 0): $\mathbb\{R\}$  
  old = start
  while True:
    new = old - f(old)/grad(f)(old)                 
    # Convergence criterion                              
    if abs((new-old)/new) < 10**(-8):           
      break
    old = new
  return new
```

Note that Newton Raphson converges much faster than the false position
method for the same function. As an exercise, try the function $x^3-x-3$
with $x=0$ as the starting point.

## Secant Method

The Newton-Raphson method requires the evaluation of the derivative for
its implementation. Although this is not inconvenient for polynomials
and many other functions, there are certain functions whose derivatives
may be difficult or inconvenient to evaluate. For these cases, the
derivative can be approximated by a backward finite divided difference:
$$\begin\{aligned\}
 f'(x_i) \cong \dfrac\{f(x_\{i-1\}) - f(x_\{i\})\}\{x_\{i-1\} - x_\{i\}\}
\end\{aligned\}$$ The approximation is substituted into the
*Newton-Raphson formula* to obtain the following iterative equation:
$$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{f(x_i)(x_\{i-1\} - x_\{i\})\}\{f(x_\{i-1\}) - f(x_\{i\})\}
\end\{aligned\}$$ The above equation is called the *Secant-method
formula*. Notice that the approach requires two initial estimates of x.
However, because f (x) is not required to change signs between the
estimates, it is not classified as a bracketing method.

Rather than using two arbitrary values to estimate the derivative, an
alternative approach involves a fractional perturbation of the
independent variable to estimate the derivative. $$\begin\{aligned\}
 f'(x_i) \cong \dfrac\{f(x_\{i\} + \delta) - f(x_\{i\})\}\{\delta\}
\end\{aligned\}$$ where $\delta$ is a small perturbation. The
approximation is substituted into the *Newton-Raphson formula* to obtain
the following iterative equation: $$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{\delta \times f(x_i)\}\{f(x_\{i\} + \delta) - f(x_\{i\})\}
\end\{aligned\}$$ We call this the *modified secant method*. It provides a
nice means to attain the efficiency of Newton-Raphson without having to
compute derivatives.

Let's use the same example and find a root using the secant method. Note
that this requires two starting points.

``` \{.python language="python"\}
def secant_method(f: $\mathbb\{R\} \to \mathbb\{R\}$, now: $\mathbb\{R\}$ = 120, prev: $\mathbb\{R\}$ = 100): $\mathbb\{R\}$                                            
  while True: 
    f_prime = (f(prev)-f(now))/(prev-now)        
    new = now-f(now)/f_prime                       
    # Convergence criterion 
    if abs((new-now)/new) < 10**(-8):           
      break 
    prev = now
    now = new
  return new
```

Note that the secant method also converges much faster than the false
position method in this case for the same tolerance level.

## Optimization as Root Finding

In this section, we will learn how to use root-finding techniques to
find local optima. We will understand how Newton's method for
on-dimensional optimization is an extension from the Newton-Raphson
method for root finding.

## Newton's Method

The same techniques learnt in root-finding can be extended to
optimization by changing the function under consideration to $f'(x)$
from $f(x)$ i.e. the values of x where $f'(x) = 0$ are local optima (or)
inflection-points.

Considering the Newton-Raphson method, the update rule for root-finding
is:

$$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{f(x_i)\}\{f'(x_i)\}
\end\{aligned\}$$

Using the same Newton-Raphson method, the update rule for optimization
is:

$$\begin\{aligned\}
x_\{i+1\} = x_i - \dfrac\{f'(x_i)\}\{f''(x_i)\}
\end\{aligned\}$$

This approach to optimization is called the Newton's Method in one
dimension. It is worth highlighting that this is an open method for
optimization.

``` \{.python language="python"\}
def newton_raphson_opt(f: $\mathbb\{R\} \to \mathbb\{R\}$, start: $\mathbb\{R\}$, tol:$\mathbb\{R\}$ = 1e-8): 
  old = start
  while True: 
    new = old-grad(f)(old)/grad(grad(f))(old)
    if abs((new-old)/new) < tol:
      break
    old = new
  return new
```

In a similar manner, other root finding techniques can also be used for
optimization to converge to local optima where the derivative is zero.

## Exercises

1.  Apply the Newton-Raphson method to the function $x^3 - x - 3$ with
    $x=0$ as the starting point

2.  Apply the Secant method to the function $x^3 - x - 3$ with $x=0$ and
    $x=2$ as the two starting points.

3.  Determine a local optima of the function $f(x) = \exp(x)-2x$ using
    the Newton-Raphson method
