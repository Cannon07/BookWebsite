# Neural Tangent Kernel \{#chap:ntk\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

The neural tangent kernel provides a theoretical methodology to study an
\"infinite-width\" approximation of a fully connected network. Even more
interestingly, the evolution of a neural network under gradient descent
can also be described by a suitable kernel.

## Introduction to Neural Tangent Kernels

A recursion relation on neural networks can be mapped over to a relation
on Gaussian processes. For a single layer network, we can write

$$\begin\{aligned\}
f_i^0(x) &= b_i^0 + \sum_j W^0_\{ij\} x_j \\
f_i^1(x) &= b_i^1 + \sum_j \sigma(W^0_\{ij\}) x_j \\
K^0(x, x') &= E[f_j(x)f_j(x')]
\end\{aligned\}$$

Let $f$ denote the output of the full network. For simplicity, assume
that $f$ has a one dimensional output. Let $\theta_j$ denote the $j$-th
learnable parameter. We then define the tangent kernel as

$$\begin\{aligned\}
\Theta(x, x') &= \sum_j \frac\{\partial f(x)\}\{\partial \theta_j\} \frac\{\partial f(x')\}\{\partial \theta_j\}
\end\{aligned\}$$

Note that the type of this kernel is $$\begin\{aligned\}
\Theta: \mathbb\{R\}^n \times \mathbb\{R\}^n \to \mathbb\{R\}
\end\{aligned\}$$ where $x, y \in \mathbb\{R\}^n$.

The neural tangent kernel lets us model the dynamics of the neural
network during training. Let us define a dataset

$$\begin\{aligned\}
D &= \{(x_1, y_i),\dotsc, (x_N, y_N)\}
\end\{aligned\}$$

We can then model the time evolution of the output of the neural network
$f$ by the function

$$\begin\{aligned\}
\frac\{df(x; \theta)\}\{dt\} &= - \sum_\{i=1\}^N \Theta(x, x_i) \frac\{\partial \mathcal\{L\}(f(x_i; \theta), y_i)\}\{\partial f(x_i; \theta)\} 
\end\{aligned\}$$

We can interpret the term $\Theta(x, x_i)$ as a measure of the influence
of the gradient of the loss on the $i$-th datapoint $(x_i, y_i)$ on the
output of the model. To first order, the evolution of the neural network
during training g is given by the linear model $$\begin\{aligned\}
f(x; \theta(t)) &= f(x; \theta(0)) + \sum_j \frac\{\partial f(x; \theta_j(0))\}\{\partial \theta_j\} (\theta_j(t) - \theta_j(0))
\end\{aligned\}$$

As the width of the neural network goes to infinity, its training
behavior converges to this linear model. Note that for a finite width
neural network, the neural tangent kernel $\Theta$ is a random quantity
since it depends on the initialization of $\theta$. However for an
infinitely wide network, the neural tangent kernel converges to a
deterministic quantity.

We can also compute the evolution of the parameters at a general point
$x$. There is a Gaussian process that we can write down that describes
the evolution of this system as a point process. This is called the NNGP
kernel as opposed to the NNTK kernel.

When are NNTK/NNGP methods useful? They serve as a useful starting point
for perturbation theoretic expansions. NNTK methods can also be
competitive with neural network methods in their own right as predictive
models.
