# SchNet \{#chap:schnet\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

SchNet aims to develop continuous filter convolutional layers that
generalize the notion of convolution to point clouds. The goal of
designing SchNet is to enable accurate computation of forces for
molecular dynamics simulations and other applications. In particular,
these models must be capable of modeling multiple conformations of a
given small molecule.

## Basic Formulation

Suppose we have $n$ objects $$\begin\{aligned\}
X &= (x_1,\dotsc, x_n)
\end\{aligned\}$$ with $x_i \in \mathbb\{R\}^F$. Each object is paired with
position $r_i \in \mathbb\{R\}^D$. The SchNet network features a series of
interaction rules which update the featurization for each atom, without
changing its dimensionality. SchNet does not add or remove atoms.

## Components of SchNet

The core SchNet architecture consists of a series of interaction blocks
which exchange information between atoms followed per atom-wise update
blocks. Interactions between atoms are gated by nonlinear functions of
the distance between the two atoms.

![Schnet Architecture
Diagram](figures/Differentiable Physics/Neural Potentials/Schnet/schnet_diagram.png)\{#fig:schnet_diagram\}

### Molecular Representation

SchNet does not allow for modifying the feature dimension of atoms
within its continuous convolutions. For this reason, atoms must be
initialized with random vectors of the appropriate shape at the start.
Initializations are shared across atom type.

$$\begin\{aligned\}
x_i^0 &= a_\{Z_i\}
\end\{aligned\}$$

### Atom-wise Layers

Atom-wise layers update each atom in isolation through a learnable
linear layer.

$$\begin\{aligned\}
x^\{\ell + 1\}_i &= W^\{\ell+1\}x^\ell_i + b^\ell
\end\{aligned\}$$

### Interaction

Interaction layers feature residual blocks that help preserve the
original atomic featurizations by adding them onto the new interaction
featurizations.

$$\begin\{aligned\}
x^\{\ell + 1\}_i &= x^\ell_i + v^\ell_i
\end\{aligned\}$$

The interaction terms are given by the output of a convolutional layer.
This network computes terms by the following update $$\begin\{aligned\}
x^\{\ell+1\}_\{i\} &= \sum_j x_j \circ W^\ell(r_i - r_j)
\end\{aligned\}$$ where $a \circ b$ represents elementwise multiplication
and $W^\ell$ is a filter generating network. The filter generating
network has type $$\begin\{aligned\}
W^\ell: \mathbb\{R\}^D \to \mathbb\{R\}^F
\end\{aligned\}$$

### Filter Generating Networks

A filter generating network constructs the filter $W^\ell$ that we need
to implement a continuous convolution. This network starts by computing
pairwise distances between atoms. $$\begin\{aligned\}
d_\{ij\} &= \|r_i - r_j\|
\end\{aligned\}$$

$$\begin\{aligned\}
e_k(r_i - r_j) &= \exp(-\gamma \|d_\{ij\} - \mu_k\|^2)
\end\{aligned\}$$

This function is expanded for different values of $\mu$ from 0 to 30
angstroms and $\gamma$ is set to 10 angstroms. The output feature vector
is fed through two dense layers which use the $\textrm\{ssp\}$ (shifted
softplus) activation.

$$\begin\{aligned\}
\textrm\{ssp\}(x) &= \ln ( 0.5 e^x + 0.5 )
\end\{aligned\}$$

The expanded feature space allows for design of different filters that
emphasize different distance ranges.

## Training Against Forces

Models are trained with a combination of energies and forces. This joint
training helps ensure that the models learn physically accurate
energies. $$\begin\{aligned\}
\mathcal\{L\}(\hat\{E\}, (E, F_1,\dotsc,F_n)) &= \rho \|E - \hat\{E\}\|^2 + \frac\{1\}\{n\} \sum_\{i=0\}^n \|F_i  - \left ( \frac\{\partial \hat\{E\}\}\{\partial R_i\} \right )^2 \|^2 
\end\{aligned\}$$

## Exercises

1.  
