# Introduction to Scientific Machine Learning \{#ch:intro-molecular-ml\}

While applying machine learning algorithms to physical systems, there
typically are additional constraints in the form of respecting any given
laws of physics often described by general nonlinear partial
differential equations. In this chapter, we will introduce approaches to
incorporate such additional constraints into machine learning
algorithms. We will also discuss approaches to discover and learn the
underlying partial differential equations for systems where the
underlying physics can emerge from collected data.

This chapter will cover the three broad ways in which one can impose the
known physics and symmetries in the physical systems. Often, a
combination of the three broad verticals are employed.

-   Chapter Part-1: Hard-code the known governing equations and boundary
    conditions in the **loss function** while training the
    physics-driven model, which comprises of approaches that fall under
    the category of **physics-informed models**

-   Chapter Part-2: Build in the physics-based equations and constraints
    **within the model architecture** so that intermediate variables are
    physically motivated

-   Chapter Part-3: Impose known symmetries of the system that need to
    be obeyed **within the featurizer(s)** before feeding into the model

## Physics-informed Machine Learning

Let's begin with discussing physics-informed machine learning using
neural networks. The approach by Raissi et al.[@raissi2019physics]
employs deep neural networks to leverage their well known capability as
universal function approximators. This facilitatest the ability to
directly tackle nonlinear problems without the need for committing to
any prior assumptions, linearization, or local time-stepping. In
addition we can exploit recent developments in automatic differentiation
-- one of the most useful but perhaps under-utilized techniques in
scientific computing -- to differentiate neural networks with respect to
their input coordinates and model parameters to obtain physics-informed
neural networks. Such neural networks are constrained to respect any
symmetries, invariances, or conservation principles originating from the
physical laws that govern the observed data, as modeled by general
time-dependent and nonlinear partial differential equations. This simple
yet powerful construction allows us to tackle a wide range of problems
in computational science and introduces a potentially transformative
technology leading to the development of new data-efficient and
physics-informed learning machines, new classes of numerical solvers for
partial differential equations, as well as new data-driven approaches
for model inversion and systems identification. The approach sets the
foundations for a new paradigm in modeling and computation that enriches
deep learning with the longstanding developments in mathematical
physics.

### Problem Formulation

A parametrized and nonlinear partial differential equations of the
following general form are considered
$$u_t + \mathcal\{N\}[u;\lambda]=0, x \in \Omega, t\in [0,T],$$ where
$u(t, x)$ denotes the hidden solution, $\mathcal\{N[\cdot;\lambda]\}$ is a
nonlinear operator parametrized by $\lambda$, and $\Omega$ is a subset
of $\mathbb\{R\}^D$. This formulation applies to a wide range of problems
in physical systems including conservation laws, diffusion processes,
kinetic equations and physical systems.

### Data-driven solutions of partial differential equations

Let's begin with solutions to formulations of the general form
$$u_t + \mathcal\{N\}[u]=0, x \in \Omega, t\in [0,T],
    \label\{eq_generalform\}$$ where $u(t, x)$ denotes the hidden
solution, $\mathcal\{N[\cdot]\}$ is a nonlinear operator, and $\Omega$ is
a subset of $\mathbb\{R\}^D$.

The approach discussed below pertains to an algorithm that can be used
on **continuous time models**.

Let's define $f(t, x)$ to be given by the equation on the left hand
side,

$$f:=u_t + \mathcal\{N\}[u]
    \label\{eq_contTime\}$$

and the approach is to approximate $u(t,x)$ by a deep neural network.
This assumption along with equation
[\[eq_contTime\]](#eq_contTime)\{reference-type="ref"
reference="eq_contTime"\} gives rise to the physics-informed neural
network $f(t,x)$. The network can be derived by applying the chain rule
for differentiating compositions of functions using automatic
differentiation, and has the same parameters as the network representing
$u(t, x)$, albeit with different activation functions due to the action
of the differential operator $\mathcal\{N\}$. The shared parameters
between the neural networks $u(t, x)$ and $f(t, x)$ can be learned by
minimizing the mean-squared error loss

$$MSE=MSE_u + MSE_f$$

where $$MSE_u=\frac\{1\}\{N_u\} \sum_\{i=1\}^\{N_u\}|u(t_u^i,x_u^i)-u^i|^2$$

and $$MSE_f=\frac\{1\}\{N_f\} \sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2$$

In the problem formulation, the initial and boundary training data on
$u(t, x)$ are denoted by $\{\{t^i_u, x^i_u, u^i\}\}^\{N_u\}_\{i=1\}$, and
$\{\{t^i_f, x^i_f\}\}^\{N_f\}_\{i=1\}$ specify the collocation points for
$f(t, x)$. The loss function $MSE_u$ corresponds to the initial and
boundary data while $MSE_f$ enforces the structure imposed by the
general equation (equation
[\[eq_generalform\]](#eq_generalform)\{reference-type="ref"
reference="eq_generalform"\}) at a finite set of collocation points.

::: example
This example aims to highlight the ability of The method to handle
periodic boundary conditions, complex-valued solutions, as well as
different types of nonlinearities in the governing partial differential
equations. The nonlinear Schr$\mathrm\{\ddot\{o\}\}$dinger equation along
with periodic boundary conditions is given by

$$\begin\{aligned\}
\begin\{split\}
i h_t + 0.5 h_\{xx\} + |h|^2 h = 0,\ \ \ x \in [-5, 5],\ \ \ t \in [0, \pi/2],\\
h(0,x) = 2\ \mathrm\{sech\}(x),\\
h(t,-5) = h(t, 5),\\
h_x(t,-5) = h_x(t, 5),
\end\{split\}
\end\{aligned\}$$

where $h(t,x)$ is the complex-valued solution. Let us define $f(t,x)$ to
be given by

$$f := i h_t + 0.5 h_\{xx\} + |h|^2 h,$$

and proceed by placing a complex-valued neural network prior on
$h(t,x)$. In fact, if $u$ denotes the real part of $h$ and $v$ is the
imaginary part, we are placing a multi-out neural network prior on
$h(t,x) = \begin\{bmatrix\}
u(t,x) & v(t,x)
\end\{bmatrix\}$. This will result in the complex-valued (multi-output)
physics informed neural network $f(t,x)$. The shared parameters of the
neural networks $h(t,x)$ and $f(t,x)$ can be learned by minimizing the
mean squared error loss

$$MSE = MSE_0 + MSE_b + MSE_f$$

where

$$MSE_0 = \frac\{1\}\{N_0\}\sum_\{i=1\}^\{N_0\} |h(0,x_0^i) - h^i_0|^2$$

$$MSE_b = \frac\{1\}\{N_b\}\sum_\{i=1\}^\{N_b\} \left(|h^i(t^i_b,-5) - h^i(t^i_b,5)|^2 + |h^i_x(t^i_b,-5) - h^i_x(t^i_b,5)|^2\right)$$

and

$$MSE_f = \frac\{1\}\{N_f\}\sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2.$$

Here, $\{x_0^i, h^i_0\}_\{i=1\}^\{N_0\}$ denotes the initial data,
$\{t^i_b\}_\{i=1\}^\{N_b\}$ corresponds to the collocation points on the
boundary, and $\{t_f^i,x_f^i\}_\{i=1\}^\{N_f\}$ represents the collocation
points on $f(t,x)$. Consequently, $MSE_0$ corresponds to the loss on the
initial data, $MSE_b$ enforces the periodic boundary conditions, and
$MSE_f$ penalizes the Schr$\mathrm\{\ddot\{o\}\}$dinger equation not being
satisfied on the collocation points.

![Schrödinger equation: **Top:** Predicted solution along with the
initial and boundary training data. In addition we are using 20,000
collocation points generated using a Latin Hypercube Sampling strategy.
**Bottom:** Comparison of the predicted and exact solutions
corresponding to the three temporal snapshots depicted by the dashed
vertical lines in the top panel.](figures/NLS_bookl.png)
:::

One potential limitation of the continuous time neural network models
considered so far stems from the need to use a large number of
collocation points $N_f$ in order to enforce physics informed
constraints in the entire spatio-temporal domain. Although this poses no
significant issues for problems in one or two spatial dimensions, it may
introduce a severe bottleneck in higher dimensional problems, as the
total number of collocation points needed to globally enforce a physics
informed constrain (i.e., in this case a partial differential equation)
will increase exponentially. In the next section, we put forth a
different approach that circumvents the need for collocation points by
introducing a more structured neural network representation leveraging
the classical
[Runge-Kutta](https://en.wikipedia.org/wiki/Runge–Kutta_methods)
time-stepping schemes.

The approach discussed below pertains to a methodology that can be used
on **discrete-time models**. Let's first revist the classical
Runge-Kutta method (4 steps) before discussing the general case of $q$
steps.

##### The Classical Runge-Kutta Method

Let an initial value problem be specified as follows:

$\frac\{dy\}\{dt\}=f(t,y)$,$\quad y(t_\{0\})=y_\{0\}$

Now pick a step-size h \> 0 and define

$\begin\{aligned\}y_\{n+1\}&=y_\{n\}+\{\frac \{1\}\{6\}\}h\left(k_\{1\}+2k_\{2\}+2k_\{3\}+k_\{4\}\right),\\t_\{n+1\}&=t_\{n\}+h\\\end\{aligned\}$

$\begin\{aligned\}k_\{1\}&=\ f(t_\{n\},y_\{n\}),\\k_\{2\}&=\ f\left(t_\{n\}+\{\frac \{h\}\{2\}\},y_\{n\}+h\{\frac \{k_\{1\}\}\{2\}\}\right),\\k_\{3\}&=\ f\left(t_\{n\}+\{\frac \{h\}\{2\}\},y_\{n\}+h\{\frac \{k_\{2\}\}\{2\}\}\right),\\k_\{4\}&=\ f\left(t_\{n\}+h,y_\{n\}+hk_\{3\}\right).\end\{aligned\}$

##### The General Form of the Runge-Kutta Method \{#the-general-runge-kutta-method\}

The general form of Runge-Kutta methods with $q$ stages is applied to
the governing equation to arrive at the following:

$$\begin\{aligned\}
    \begin\{split\}
        u^\{n+c_i\}=u^n-\Delta t\sum_\{i=1\}^\{q\}a_\{ij\}\mathcal\{N\}[u^\{n+c_j\}], i=1,\cdot\cdot\cdot,q,\\
        u^\{n+1\}=u^n-\Delta t\sum_\{i=1\}^\{q\}b_\{j\}\mathcal\{N\}[u^\{n+c_j\}].
    \end\{split\}
\end\{aligned\}$$

In the formulation, $u^\{n+c_j\}(x)=u(t^n+c_j\Delta t,x)$ for
$j=1,\cdot\cdot\cdot,q$. Depending on the choice of the parameters
$\{a_\{ij\},b_j,c_j\}$.

The above equations can also be expressed as

$$\begin\{aligned\}
    \begin\{split\}
        u^n=u^n_i, i=1,\cdot\cdot\cdot,q,\\
        u^n=u^n_\{q+1\}
    \end\{split\}
\end\{aligned\}$$

where $$\begin\{aligned\}
    \begin\{split\}
        u_i^n:=u^\{n+c_i\}+\Delta t\sum_\{j=1\}^\{q\}a_\{ij\}\mathcal\{N\}[u^\{n+c_j\}], i=1,\cdot\cdot\cdot,q,\\
          u^\{n\}_\{q+1\}=u^\{n+1\}+\Delta t\sum_\{i=1\}^\{q\}b_\{j\}\mathcal\{N\}[u^\{n+c_j\}].
    \end\{split\}
\end\{aligned\}$$

We can place a multi-output neural network prior on
$$\label\{eq_discrete\}
    [u^\{n+c_1\}(x),\cdot\cdot\cdot,u^\{n+c_q\}(x),u^\{n+1\}(x)]$$ The
assumption along with equations
[\[eq_discrete\]](#eq_discrete)\{reference-type="ref"
reference="eq_discrete"\} result in a physics-informed neural network
that takes $x$ as an input and gives as the output
$$[u^\{n\}_1(x),\cdot\cdot\cdot,u^\{n\}_q(x),u_\{q+1\}^\{n+1\}(x)]$$

::: example
Burgers' Equation\
The Burgers' equation in one space dimension along with Dirichlet
boundary conditions reads as follow:

$$\begin\{array\}\{l\}
u_t + u u_x - (0.01/\pi) u_\{xx\} = 0,\ \ \ x \in [-1,1],\ \ \ t \in [0,1],\\
u(0,x) = -\sin(\pi x),\\
u(t,-1) = u(t,1) = 0.
\end\{array\}$$

Let us define $f(t,x)$ to be given by

$$f := u_t + u u_x - (0.01/\pi) u_\{xx\},$$

and proceed by approximating $u(t,x)$ by a deep neural network.

Correspondingly, the physics informed neural network $f(t,x)$ takes the
form

Loss function: $MSE = MSE_u + MSE_f,$

where

$MSE\_u = \frac\{1\}\{N_u\}\sum\emph\{\{i=1\}\^\{\}\{N\}u\}
\textbar\{\}u(t\textsuperscript\{i\_u,x\_u\}i) -
u\textsuperscript\{i\textbar\{\}\}2$ and
$MSE_f = \frac\{1\}\{N_f\}\sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2$

The following figure summarizes the results for the data-driven solution
of the Burgers' equation.

![*Top:* Predicted solution along with the initial and boundary training
data. In addition we are using 10,000 collocation points generated using
a Latin Hypercube Sampling strategy. *Bottom:* Comparison of the
predicted and exact solutions corresponding to the three temporal
snapshots depicted by the white vertical lines in the top panel. Model
training took approximately 60 seconds on a single NVIDIA Titan X GPU
card.](figures/Burgers_CT_inference_book.png)\{#fig:Burgers's\}
:::

::: example
Below we show an example for the Allen-Cahn equation that appears in
systems with phase separations, e.g., battery electrode-electrolyte
interfaces. The example is used to demonstrate the ability of the
proposed discrete time models to handle different types of nonlinearity
in the governing partial differential equation. Let's consider the
Allen--Cahn equation along with periodic boundary conditions

$$\begin\{aligned\}
     u_t-0.0001u_\{xx\}+5u^3-5u^3-5u &= 0, x\in[-1,1], t \in [0,1]\\
     u(0,x) &=x^2 cos(\pi x)\\
     u(t,-1) &=u(t,1)\\
     u_x(t,-1) &=u_x(t,1)\\
 
\end\{aligned\}$$

For the Allen-Cahn equation, the non-linear operator is given by

$$\mathcal\{N\}[u^\{n+c_j\}]=-0.0001u^\{n+c_j\}_\{xx\}+5(u^\{n+c_j\})^3-5u^\{n+c_j\}$$

and the shared parameters of the neural networks can be learnt by
minimizing the loss function as discussed earlier,

$$SSE=SSE_n+SSE_b$$

where
$$SSE_n=\sum_\{j=1\}^\{q+1\}\sum_\{i=1\}^\{N_n\}|u^n_j(x^\{n,i\})=u^\{n,i\}|^2$$

and $$\begin\{aligned\}
 \begin\{split\}
     SSE_b=\sum_\{i=1\}^q|u^\{n+c_i\}(-1)-u^\{n+c_i\}(1)|^2+|u^\{n+1\}(-1)-u^\{n+1\}(1)|^2\\
     +\sum_\{i=1\}^q|u_x^\{n+c_i\}(-1)-u_x^\{n+c_i\}(1)|^2+|u_x^\{n+1\}(-1)-u_x^\{n+1\}(1)|^2
 \end\{split\}
 
\end\{aligned\}$$

![Allen-Cahn Equation: Discrete Time
Domain](figures/allenCahn_PINN.png)\{#fig:allencahn\}
:::

## Data-Driven Discovery of Nonlinear Partial Differential Equations

We shift our attention to the problem of data-driven discovery of
partial differential equations. To this end, let us consider
parametrized and nonlinear partial differential equations of the general
form $$u_t + \mathcal\{N\}[u;\lambda] = 0,\ x \in \Omega, \ t\in[0,T],$$

where $u(t,x)$ denotes the latent (hidden) solution,
$\mathcal\{N\}[\cdot;\lambda]$ is a nonlinear operator parametrized by
$\lambda$, and $\Omega$ is a subset of $\mathbb\{R\}^D$. Now, the problem
of data-driven discovery of partial differential equations poses the
following question: given a small set of scattered and potentially noisy
observations of the hidden state $u(t,x)$ of a system, what are the
parameters $\lambda$ that best describe the observed data?

In what follows, we will provide an overview of the two main approaches
to tackle this problem, namely continuous time and discrete time models,
as well as a series of results and systematic studies for a diverse
collection of benchmarks. In the first approach, we will assume
availability of scattered and potential noisy measurements across the
entire spatio-temporal domain. In the latter, we will try to infer the
unknown parameters $\lambda$ from only two data snapshots taken at
distinct time instants

### Continuous Time Models

We define $f(t,x)$ to be given by

$$f := u_t + \mathcal\{N\}[u;\lambda],\label\{eq:PDE_RHS\}$$

and proceed by approximating $u(t,x)$ by a deep neural network. This
assumption results in a [physics informed neural
network](https://arxiv.org/abs/1711.10566) $f(t,x)$. This network can be
derived by the calculus on computational graphs:
[Backpropagation](http://colah.github.io/posts/2015-08-Backprop/). It is
worth highlighting that the parameters of the differential operator
$\lambda$ turn into parameters of the physics informed neural network
$f(t,x)$.

::: example
Navier-Stokes Equation

The next example involves a realistic scenario of incompressible fluid
flow as described by the Navier-Stokes equations. Navier-Stokes
equations describe the physics of many phenomena of scientific and
engineering interest. They may be used to model the weather, ocean
currents, water flow in a pipe and air flow around a wing. The
Navier-Stokes equations in their full and simplified forms help with the
design of aircraft and cars, the study of blood flow, the design of
power stations, the analysis of the dispersion of pollutants, and many
other applications. Let us consider the Navier-Stokes equations in two
dimensions (2D) given explicitly by

$$\begin\{aligned\}
\begin\{split\}
u_t + \lambda_1 (u u_x + v u_y) = -p_x + \lambda_2(u_\{xx\} + u_\{yy\}),\\
v_t + \lambda_1 (u v_x + v v_y) = -p_y + \lambda_2(v_\{xx\} + v_\{yy\}),    
\end\{split\}
\end\{aligned\}$$

where $u(t, x, y)$ denotes the $x$-component of the velocity field,
$v(t, x, y)$ the $y$-component, and $p(t, x, y)$ the pressure. Here,
$\lambda = (\lambda_1, \lambda_2)$ are the unknown parameters. Solutions
to the Navier-Stokes equations are searched in the set of
divergence-free functions; i.e.,

$$u_x + v_y = 0.$$

This extra equation is the continuity equation for incompressible fluids
that describes the conservation of mass of the fluid. We make the
assumption that

$$u = \psi_y,\ \ \ v = -\psi_x,$$

for some latent function $\psi(t,x,y)$. Under this assumption, the
continuity equation will be automatically satisfied. Given noisy
measurements

$$\{t^i, x^i, y^i, u^i, v^i\}_\{i=1\}^\{N\}$$

of the velocity field, we are interested in learning the parameters
$\lambda$ as well as the pressure $p(t,x,y)$. We define $f(t,x,y)$ and
$g(t,x,y)$ to be given by

$$\begin\{aligned\}
f &:= u_t + \lambda_1 (u u_x + v u_y) + p_x - \lambda_2(u_\{xx\} + u_\{yy\})\\
g &:= v_t + \lambda_1 (u v_x + v v_y) + p_y - \lambda_2(v_\{xx\} + v_\{yy\})        
\end\{aligned\}$$

and proceed by jointly approximating $$\begin\{bmatrix\}
\psi(t,x,y) & p(t,x,y)
\end\{bmatrix\}$$ using a single neural network with two outputs. This
prior assumption results into a physics informed neural network
$$\begin\{bmatrix\}
f(t,x,y) & g(t,x,y)
\end\{bmatrix\}$$ The parameters $\lambda$ of the Navier-Stokes operator
as well as the parameters of the neural networks $\begin\{bmatrix\}
\psi(t,x,y) & p(t,x,y)
\end\{bmatrix\}$ and $\begin\{bmatrix\}
f(t,x,y) & g(t,x,y)
\end\{bmatrix\}$ can be trained by minimizing the mean squared error loss

$$\begin\{aligned\}
    \begin\{split\}
    MSE :=& \frac\{1\}\{N\}\sum_\{i=1\}^\{N\} \left(|u(t^i,x^i,y^i) - u^i|^2 + |v(t^i,x^i,y^i) - v^i|^2\right) \\
    +& \frac\{1\}\{N\}\sum_\{i=1\}^\{N\} \left(|f(t^i,x^i,y^i)|^2 + |g(t^i,x^i,y^i)|^2\right)
    \end\{split\}
\end\{aligned\}$$

![Navier-Stokes equation: **Top:** Incompressible flow and dynamic
vortex shedding past a circular cylinder at Re$=100$. The
spatio-temporal training data correspond to the depicted rectangular
region in the cylinder wake. **Bottom:** Locations of training
data-points for the the stream-wise and transverse velocity components.
](figures/NavierStokes_data_book.png)

![Navier-Stokes equation: **Top:** Predicted versus exact instantaneous
pressure field at a representative time instant. By definition, the
pressure can be recovered up to a constant, hence justifying the
different magnitude between the two plots. This remarkable qualitative
agreement highlights the ability of physics-informed neural networks to
identify the entire pressure field, despite the fact that no data on the
pressure are used during model training. **Bottom:** Correct partial
differential equation along with the identified
one.](figures/NavierStokes_prediction_book.png)
:::

The approach so far assumes availability of scattered data throughout
the entire spatio-temporal domain. However, in many cases of practical
interest, one may only be able to observe the system at distinct time
instants. In the next section, we introduce a different approach that
tackles the data-driven discovery problem using only two data snapshots.
We will see how, by leveraging the classical Runge-Kutta time-stepping
schemes, one can construct discrete time physics informed neural
networks that can retain high predictive accuracy even when the temporal
gap between the data snapshots is very large.

### Discrete Time Models

We begin by employing the general form of Runge-Kutta methods with $q$
stages and obtain

$$\begin\{aligned\}
u^\{n+c_i\} &= u^n - \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q\\
u^\{n+1\} &= u^\{n\} - \Delta t \sum_\{j=1\}^q b_j \mathcal\{N\}[u^\{n+c_j\};\lambda].        
\end\{aligned\}$$

Here, $u^\{n+c_j\}(x) = u(t^n + c_j \Delta t, x)$ for $j=1, \ldots, q$.
This general form encapsulates both implicit and explicit time-stepping
schemes, depending on the choice of the parameters $\{a_\{ij\},b_j,c_j\}$.
The above equations can be equivalently expressed as

$$\begin\{aligned\}
u^\{n\} &= u^n_i, \ \ i=1,\ldots,q\\
u^\{n+1\} &= u^\{n+1\}_\{i\}, \ \ i=1,\ldots,q
\end\{aligned\}$$

where

$$\begin\{aligned\}
u^n_i &:= u^\{n+c_i\} + \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q,\\
u^\{n+1\}_\{i\} &:= u^\{n+c_i\} + \Delta t \sum_\{j=1\}^q (a_\{ij\} - b_j) \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q.
\end\{aligned\}$$

We proceed by placing a multi-output neural network prior on

$$u^\{n+c_1\}(x), \ldots, u^\{n+c_q\}(x)$$

This prior assumption result in two physics informed neural networks

$$u^\{n\}_1(x), \ldots, u^\{n\}_q(x), u^\{n\}_\{q+1\}(x)$$

and

$$u^\{n+1\}_1(x), \ldots, u^\{n+1\}_q(x), u^\{n+1\}_\{q+1\}(x)$$

Given noisy measurements at two distinct temporal snapshots
$\{\mathbf\{x\}^\{n\}, \mathbf\{u\}^\{n\}\}$ and
$\{\mathbf\{x\}^\{n+1\}, \mathbf\{u\}^\{n+1\}\}$ of the system at times $t^\{n\}$
and $t^\{n+1\}$, respectively, the shared parameters of the neural
networks along with the parameters $\lambda$ of the differential
operator can be trained by minimizing the sum of squared errors

$$SSE = SSE_n + SSE_\{n+1\},$$

where

$$SSE_n := \sum_\{j=1\}^q \sum_\{i=1\}^\{N_n\} |u^n_j(x^\{n,i\}) - u^\{n,i\}|^2,$$

and

$$SSE_\{n+1\} := \sum_\{j=1\}^q \sum_\{i=1\}^\{N_\{n+1\}\} |u^\{n+1\}_j(x^\{n+1,i\}) - u^\{n+1,i\}|^2.$$

Here, $\mathbf\{x\}^n = \left\{x^\{n,i\}\right\}_\{i=1\}^\{N_n\}$,
$\mathbf\{u\}^n = \left\{u^\{n,i\}\right\}_\{i=1\}^\{N_n\}$,
$\mathbf\{x\}^\{n+1\} = \left\{x^\{n+1,i\}\right\}_\{i=1\}^\{N_\{n+1\}\}$, and
$\mathbf\{u\}^\{n+1\} = \left\{u^\{n+1,i\}\right\}_\{i=1\}^\{N_\{n+1\}\}$.

::: example
Korteweg--de Vries Equation

The final example aims to highlight the ability of the proposed
framework to handle governing partial differential equations involving
higher order derivatives. Here, we consider a mathematical model of
waves on shallow water surfaces, the Korteweg-de Vries (KdV) equation.
The KdV equation reads as

$$u_t + \lambda_1 u u_x + \lambda_2 u_\{xxx\} = 0,$$

with $(\lambda_1, \lambda_2)$ being the unknown parameters. For the KdV
equation, the nonlinear operator is given by

$$\mathcal\{N\}[u^\{n+c_j\}] = \lambda_1 u^\{n+c_j\} u^\{n+c_j\}_x - \lambda_2 u^\{n+c_j\}_\{xxx\}$$

and the shared parameters of the neural networks along with the
parameters $\lambda = (\lambda_1, \lambda_2)$ of the KdV equation can be
learned by minimizing the sum of squared errors given above.

![KdV equation: **Top:** Solution along with the temporal locations of
the two training snapshots. Middle: Training data and exact solution
corresponding to the two temporal snapshots depicted by the dashed
vertical lines in the top panel. **Bottom:** Correct partial
differential equation along with the identified
one.](figures/KdV_book.png)\{#fig:my_label\}
:::

As we have shown in this chapter Physics-informed neural networks are a
versatile class of universal function approximators that are capable of
encoding any underlying physical laws that govern a given dataset.
Physics-informed neural networks enable data-driven algorithms for
inferring solutions to general nonlinear partial differential equations.
In addition, it enables the construction of computationally efficient
physics-informed surrogate models.

## PINNs Implementation and Physika Code

For the example case of obtaining the solution of the Burgers equation,
which is a prototype example of a hyperbolic conservation law, the
physika code is discussed below. The simplicity of the implementing the
idea can be seen To recapitulate, the the Burger's equation along with
Dirichlet boundary conditions reads as follows:

$$\begin\{array\}\{l\}
u_t + u u_x - (0.01/\pi) u_\{xx\} = 0,\ \ \ x \in [-1,1],\ \ \ t \in [0,1],\\
u(0,x) = -\sin(\pi x),\\
u(t,-1) = u(t,1) = 0.
\end\{array\}$$

We define $f(t,x)$ to be given by
$$f := u_t + u u_x - (0.01/\pi) u_\{xx\},$$

$u(t,x)$ is therefore approximated by a deep neural network.

    def u(t,x):
        u=neural_net(concat([t,x],1),weights,biases)
        return u

The physics-informed neural network $f(t, x)$ would be:

    def f(t,x):
        u=u(t,x)
        u_t=gradients(u,t)[0]
        u_x=gradients(u,x)[0]
        u_xx=gradients(u_x,x)[0]
        f=u_t+(u)(u_x)−(0.01/pi)u_xx
        return f

The loss function is defined in the following way. A precise description
of the loss function is discussed earlier in the chapter for the same
application example.

    def loss:
        MSE_u=(1/N_u) [(u0 - u0_pred) + (v0 - v0_pred) + (u_lb_pred - u_ub_pred)^2 +
                        (v_lb_pred - v_ub_pred)^2 + 
                        (u_x_lb_pred - u_x_ub_pred)^2 + 
                        (v_x_lb_pred - v_x_ub_pred)^2 + 
                        (f_u_pred)^2 + 
                        (f_v_pred)^2)]

The solution process at a high level can be described as follows:

    model = PhysicsInformedNN(collocation points, initial conditions, boundary conditions, layers, lower and upper bounds)
    model.train(10)
    u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)
