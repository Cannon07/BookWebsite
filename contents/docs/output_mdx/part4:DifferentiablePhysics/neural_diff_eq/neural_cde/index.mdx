# Neural Controlled Differential Equations \{#chap:neural_cde\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:pde\]](#chap:pde)\{reference-type="ref+label"
reference="chap:pde"\},
[\[chap:numerical_pde\]](#chap:numerical_pde)\{reference-type="ref+label"
reference="chap:numerical_pde"\},
[\[chap:neural_ode\]](#chap:neural_ode)\{reference-type="ref+label"
reference="chap:neural_ode"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Neural controlled differential equations (Neural CDEs) provide a
powerful generalization of recurrent neural networks to continuous time
stochastic and controlled differential equation solution. Neural CDEs
can serve to extend RNNs to continuous time, or to provide a framework
to solve SDEs and control problems with a powerful neural ansatz.

## Mathematical Framework

A neural controlled differential equation is defined by the following
equation $$\begin\{aligned\}
    y(t) &= y(0) + \int_0^t f_\theta(y(s))\ dX(s)
\end\{aligned\}$$ where $f_\theta(y(s)) dX(s)$ is a matrix vector product.
Here $f_\theta$ is the neural network ansatz for the problem.

Let $X: [0, 1] \to \mathbb\{R\}^v$ be some learned data. Let $\xi_\theta$,
$f_\theta$, $\ell_\theta$ such that $$\begin\{aligned\}
    y(0) &= \xi(X(0)) \\
    y(t) &= y(0) + \int_0^t f_\theta(y(s))dX(s)
\end\{aligned\}$$ Here $y$ is the hidden state as in the case of a
recurrent neural network. We define the evolution rule.
$$\begin\{aligned\}
    y_\{n+1\} &= f_\theta(y_n, x_n)
\end\{aligned\}$$ The output is $$\begin\{aligned\}
    \ell_\theta(y(t))
\end\{aligned\}$$ The neural differential equation is used to evolve in
latent space. This provides us a way to model a a process that evolves
in a non-Markovian fashion. We observe the time series $$\begin\{aligned\}
    (t_1, x_1),\dotsc,(t_n, x_n)
\end\{aligned\}$$ The neural controlled differential equation is analogous
to a continuous time recurrent neural network. Think of a recurrent
neural network as a function of a time series. The output might be
something entirely different like a time series. We can think of a
neural CDE as an operator $$\begin\{aligned\}
    X \mapsto y
\end\{aligned\}$$ That is, the neural CDE learns functions on paths.

### Control Theory

What is the comparison or link between CDEs and control theory? In
control theory, we have a fixed system (with some possibility of system
identification) and we want to find an optimal control. In a CDE, we
have an orthogonal setup where the input is fixed but the goal is to
find an optimal system.

The form $$\begin\{aligned\}
f_\theta(y(s)) dX(s)
\end\{aligned\}$$ is connected to the theory of rough path theory. Neural
CDEs can also be viewed as universal approximators implemented on the
space of paths.

## Stochastic Differential Equations

A stochastic differential equation provides a method for modeling a
differential equation with noise present.

$$\begin\{aligned\}
    dX_t &= \mu(t, X_t) dt + \sigma(t, X_t) dW_t
\end\{aligned\}$$ $X$ and $W$ are vector valued and $\sigma$ is matrix
valued. The strong solution to an SDE is a map $$\begin\{aligned\}
 (X_0, W) \mapsto X
 
\end\{aligned\}$$ We get an output distribution over paths. We can sample
from these output densitites with SDE solvers but it's hard to write
down a notion of probability density over path space.

SDEs are trained against empirical data by matching statistics
$$\begin\{aligned\}
    E_X F_i(X) \approx E_\{\textrm\{data\}\} F_i(\textrm\{data\})
\end\{aligned\}$$ for all $i \in 1,\dotsc, n$. These $F_i$ are called the
payoff functions.

Recall that a GAN can be formulated as noise
$A(\omega) \in \mathbb\{R\}^\{d_1\}$, target
$B(\omega) \in \mathbb\{R\}^\{d_2\}$. A generative model is a map
$$\begin\{aligned\}
     g_\theta: \mathbb\{R\}^\{d_1\} \to \mathbb\{R\}^\{d_2\} 
\end\{aligned\}$$ such that $$\begin\{aligned\}
    g_\theta(A) \approx B
\end\{aligned\}$$ We train $\theta$ to minimize $$\begin\{aligned\}
    W(g_\theta(A), B) \approx \left | \frac\{1\}\{N\} \sum_\{i=1\}^N g_\theta(A(w)) - \frac\{1\}\{M\} \sum_\{j=1\}^M F_\phi(B(\omega_j)) \right|
\end\{aligned\}$$ Note that this matches the payoff function matching. It
turns out that classical methods for training SDEs roughly matches
techniques for training GANs.

Now we move to neural stochastic differential equation. A neural SDE is
a distribution over path space. We want to model target distribution
$B$, have Wiener noise $W$ and initial noise $V$ $$\begin\{aligned\}
    X_0 &= \xi_\theta(V) \\
    dX_t &= \mu_\theta(t, X_t) dt + \sigma_\theta(t, X_t) dW_t \\
    Y_t &= \ell_\theta(X_t)
\end\{aligned\}$$ where $Y \approx B$. Here $Y$ is the output and $X$ is
the hidden state. We need to work in a latent space to handle
non-Markovian process.

Note that this approach takes a slightly different tack from the usual
adversarial presentation of GANs. We train a neural SE as follow. The
outputs of the model are continuous time paths $Y$. We need to create a
discriminator $Y_\phi$ that accepts continuous time paths. The natural
choice for such a discriminator is a neural controlled differential
equations. These path models now separate real time series from fake
time series sampled by the neural SDE. $$\begin\{aligned\}
    H_0 &= \xi_\phi(Y_0) \\
    H_t &= f_\phi(t, H_t) dt + g_\phi(t, H_t) dY_t
\end\{aligned\}$$ There is a natural connection between neural SDEs and
neural SDEs.

![Architecture of a neural differential
equation](figures/Differentiable Physics/Neural Diff Eq/Neural CDE/Neural Differential equation (2).png)\{#fig:my_label\}

Neural SDEs have a connection to latent ODEs as well. The choice of
$f_\phi$, $g_\phi$ are often fully connected networks.

## Solving ODEs and SDEs with CDEs

We can fit a neural SDE to real world time series as well.

It is possible to solve ODEs and SDEs by reducing to CDEs and solving
these classes of equations in a unified way $$\begin\{aligned\}
    dy(t) &= f(t, y(t))dt \\
    dy(t) &= f_1(t, y(t))dt + f_2(t, y(t)) dW_t 
\end\{aligned\}$$ are special cases of $$\begin\{aligned\}
dy(t) &= f(t, y(t))dx(t)
\end\{aligned\}$$ Consider the Euler update for ODEs $$\begin\{aligned\}
y_\{n+1\} &= y_n + dy(t)
\end\{aligned\}$$ We can write down an Euler update for neural CDEs and
apply to both neural ODEs and neural SDEs.
