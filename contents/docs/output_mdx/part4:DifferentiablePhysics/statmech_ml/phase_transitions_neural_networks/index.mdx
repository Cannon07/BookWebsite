# Phase Transitions of Learning in Neural Networks \{#chap:phase_neural\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

When studying the theory of neural networks, it is often useful to begin
with the asymptotic limit of infinite hidden units. As the number of
hidden neurons approaches $\infty$, a neural network approaches a
Gaussian process. The resulting system can be studied with the
methodology of kernel machines, leading to the neural tangent kernel.
However, the connection between asymptotic limit and the finite realm
breaks down as the learning rate increases. In particular, empirical
studies offer evidence that large early learning rates during training
can improve performance. There is some evidence that a phase transition
occurs between different regions of the learning process. In this
chapter we study the behavior of this phase transition
[@bahri2020statistical], [@lee2017deep], [@lewkowycz2020large]. In
particular, we learn about the theorized lazy, catapult, and divergent
phases.

## Phase Transitions and Coupling Method

There can be sharp differences in model training behavior based on the
choice of learning rate. There is a condition on when behavior modeled
by the neural tangent kernel (NTK) sets in, whereby you need the
learning rate to be below some critical threshold. Empirically there is
a phase transition in the behavior of gradient descent as a function of
learning rate where width $$\begin\{aligned\}
n \to \infty
\end\{aligned\}$$ This behavior is matched by evidence on the numerical
side.

The three phases are the Neural Tangent Kernel phase, the catapult
phase, and the divergent phase.

### The Lazy or Neural Tangent Kernel Phase

$\lambda_0$ is the top eigenvalue of the NTK kernel. On the width
$n \to \infty$, we effectively approach a convex learning problem with a
quadratic loss. There the NTK is the Hessian, and there is a maximum
learning rate that naturally arises.

$$\begin\{aligned\}
\Theta_\{\mathcal\{D\}_x, \mathcal\{D\}_\{x'\}\} \to \lambda_0
\end\{aligned\}$$

When we have the condition that learning rate $\eta$ satisfies

$$\begin\{aligned\}
    \eta < \frac\{2\}\{\lambda_0\},
\end\{aligned\}$$ we say that we are in the lazy phase.

### The Catapult Phase

We define the catapult phase of learning as the regimen in which
$$\begin\{aligned\}
    \frac\{2\}\{\lambda_0\} < \eta < \eta_\{\mathrm\{max\}\}
\end\{aligned\}$$

Here $\eta_\{\mathrm\{max\}\}$ is the maximum learning rate supported by the
model. One empirical observation, is that in the catapult phase the loss
can diverge at first before it comes down stably.

### The Divergent Phase

The divergent phase is characterized by $$\begin\{aligned\}
    \eta_\{\mathrm\{max\}\} < \eta
\end\{aligned\}$$

In this phase, the model fails to learn entirely, unlike the catapult
phase in which the model training eventually stabilizes.

## A Simplified Mathematical Model

Suppose that we have a neural network $$\begin\{aligned\}
    f: \mathbb\{R\}^d \mapsto \mathbb\{R\}
\end\{aligned\}$$ Suppose that we have a training dataset
$$\begin\{aligned\}
\{(x_i, y_i)\}_\{i=1\}^M
\end\{aligned\}$$ Assume that we have a $p$ dimensional set of parameters
$$\begin\{aligned\}
    \theta \in \mathbb\{R\}^p
\end\{aligned\}$$ The mean-squared-error loss is given by
$$\begin\{aligned\}
    \mathcal\{L\} &= \frac\{1\}\{2m\} \sum_\{i=1\}^m \| f(x_\{i\}) -y_i \|^2
\end\{aligned\}$$ The neural tangent kernel is defined by
$$\begin\{aligned\}
    \Theta(x, x') &= \frac\{1\}\{M\} \sum_\{\mu=1\}^p \frac\{\partial f(x)\}\{\partial \theta_\mu\} \frac\{\partial f(x')\}\{\partial \theta_\mu\} 
\end\{aligned\}$$ $\Theta$ is consequently a matrix of shape
$\mathbb\{R\}^\{d \times d\}$ where $$\begin\{aligned\}
    x_i \in \mathbb\{R\}^d
\end\{aligned\}$$ Let $\lambda$ denote the maximum eigenvalue of this
kernel. Here $\eta$ is the learning rate.

### A One Dimensional Linear Model

The simplest case we can analyze is when $x$ is one dimensional

$$\begin\{aligned\}
f(x) &= \frac\{1\}\{\sqrt\{n\}\}v^T u x
\end\{aligned\}$$ Suppose we one training example $$\begin\{aligned\}
    (x, y) &= (1, 0)
\end\{aligned\}$$ In this case, the loss is simply $$\begin\{aligned\}
    \mathcal\{L\} &= \frac\{f^2\}\{2\} \\
    f &= \frac\{v^tu\}\{\sqrt\{n\}\}
\end\{aligned\}$$ We can turn gradient descent updates for the parameters
into gradient descent updates for the functions. $$\begin\{aligned\}
    f_\{t+1\} &= (1 - \eta \lambda_t + \frac\{\eta^2 f_t^2\}\{n\} ) f_t \\
    \lambda_\{t+1\} &= \lambda_t + \frac\{\eta \lambda_t^2\}\{n\}(\eta \lambda_t -4) 
\end\{aligned\}$$ The catapult phase occurs at $$\begin\{aligned\}
    2/\lambda_0 < \eta < 4/\lambda_0
\end\{aligned\}$$

### Full Model

We can then derive full updates on the parameters $u$ and $v$ and on the
NTK $\Theta$. These parameter space updates can be converted into
function space updates. We define the notation $$\begin\{aligned\}
    f_i &= f(x_i) \\
    \tilde\{f\}_i &= f(x_i) - y_i \\
    \Theta_\{ij\} &= \Theta(x_i, x_j)
\end\{aligned\}$$ With this notation, we can write $$\begin\{aligned\}
    \tilde\{f\}_\{i\}^\{t+1\} &= \sum_j (\delta_\{i j\} - \eta \Theta_\{i j\}) \tilde\{f\}_j + \frac\{\eta^2\}\{NM\} (x^T_i \zeta)(f^T\tilde\{f\})
\end\{aligned\}$$ Here we define $$\begin\{aligned\}
    \zeta &= \frac\{1\}\{M\} \sum_i \tilde\{f\}_i x_i
\end\{aligned\}$$

We can look at a projected function update which looks more
understandable $$\begin\{aligned\}
    \tilde\{f\}^T \Theta \tilde\{f\} &= \tilde\{f\}^T \Theta \tilde\{f\} + \frac\{\eta\}\{N\} \zeta^T \zeta (\eta \tilde\{f\}^T \Theta \tilde\{f\} - 4 f^T \tilde\{f\})
\end\{aligned\}$$

However we lose something by looking at this projected version since
some details are lost. There's a series expansion $$\begin\{aligned\}
    f_t = f_t^0 + \frac\{1\}\{n\} f_t^1 + \dotsc 
\end\{aligned\}$$ There is a breakdown in this series expansion where all
terms become of the same size and then the finite-width perturbation
series expansion breaks down. When the curvature scale drops though,
this expansion works and the perturbation theory works again.

The model can provide predictions on when we enter the catapult phase.
The wider the network, the greater the \"control.\" We can consequently
get a robust empirical explanation for the catapult phase.
