# Foundations of Low Data Learning \{#chap:low_data\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:eft\]](#chap:eft)\{reference-type="ref+label"
reference="chap:eft"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

\"\"\" The relevant limit for machine learning is not N $\to$ infinity
but instead N $\to$ 0. The human visual system is proof that it is
possible to learn categories with extremely few samples. This talk will
discuss steps towards building such systems. It is structured in three
parts. The first part will discuss algorithms to adapt representations
of deep networks to new categories with few labeled data. The second
part will exploit a formal connection between thermodynamics and machine
learning to characterize such adaptation. We will see how this theory
leads to algorithms for transfer learning that can guarantee high
accuracy on the target task. It stands to reason that adaptation cannot
always work, it should be difficult for tasks that are \"far away\". The
third part will characterize when learning multiple tasks is difficult,
and demonstrate ways to work around such difficulties. \"\"\"

References [@dhillon2019baseline], [@gao2020free],
[@gao2021information], [@ramesh2021boosting]
