# An Introduction to Information Theory \{#chap:calculus\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:math_basics\]](#chap:math_basics)\{reference-type="ref+label"
reference="chap:math_basics"\}\
**Difficulty Level:** \*\

------------------------------------------------------------------------

The entropy of a information set $Y$, otherwise known as the Shannon
entropy, is defined below. Note that while this entropy is defined
similarly to the entropy used in Thermodynamics, it is not the same
term. $$\begin\{aligned\}
H(Y) &= -\sum_\{i\} p_i \log_2 (p_i)
\end\{aligned\}$$ In this equation, $p_i$ is the probability of $i$-th
possible occurrence within the data set. Imagine a data set containing
results for whether a flipped coin resulted in either heads or tails.
Suppose the number of trials is $50$ and exactly $25$ of the flips were
tails and $25$ of the flips were heads. In this case, the two different
occurrences are heads and tails and each have a probability of occurring
of $\frac\{1\}\{2\}$. Using the equation above, we would calculate that the
entropy of the data set would be 1. This means that this data set has
maximum entropy. In other words, this data set has no order to it is
equally likely for one to get any of the occurrences.

If, on the other hand, we somehow had the flip of the coin result in all
heads or all tails, then the probability of one of the occurrences would
be 0 while the other one would be 1. When evaluating the entropy of this
case, we would find that one of the terms would be $$\begin\{aligned\}
0 * -\infty
\end\{aligned\}$$ In this case, the $0$ term dominates which means that
the entropy of this case is $0$. This means, as clearly seen, the data
set is perfectly determined and we know whether or not we will get heads
or tails. This means that the entropy term we defined will always be
between 0 and 1 where 1 is the most disordered and 0 is the most ordered
as far as data set information is concerned.

Specific conditional entropy is similarly defined and is represented as
follows where Y is again our data set and X is one possible feature from
which we can separate our data which has some specific occurrence x'.
$$\begin\{aligned\}
H(Y|X=x') &= -\sum_\{i\} p_i \log_2 (p_i)\\
\end\{aligned\}$$ In this case, $p_i$ is again the probability of $i$-th
possible occurrence within the data set however now our data set is
further conditioned based on the specific value of $X$, $x'$. Using our
previous example of flipping coins, imagine now that of those 50 trials,
20 of them were done with gloves on and the others were done without any
gloves on at all. Of the 20 trials done with gloves on, 19 of them
resulted in heads while only 1 resulted in tails whereas where there
were no gloves on, 24 of them resulted in tails while only 6 of them
resulted in heads. We can therefore evaluate the specific conditional
entropy for when gloves were on and when gloves were off as done below.
$$\begin\{aligned\}
H(Y|X=\textrm\{gloves\}) &= -(\frac\{19\}\{20\}\log_2(\frac\{19\}\{20\})+\frac\{1\}\{20\}\log_2(\frac\{1\}\{20\}))=0.286\\
H(Y|X=\textrm\{no\ gloves\}) &= -(\frac\{24\}\{30\}\log_2(\frac\{24\}\{30\})+\frac\{6\}\{30\}\log_2(\frac\{6\}\{30\}))=0.722
\end\{aligned\}$$ We can then define the general conditional entropy as
shown below. $$\begin\{aligned\}
H(Y|X) &= \sum_\{x_i\} p_\{x_i\} H(Y|X=x_i)
\end\{aligned\}$$ In our example, the conditional entropy of the data set
on whether or not the person was wearing gloves is calculated below.
$$\begin\{aligned\}
H(Y|X) = \frac\{20\}\{50\} * 0.286 + \frac\{30\}\{50\} * 0.722=0.548
\end\{aligned\}$$ Once the conditional entropy is determined, we can then
determine how much information was gained as a result of the $X$ feature
being used to classify the data. This is formally defined as follows
where I represents the information gain or mutual information.
$$\begin\{aligned\}
I(Y;X) &= H(Y) - H(Y|X)
\end\{aligned\}$$ In general, you want to maximize the information gain
when choosing the attributes X that will further classify your data set
Y. Note that for our example, $I(Y;X) = 0.452$. This term is essentially
a way to quantify how much more we know about Y when given X and will
always be greater than or equal to 0. Below are listed several
identities that will be used throughout the derivation of the rest of
the chapter. $$\begin\{aligned\}
    I(A;B) &= H(A)-H(A|B)\\
    I(A;B) &= H(B)-H(B|A)\\
    I(A;B) &= H(A)+H(B)-H(A,B)\\
    I(A;B) &= H(A,B)-H(A|B)-H(B|A)\\
    I(A,B;C) &= I(A;C)+I(B;C|A)\geq 0\\
    H(B|A) &= H(A,B) - H(A)
\end\{aligned\}$$ In the above equations, terms such as $H(A,B)$ denote
joint terms.
