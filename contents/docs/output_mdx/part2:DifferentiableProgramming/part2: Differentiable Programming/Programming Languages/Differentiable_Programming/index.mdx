# Differentiable Programming \{#chap:diffprog\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\},
[\[chap:autodiff\]](#chap:autodiff)\{reference-type="ref+label"
reference="chap:autodiff"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter, we will seek to extend the basic material on machine
learning with material more specific to \"differentiable programming.\"
You might very reasonably ask here, what's the difference between
differentiable programming and more standard machine learning? There
really isn't a clear dividing line, but perhaps differentiable programs
are more akin to classical programs with control structures such as
loops, conditionals, and branching.

You might also ask reasonably, which programs can be made
differentiable? This is an open question for now, but as you will see in
this chapter, a very broad range of programs can fruitfully be made
differentiable. We will up front state an intriguing conjecture

::: conjecture
Any algorithm can be transformed into a differentiable program.
:::

To be up front, we don't entirely believe in this conjecture ourselves,
but it serves as a powerful goal to either prove or disprove. If true,
it implies that all the machinery of classical computer science can be
adapted for differentiable programs. If false, it hints at potential
future generalizations of differentiable programming.

## Adapting Classical Control Structures

In this section, we will consider how to adapt classical control
structures into differentiable programs explicitly.

### Condition Statements

Here's a simple example of a conditional statement in Physika

    def h(x):
      if condition:
        f(x)
      else:
        g(x)

We can convert this into a differentiable program by the following
branch statement

    h(x) = $\sigma$ f(x) + $(1 - \sigma)$ g(x) 

The basic idea here is that we allow for a linear combination of $f$ and
$g$ controlled by a differentiable parameter $\sigma$ that can be
learned.

You might at this point reasonably ask, wait, we said Physika supports
differentiation through the source code. Why would we use this
alternative style instead? The answer comes down to optimization. At
times, this alternative structure may perform more smoothly. As with
many things in machine learning (and differentiable programming), at
times the best answer is to just try both and see.

### Loops

Here's a generic loop structure

    def h(a : int[N]):
      for i in range(N):
        z = f(z, a[i], i)
      return z

Note that this loop structure can be modeled by an RNN

TODO: Write down RNN equations here or elsewhere.

### Top K

### SAT Solving
