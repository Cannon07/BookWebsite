# Common Statistical Ensembles \{#chap:statistical_ensemble\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:stat_thermo\]](#chap:stat_thermo)\{reference-type="ref+label"
reference="chap:stat_thermo"\},
[\[chap:noninteracting\]](#chap:noninteracting)\{reference-type="ref+label"
reference="chap:noninteracting"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In the previous chapter, we briefly introduced the notion of a
statistical ensemble. In this chapter, we introduce common ensembles
that are widely studied in statistical mechanics.

Mathematically, an ensemble is a probability density function over phase
space. We can further separate ensembles into closed and open ensembles
depending on whether the number of particles in the system is allowed to
vary.

    ClosedEnsemble = ProbabilityDistribution[ClassicalPhaseSpace]
    OpenEnsemble = ProbabilityDistribution[OpenClassicalPhaseSpace]
    Ensemble = OpenEnsemble | ClosedEnsemble

Different ensembles are defined in terms of different fundamental
physical quantities.

## Defining Relationships

The governing thermodynamic potential of the closed, isolated system is
the entropy $S(U,V,N)$, *i.e.* the control or canonical variables are
$U$, $V$, and $N$. However, the closed, isolated system is only one
possible thermodynamic system. In many instances, it is desirable to
have different control variables, where we replace one (or more)
extensive variable with their conjugate intensive variable
(Fig.Â [\[fig:Fig4\]](#fig:Fig4)\{reference-type="ref"
reference="fig:Fig4"\}).

::: marginfigure
![image](figures/Physics/statistical_mechanics/statistical_ensembles/four ensembles.png)\{width="\\linewidth"\}
:::

For convenience, we start from the equation of state $U(S,V,N)$, which
is equivalent to $S(U,V,N)$. The extensive variables and their conjugate
intensive variables are related through the relationships

$$\begin\{aligned\}
\label\{Entropy Change2\}
S \leftrightarrow T = \left(\frac\{\partial U\}\{\partial S\}\right)_\{V,N_1,...,N_r\}\\ 
V \leftrightarrow -p = \left(\frac\{\partial U\}\{\partial V\}\right)_\{S,N_1,...,N_r\}\\
N_i \leftrightarrow \mu_i = \left(\frac\{\partial U\}\{\partial N_i\}\right)_\{S,V,N_\{j \neq i\}\}
\end\{aligned\}$$ Recall the notation we defined in the previous chapter
where underscored quantities on partial derivatives are held constant.

A thermodynamic ensemble is defined by the canonical variables of the
system. Changing from one ensemble to another amounts to shifting from
one thermodynamic potential to another that depends on the new canonical
variables. Replacing an extensive variable for its conjugate intensive
variable is effectively replacing the control variable with the slope of
the control variable.

## Microcanonical Ensemble

The microcanonical ensemble represents the possible states of a closed,
isolated system with a specified total amount of energy. This system is
thermodynamically characterized by the entropy $$\begin\{aligned\}
S = S(U,V,N)
\end\{aligned\}$$.

The statistical distribution for the molecular states of the system
needs to obeys the first and second laws of thermodynamics. The internal
energy $U$ identifies the total kinetic and potential energy of the
molecules in the system, thus in order to satisfy the first law we
require that only states with total energy $E = U$ are permitted in this
ensemble. The second law is satisfied by finding the probability
distribution that maximizes the system entropy.

::: marginfigure
![image](figures/Physics/statistical_mechanics/statistical_ensembles/Isolated_closed and open.png)\{width="\\linewidth"\}
:::

For fixed $E$, $V$, and $N$, there are many degenerate quantum states.
We define the system state as $\nu$, characterizing the quantum state of
the system for a given $E$, $V$, $N$, and the total degeneracy as
$\Omega$. The probability that the system exists in state $\nu$ at any
given time is $P_\nu$.

The connection between the microscopic and macroscopic approaches is
given by the Gibbs Entropy formula, which defines the system entropy $S$
as: $$\begin\{aligned\}
\label\{GibbsEntropy\}
S = - k_B \sum_\nu P_\nu \log P_\nu
\end\{aligned\}$$

where $k_B = 1.8\times10^\{-23\} J/K$ is the Boltzmann's constant. Note
the implicit assumption here that the number of states in the system is
summable. This assumption holds true since we discretize the system's
phase space into discrete microstates.

The Gibbs Entropy formula can be better understood from the perspective
of information theory. According to this $I(P) = \log P_\nu$ is the
*information function*, which defines the amount of information acquired
due to the observation of event $\nu$. The information function has the
following properties:

1.  It is a non-negative quantity $I(P) \ge 0$

2.  Events that always occur do not communicate information $I(1) = 0$

3.  The joint probability communicates as much information as two
    individual events separately $I(P_m P_n) = I(P_m) + I(P_n)$.

Thus, the definition of entropy from
([\[GibbsEntropy\]](#GibbsEntropy)\{reference-type="ref"
reference="GibbsEntropy"\}) might be thought as the expected value of the
amount of information extracted from all the microscopic states in the
macroscopic system. Our task is to determine the value of $P_\nu$ that
maximizes the entropy while maintaining the normalization, thus we must
maximize $S$ subject to a constraint $$\begin\{aligned\}
\sum_\nu P_\nu = 1
\end\{aligned\}$$ A good mathematical strategy to maximize a function
subject to different constraints is by using a Lagrange multiplier,
which defines that the maximization of a function $f(x,y)$ subject to
the constraint $g(x,y) = c$, is given by maximizing $$\begin\{aligned\}
\mathcal\{L\} (x,y,\lambda) = f(x,y) + \lambda g(x,y)
\end\{aligned\}$$ Thus, we seek the maximization of the function:
$$\begin\{aligned\}
\mathcal\{L\}(P_\nu,\lambda_0) = k_B\sum_\nu P_\nu \log P_\nu - \lambda_0\sum_\nu P_\nu 
\end\{aligned\}$$

Maximization occurs when a perturbation $\delta P_\nu$ does not alter
the function (first variation is zero), occurring when:
$$\begin\{aligned\}
0&= \frac\{d\}\{d P_\nu\} \mathcal\{L\}(P_\mu, \lambda_0) \\
&= -k_B\sum_\nu\left(\log P_\nu +1\right) \delta P_\nu - \lambda_0 \sum_\nu\delta P_\nu 
\end\{aligned\}$$

Since $\delta P_\nu$ is arbitrary, the function is maximized if:
$$\begin\{aligned\}
-k_B \log P_\nu - k_B - \lambda_0 &= 0 \\ P_\nu &=\exp\left(-1-\frac\{\lambda_0\}\{k_B\}\right) 
\end\{aligned\}$$

Considering $\Omega$ as the total number of microstates, and the
normalization of the probability, we get the following relation:
$$\begin\{aligned\}
\sum_\nu^\Omega P_\nu &= 1 \\
\sum_\mu^\Omega \exp \left ( -1 - \frac\{\lambda_0\}\{k_B\} \right ) &= 1\\
\Omega \exp \left ( -1 - \frac\{\lambda_0\}\{k_B\} \right ) &= 1 \\
\frac\{1\}\{\Omega\} &= \exp\left(-1 - \frac\{\lambda_0\}\{k_B\}\right)
\end\{aligned\}$$

This leaves the probability distribution: $$\begin\{aligned\}
\label\{ProbMicrocano\}
P_\nu = \frac\{1\}\{\Omega\}
\end\{aligned\}$$

Equation ([\[ProbMicrocano\]](#ProbMicrocano)\{reference-type="ref"
reference="ProbMicrocano"\}) means that the system is equally likely to
be found in any of its accessible states. This is called the principle
of *equal a priori probabilitie*. Additionally we found that in an
isolated system at thermal equilibrium at a given $E$, $V$, $N$, the
system spends equal amount of time in each state over a sufficiently
long period of time, *i.e* the system is *ergodic*. This is always true
provided that the motion of molecules must be sufficiently random. It
implies that the observation time $t\gg \tau$, where $\tau$ is a
microscopic relaxation time for the system (*e.g.* collision time in
gases).

Finally we find that the entropy of the macroscopic system is given by:
$$\begin\{aligned\}
\label\{Entropy\}
S = k_B \log \Omega
\end\{aligned\}$$

which gives complete knowledge of all thermodynamic behavior of the
system, provided it can be evaluated.

    class MicrocanonicalEnsemble[ProbabilityDistribution]($S$: $\mathbb\{R\}$):
      def $\lambda$():
        # Sample Any Accessible State with entropy $S$

### Canonical Ensemble

The canonical ensemble represents the possible states of a closed,
isolated system that is in thermal equilibrium with a heat bath. This
system is thermodynamically characterized by the Helmholtz free energy
$$\begin\{aligned\}
F = F(T,V,N)
\end\{aligned\}$$

We need to find a statistical distribution for the molecular states of
the system that obeys the first and second laws of thermodynamics. In
this case, in order to satisfy the first law, the internal energy $U$
identifies the statistical average of the kinetic and potential energy
of the molecules in the system, defined by $$\begin\{aligned\}
\langle E\rangle
\end\{aligned\}$$ As in the microcanonical ensamble, second law is
satisfied by finding the probability distribution maximizes the system
entropy.

In this ensemble, the total energy $E$ is not uniquely fixed. As a
result, the summation over states of the system includes all microstates
of the system, with each weighted by the appropriate probability.

To illustrate what is meant by this summation, consider the total energy
from the translational microstates of a single point particle in two
dimensions. Therefore, the internal energy is found to satisfy:
$$\begin\{aligned\}
U = \langle E \rangle = \sum_\{n_x=1\}^\infty \sum_\{n_y=1\}^\infty E_\{n_x,n_y\} P_\{n_x,n_y\} 
\end\{aligned\}$$

where $P_\{n_x,n_y\}$ is the probability of finding the system in the
quantum state $n_x,n_y$ (to be explicitly determined).

Begin with our microscopic definition of entropy: $$\begin\{aligned\}
S = - k_B \sum_\mu P_\mu \log P_\mu 
\end\{aligned\}$$

where the sum over $\mu$ implies a sum over all microstates.

In a fixed-temperature ensemble, the total system energy fluctuates
about an average value, such that $\langle E \rangle = U$ (internal
energy). We introduce the two constraints on the probability
distribution: $$\begin\{aligned\}
\sum_\mu P_\mu &= 1 \\
\langle E \rangle &= \sum_\mu E_\mu P_\mu = U 
\end\{aligned\}$$

The probability distribution that maximizes entropy while satisfying
these constraints corresponds to the maximization of: $$\begin\{aligned\}
S - \lambda_0 \sum_\mu P_\mu - \lambda_1 \langle E \rangle = \sum_\mu \left( - k_B P_\mu \log P_\mu - \lambda_0 P_\mu - \lambda_1 E_\mu P_\mu \right) 
\end\{aligned\}$$

The Lagrange multipliers $\lambda_0$ and $\lambda_1$ assume values that
satisfy the constraints. The first variation of this function is set to
zero, giving: $$\begin\{aligned\}
\sum_\mu \left( - k_B \log P_\mu - k_B - \lambda_0 - \lambda_1 E_\mu \right)\delta P_\mu = 0 
\end\{aligned\}$$ Since the $\delta P_\mu$ are arbitrary, each term is set
to zero, thus: $$\begin\{aligned\}
 0 &= -k_B \log P_\mu - k_B - \lambda_0 - \lambda_1 E_\mu  \\
 P_\mu &= \exp \left( -1-\frac\{\lambda_0\}\{k_B\}\right)\exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) 
\end\{aligned\}$$

The first constraint $\sum_\mu P_\mu = 1$ results in: $$\begin\{aligned\}
1 &= \exp \left( -1-\frac\{\lambda_0\}\{k_B\}\right)\sum_\mu \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) \\
\exp \left( 1 + \frac\{\lambda_0\}\{k_B\}\right) &= \sum_\mu \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) 
\end\{aligned\}$$ We define the *partition function* $Z$ as
$$\begin\{aligned\}
    Z &= \exp \left( 1 + \frac\{\lambda_0\}\{k_B\}\right)
\end\{aligned\}$$

The partition function is a powerful mathematical tool for describing a
statistical mechanical system. We will use partition functions to derive
a number of useful quantities.

The probability $P_\mu$ now satisfies: $$\begin\{aligned\}
P_\mu=\frac\{1\}\{Z\} \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\mu \right) 
\end\{aligned\}$$

where $\lambda_1$ is a currently unspecified Lagrange multiplier.

To satisfy the second constraint
$\sum_\mu E_\mu P_\mu = \langle E\rangle$, we invoke thermodynamic
properties. The differential of the entropy $S$ gives $$\begin\{aligned\}
\left(\delta S\right)_\{V,N\} &= \sum_\mu \left( - k_B \log P_\mu - k_B\right)\delta P_\mu \\ &= \sum_\mu \left(\lambda_1 E_\mu + k_B \log Z - k_B\right)\delta P_\mu
\end\{aligned\}$$

and noting that $\delta 1 = \sum_\mu \delta P_\mu = 0$, we have:
$$\begin\{aligned\}
\left(\delta S\right)_\{V,N\}  = \sum_\mu \lambda_1 E_\mu \delta P_\mu 
\end\{aligned\}$$

The differential of the average energy $\langle E \rangle$ gives:
$$\begin\{aligned\}
\left(\delta \langle E \rangle \right)_\{N,V\} = \sum_\mu E_\mu \delta P_\mu 
\end\{aligned\}$$

From thermodynamics, the Lagrange multiplier $\lambda_1$ is:
$$\begin\{aligned\}
\left( \frac\{\delta S\}\{\delta \langle E \rangle\} \right)_\{V,N\} = \lambda_1 = \frac\{1\}\{T\}
\end\{aligned\}$$

The governing equations for the canonical ensemble are:
$$\begin\{aligned\}
\label\{ProbCanonical\}
P_\mu = \frac\{1\}\{Z\}\exp\left(-\frac\{E_\mu\}\{k_BT\}\right)
\end\{aligned\}$$

where: $$\begin\{aligned\}
\label\{PartCanonical\}
Z = \sum_\mu \exp\left(-\frac\{E_\mu\}\{k_BT\}\right)
\end\{aligned\}$$

Using the definition of the probability distribution and thermodynamics,
we have:

$$\begin\{aligned\}
k_BT \log P_\mu &= -E_\mu - k_BT \log Z \\
k_BT \sum_\mu P_\mu \log P_\mu &= -\sum_\mu P_\mu E_\mu - k_BT \log Z \sum_\mu P_\mu \\
-TS &=-\langle E \rangle - k_B T \log Z 
\end\{aligned\}$$

Since the Helmholtz free energy is given by
$F = \langle E \rangle - TS$, we get:

$$\begin\{aligned\}
F = -k_BT\log Z
\end\{aligned\}$$

The canonical partition function $Z$ provides complete information about
the thermodynamic behavior. If we write the partition function as
$$\begin\{aligned\}
Z = \sum_\mu \exp\left(-\beta E_\nu\right)
\end\{aligned\}$$ where $\beta = 1/\left(k_BT\right)$. The partition
function $Z$ acts as a generating function: $$\begin\{aligned\}
\langle E^m \rangle &= \sum_\nu E_\nu^m P_\nu \\
&= \frac\{1\}\{Z\}\sum_\nu E^m_\nu \exp\left(-\beta E_\nu \right) \\
&= \left(-1\right)^m \frac\{1\}\{Z\} \left(\frac\{\partial^m Z\}\{\partial \beta^m\}\right)_\{V,N\} 
\end\{aligned\}$$

Consider the variance of the energy: $$\begin\{aligned\}
\langle E^2 \rangle - \{\langle E \rangle\}^2 &= \frac\{1\}\{Z\} \left(\frac\{\partial^2 Z\}\{\partial \beta^2\}\right)_\{V,N\} - \frac\{1\}\{Z^2\} \left(\frac\{\partial Z\}\{\partial \beta\}\right)_\{V,N\}^2 \\
&= \left(\frac\{\partial^2 \log Z\}\{\partial \beta^2\}\right)_\{V,N\} \\
&= -\left(\frac\{\partial^2 \beta F\}\{\partial \beta^2\}\right)_\{V,N\} \\
&= k_B T^2 C_V 
\end\{aligned\}$$

connecting energy fluctuation to the heat capacity $C_V$. Bringing this
all together, we can define the canonical ensemble in Physika

``` \{.python language="python"\}
class CanonicalEnsemble[ProbabilityDistribution]
  def Z(N: $\mathbb\{N\}$) : $\mathbb\{R\}$:
    out = 0
    for i in N:
      $\mu$ = sample_microstate()
      out += exp(-$E_\mu$/($k_B$ T))
    out
```

### Grand Canonical Ensemble

The Grand Canonical ensemble represents the possible states of an open
system that is in equilibrium with a reservoir. This system is
thermodynamically characterized by the Landau potential
$$\begin\{aligned\}
-pV = -pV(T,V,\mu)
\end\{aligned\}$$ This thermodynamic system is called the *grand canonical
ensemble* when considered in statistical thermodynamics. We need to find
a statistical distribution for the molecular states of the system that
obeys the first and second laws of thermodynamics. In this case the
internal energy $U$ identifies the statistical average of the kinetic
and potential energy $\langle E\rangle$ of the molecules in the system
(first law), and the probability distribution maximizes the system
entropy (second law).

In this ensemble, the total energy $E$ and the number of particles
$N_i'$ $(i=1,2,...,r)$ are the fluctuating variables.

We begin with our microscopic definition of entropy: $$\begin\{aligned\}
S = -k_B \sum_\nu P_\nu \log P_\nu 
\end\{aligned\}$$

where the sum over $\nu$ implies a sum over all system states and all
particle number $N_i' = 0,1,...$ with $i = 1,2...,r$.

We introduce the $2+r$ constraints on the probability distribution:
$$\begin\{aligned\}
\sum_\nu P_\nu &= 1\\
\langle E\rangle &= \sum_\nu E_\nu P_\nu = U \\
\langle N_i \rangle &= \sum_\nu^r N_i' P_\nu = N_i 
\end\{aligned\}$$

The probability distribution that maximizes entropy while satisfying
these constraints corresponds to the maximization of: $$\begin\{aligned\}
\mathcal\{L\} (P_\nu,\lambda_0,\lambda_1,\Lambda_1,...,\Lambda_r)= \sum_\nu \left( -k_B P_\nu \log P_\nu - \lambda_0 P_\nu - \lambda_1 E_\nu P_\nu - \sum_\{i=1\} ^r\Lambda_iN_i'P_\nu \right) 
\end\{aligned\}$$

The Lagrange multipliers $\lambda_0, \lambda_1,$ and
$\Lambda_i = (i =1,2,...r)$ assume values that satisfy the constraints.

Setting the first variation of this function to zero, we have:
$$\begin\{aligned\}
\sum_\nu\left(-k_B \log P_\nu - k_B - \lambda_0 - \lambda_1 E_\nu - \sum_\{i=1\} ^r\Lambda_iN_i'\right)\delta P_\nu = 0 
\end\{aligned\}$$

Since the $\delta_P$ are arbitrary, each term is set to zero, giving:
$$\begin\{aligned\}
\left(-k_B  \log P_\nu - k_B - \lambda_0 - \lambda_1 E_\nu - \sum_\{i=1\} ^r\Lambda_i N_i'\right) =0 \\
P_\nu = \exp \left(-1-\frac\{\lambda_0\}\{k_B\}\right) \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right) 
\end\{aligned\}$$

The first constraint $\sum_\nu P_\nu = 1$ results in: $$\begin\{aligned\}
1 = \exp \left(-1-\frac\{\lambda_0\}\{k_B\}\right) \sum_\nu  \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right) 
\end\{aligned\}$$

from where we can define the *grand canonical partition function* $\Xi$:
$$\begin\{aligned\}
\label\{PartGrand\}
\Xi &= \exp \left(1+\frac\{\lambda_0\}\{k_B\}\right) \\
&= \sum_\nu  \exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right)
\end\{aligned\}$$

The probability $P_\nu$ now satisfies: $$\begin\{aligned\}
\label\{ProbGrand\}
P_\nu = \frac\{1\}\{\Xi\}\exp\left(-\frac\{\lambda_1\}\{k_B\}E_\nu - \sum_\{i=1\}^r \frac\{\Lambda_i\}\{k_B\} N_i'\right)
\end\{aligned\}$$

To satisfy the constraints $$\begin\{aligned\}
\sum_\nu E_\nu P_\nu &= \langle E\rangle \\ \sum_\nu N_i' P_\nu &= \langle N_i \rangle
\end\{aligned\}$$ we invoke thermodynamic properties. The differential of
the entropy $S$ gives: $$\begin\{aligned\}
\left(\delta S\right)_\{V\} &= \sum_\nu \left( - k_B \log P_\nu - k_B\right)\delta P_\nu \\ &= \sum_\nu \left(\lambda_1 E_\nu + \sum_\{i=1\}^r\Lambda_iN_i' - \log \Xi - k_B\right)\delta P_\nu 
\end\{aligned\}$$

and noting that $\delta 1 = \sum_\nu \delta P_\nu = 0$, we have:
$$\begin\{aligned\}
\left(\delta S\right)_\{V\}  = \sum_\nu \left( \lambda_1 E_\nu  + \sum_\{i=1\}^r\Lambda_iN_i'\right) \delta P_\nu 
\end\{aligned\}$$ The differentials of the average energy
$\langle E \rangle$ and $\langle N_i \rangle$ gives: $$\begin\{aligned\}
\left(\delta \langle E \rangle \right)_\{V\} &= \sum_\nu E_\nu \delta P_\nu \hspace\{0.2cm\} \\
\left(\delta \langle N_i \rangle \right)_\{V\} &= \sum_\nu N_i' \delta P_\nu 
\end\{aligned\}$$

From thermodynamics, the Lagrange multiplier $\lambda_1$ is:
$$\begin\{aligned\}
\left( \frac\{\delta S\}\{\delta \langle E \rangle\} \right)_\{V,N\} &= \lambda_1 = \frac\{1\}\{T\} \\ 
\left( \frac\{\delta S\}\{\delta \langle N_i \rangle\} \right)_\{E,V,N_\{j\neq i\}\} &= \Lambda_i = - \frac\{\mu_i\}\{T\} 
\end\{aligned\}$$

The governing equations for the grand canonical ensemble are:
$$\begin\{aligned\}
P_\mu = \frac\{1\}\{\Xi\}&\exp\left(-\frac\{E_\nu\}\{k_BT\}+ \sum_\{i=1\}^r\frac\{\mu_iN_i'\}\{k_BT\} \right) \hspace\{0.5cm\} \\ 
\Xi &= \sum_\nu \exp\left(-\frac\{E_\nu\}\{k_BT\}+ \sum_\{i=1\}^r\frac\{\mu_iN_i'\}\{k_BT\}\right) 
\end\{aligned\}$$

``` \{.python language="python"\}
class GrandCanonicalEnsemble[ProbabilityDistribution]
  def $\Xi$(N: $\mathbb\{N\}$):
    out = 0
    for _ in N:
      $\mu$ = sample_microstate()
      out += exp(-$E_\mu$/(k_B*T) + sum(1, r, $\mu_i N_i'$/($k_B$ T)))
    out
```

Using the probability distribution and thermodynamics, we have:
$$\begin\{aligned\}
k_BT\log P_\nu = -E_\nu + \sum_\{i=1\}^r\mu_iN_i' -k_BT\log\Xi \\
-TS =-\langle E \rangle + \sum_\{i=1\}^r\mu_i \langle N_i \rangle -k_BT\log\Xi \\
-pV = \langle E \rangle -TS - \sum_\{i=1\}^r \mu_i \langle N_i \rangle = -k_BT\log\Xi 
\end\{aligned\}$$

Therefore, the grand canonical partition function $\Xi$ provides
complete knowledge of the thermodynamic behavior of the system.
Furthermore, the grand canonical partition function acts as a generating
function for averages; for example, the moments are: $$\begin\{aligned\}
\langle E^m \rangle = (-1)^m\frac\{1\}\{\Xi\} \left(\frac\{\partial^m \Xi\}\{\partial \beta^m\}\right)_\{V,\mu\} \hspace\{0.2cm\} \text\{and\} \hspace\{0.2cm\} \langle N_i^m \rangle = \frac\{1\}\{\Xi\} \left(\frac\{\partial^m \Xi\}\{\partial \lambda_i^m\}\right)_\{T,V,\mu_\{j\neq i\}\} 
\end\{aligned\}$$ where $\beta= 1/(k_BT)$ and $\lambda_i = \mu_i/(k_BT)$.
