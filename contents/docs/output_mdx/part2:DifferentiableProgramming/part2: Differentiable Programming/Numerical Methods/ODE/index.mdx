# Implicit Methods and Stiffness in Ordinary Differential Equations \{#chap:implicit_methods\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\}\

------------------------------------------------------------------------

In this chapter we will learn what stiffness means and the implications
stiffness has for solving ordinary differential equations. We will then
learn how the implicit Euler's method can be used to solve stiff
ordinary differential equations.

## Stiff ODEs

Stiffness is a subtle, difficult, and important concept in the numerical
solution of ordinary differential equations. It depends on the
differential equation, the initial conditions, and the numerical method.
A *stiff* system is one which involves rapidly changing components
together with slowly changing ones. In some cases, the rapidly varying
components are ephemeral (lasting a short time) transients that die away
quickly, after which the solution becomes dominated by the slowly
varying components. Although the transient phenomena exist for only a
short part of the integration interval, they can dictate the time step
for the entire solution.

Both individual ODEs and system of ODEs can be stiff. A simple example
of a stiff ODE is given below for visualization. $$\begin\{aligned\}
    \frac\{dy\}\{dt\} = -1000y+3000-2000e^\{-t\}
\end\{aligned\}$$ Let the initial condition be $y(0)=0$. The analytical
solution for this problem can be solved as $$\begin\{aligned\}
y(t) = 3 - 0.998e^\{-1000t\}-2.002e^\{-t\}
\end\{aligned\}$$

::: marginfigure
![image](figures/part1b/implicit_methods/stiff_0.png)\{width="2.5in"\}
:::

This solution is plotted and as it can be seen, the solution is
initially dominated by the fast exponential term of $e^\{-1000t\}$.
Although the solution appears to start at 1, there is actually a fast
transient from $y$ = $0$ to $1$ that occurs in less than the 0.005 time
unit. This transient is perceptible only when the response is viewed on
the finer timescale in the inset. After a short period (t \< 0.005),
this transient dies out and the solution becomes governed by the slow
exponential ($e^\{-t\}$).

Insight into the step size required for stability of such a solution can
be gained by examining the homogeneous part of the differential
equation. $$\begin\{aligned\}
\frac\{dy\}\{dt\} = -1000y
\end\{aligned\}$$ If $y(0)=y_0$, calculus can be used to determine the
solution as $y = y_0e^\{-1000t\}$. Thus, the solution starts at $y_0$ and
asymptotically approaches zero. Euler's method can be used to solve the
same problem numerically: $$\begin\{aligned\}
y_\{i+1\} = y_i + \dfrac\{dy_i\}\{dt\}h
\end\{aligned\}$$ For this ODE, the above equation is simplified to
$$\begin\{aligned\}
y_\{i+1\} = y_i -1000y_ih = y_i(1-1000h)
\end\{aligned\}$$ The stability of this formula clearly depends on the
step size $h$. That is, $|1-1000h|$ must be less than $1$. Thus, if
$h>2/1000$, $|y_i| \rightarrow \infty$ as $i \rightarrow \infty$. For
the fast transient part of the solution ($e^\{-1000t\}$ term), this
criterion shows that the step size to maintain stability must be less
than $2/1000 = 0.002$. In addition, it should be noted that, whereas
this criterion maintains stability (i.e., a bounded solution), an even
smaller step size would be required to obtain an accurate solution.
Thus, although the transient occurs for only a small fraction of the
integration interval, it controls the maximum allowable step size.

## Implicit methods and Adaptive time stepping

One way of solving the above problem is to use adaptive time-stepping,
to avoid the small time-step throughout. This is obtained by bounding
the local truncation error as discussed in earlier chapter. Some Physika
code for finding the adaptive time steps to solve the integral is given
below.

::: marginfigure
![image](figures/part1b/implicit_methods/adaptive.png)\{width="2.5in"\}
:::

``` \{.python language="python"\}
def adaptive_step($\frac\{dy\}\{dt\}$: $\mathbb\{R\} \to \mathbb\{R\}$, t$_i$: $\mathbb\{R\}$ = 0, t$_f$: $\mathbb\{R\}$ = 3, t_adp: $\mathbb\{R\}[n]$):
  f_d = $\nabla$($\frac\{dy\}\{dt\}$, t)
  y[0] = 0, e_b = 1e-10, t_adp[0] = t$_i$
  h = sqrt(2*e_b/|f_d(y[0],t_adp[0])|)
  while(t_adp[i] < t$_f$):
    y[i+1] = y[i] + $\frac\{dy\}\{dt\}$(y[i],t_adp[i]) * h[i]
    h[i+1] = sqrt(2*e_b/abs(f_d(y[i],t_adp[i])))
    t_adp[i+1] = t_adp[i]+h[i+1]
    i = i+1
```

In contrast to explicit approaches, implicit methods offer an
alternative remedy. Such representations are called implicit because the
unknown appears on both sides of the equation. An implicit form of
Euler's method can be developed by evaluating the derivative at the
future time. An implicit form of Euler's method would be:
$$\begin\{aligned\}
y_\{i+1\} = y_i + \dfrac\{dy_\{i+1\}\}\{dt\}h
\end\{aligned\}$$

This is called the *backward* or *implicit* Euler's method. For the ODE
under consideration, the above equation is simplified to
$$\begin\{aligned\}
y_\{i+1\} = y_i - 1000y_\{i+1\}h
\end\{aligned\}$$ which can be solved for as $$\begin\{aligned\}
y_\{i+1\} = \dfrac\{y_i\}\{1+1000h\}
\end\{aligned\}$$ For this case, regardless of the size of the step,
$|y_i|\rightarrow 0$ as $i\rightarrow \infty$. Hence, the approach is
called *unconditionally stable*.

::: marginfigure
![image](figures/part1b/implicit_methods/stiffeuler0.png)\{width="2.5in"\}
:::

For linear ODEs, the equations need to be rearranged to solve for the
next ODE update in the implicit Euler's method approach. For nonlinear
ODEs, the solution becomes even more difficult since it involves solving
a system of nonlinear simultaneous equations. Even though stability is
gained through implicit approaches, it is at the cost of added solution
complexity. We will look at how to solve such a non-linear system in the
example below.

## Van der Pol Equation

The van der Pol equation is a model of an electronic circuit that arose
back in the days of vacuum tubes. It evolves in time according to the
second-order differential equation: $$\begin\{aligned\}
\dfrac\{d^2y\}\{dt^2\} - \mu(1-y^2)\dfrac\{dy\}\{dt\} + y = 0
\end\{aligned\}$$

The solution to this equation becomes progressively stiffer as $\mu$
gets large. As an example, given the initial conditions, $y(0) = 0$ and
$\dfrac\{dy\}\{dt\} = 1$ at $t=0$, we want to solve the van der Pol equation
using explicit and implicit Euler's Method from $t=0$ to $t=10$, using a
step-size of 0.05 and $\mu$ of 5.

The first step is to convert the second-order ODE into a pair of
first-order ODEs by defining

::: marginfigure
![image](figures/part1b/implicit_methods/explicit.png)\{width="2.5in"\}
:::

::: marginfigure
![image](figures/part1b/implicit_methods/implicit.png)\{width="2.5in"\}
:::

$$\begin\{aligned\}
\dfrac\{dy_1\}\{dt\} &= y_2 \\
\dfrac\{dy_2\}\{dt\} &= \mu(1-y_1^2)y_2 + y_1
\end\{aligned\}$$

The update steps for the Explicit Euler's method are simply
$$\begin\{aligned\}
    y_\{1,i+1\} &=    y_\{1,i\} + \dfrac\{dy_1\}\{dt\}|_\{i\}h\\
    y_\{2,i+1\} &=    y_\{2,i\} + \dfrac\{dy_2\}\{dt\}|_\{i\}h    
\end\{aligned\}$$

The update steps for the Implicit Euler's method are obtained by adding
the derivative at the future step. These are given by $$\begin\{aligned\}
y_\{1,i+1\} &=    y_\{1,i\} + \dfrac\{dy_1\}\{dt\}|_\{i+1\}h\\
y_\{2,i+1\} &=    y_\{2,i\} + \dfrac\{dy_2\}\{dt\}|_\{i+1\}h  
\end\{aligned\}$$

The above expressions are simplified as $$\begin\{aligned\}
y_\{1,i+1\}  &=   y_\{1,i\} + y_\{2,i+1\}h\\
y_\{2,i+1\}  &=   y_\{2,i\} + (\mu(1-y_\{1,i+1\}^2)y_\{2,i+1\} + y_\{1,i+1\})h
\end\{aligned\}$$

From the above two equations, the updates are obtained by solving for
$y_\{1,i+1\}$ and $y_\{2,i+1\}$. Since, these are a pair of non-linear
equations, the solution is obtained using the `fsolve` function in
Physika. These solutions are used as the update steps in Implicit
Euler's method.

``` \{.python language="python"\}
def root2d(y, y1, y2, $\mu$, h):
 y1dash = y[0], y2dash = y[1]
 F[0] = -y1dash + y1 + (y2dash)*h
 F[1] = -y2dash + y2 + (($\mu$*(1-y1dash$^2$)*y2dash - y1dash))*h
 return F
```

``` \{.python language="python"\}
def implicit_euler_method($\mu$: $\mathbb\{R\}$, t$_f$: $\mathbb\{R\}$, dt: $\mathbb\{R\}$):
  t = 0:dt:t$_f$
  n = len(t)
  y1i = zeros(n,1), y2i = zeros(n,1)
  y1i[0] = 1, y2i[0] = 1
  for i in 1 .. n-1:
    solution = fsolve($\lambda$ (y) root2d(y,y1i[i],y2i[i],$\mu$,dt),  [y1i[i], y2i[i]])
    y1i[i+1] = solution[0]
    y2i[i+1] = solution[1]     
```

Since the van der Pol equation with $\mu=5$ is a stiff system, the
explicit method clearly fails in capturing the response of the variable.
However, the implicit method still captures the oscillations in the
response.


# The Finite Difference Method \{#chap:linear_boundary\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:linear_systems\]](#chap:linear_systems)\{reference-type="ref+label"
reference="chap:linear_systems"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter we learn how to implement the finite-difference method
and study how derivative boundary conditions are incorporated into the
finite-difference method. By the end of the chapter, you will know how
to solve 2nd-order nonlinear ODEs with the finite-difference method by
using root-location methods for systems of nonlinear algebraic
equations.

## Finite Difference Method

A boundary value problem is a differential equation together with a set
of additional constraints, called the boundary conditions. A solution to
a boundary value problem is a solution to the differential equation
which also satisfies the boundary conditions. In the finite difference
method, finite differences are substituted for the derivatives in the
original equation. Thus, a linear differential equation is transformed
into a set of simultaneous algebraic equations which can then be solved.

Consider a pod of mass $m$ traveling inside a vacuum tube which is 1
mile in length. The pod experiences a drag force proportional to its
velocity (by factor $k$) due to the magnets used to levitate and brake.
The vacuum tube is maintained at very low pressure (almost close to
vacuum) and hence the drag due to air is almost zero. A pusher attached
to the pod provides a constant propelling force $F$ to move the pod in
the tube. Friction due to wheels is neglected as the pod is assumed to
levitate. This setup can be modeled as $$\begin\{aligned\}
m\dfrac\{d^2x\}\{dt^2\} = F - k\dfrac\{dx\}\{dt\}
\end\{aligned\}$$

::: marginfigure
![image](figures/part1b/boundary_value/Hyperloop_Finite_approach.png)\{width="2.4in"\}
:::

As shown in Figure [\[fd:fig:1\]](#fd:fig:1)\{reference-type="ref"
reference="fd:fig:1"\}, the solution domain is first divided into a
series of time steps. At each time step, finite difference
approximations can be written for the derivatives in the equation. For
example, at time step $i$, the second and first derivative can be
represented by $$\begin\{aligned\}
\frac\{d^2x\}\{dt^2\} &= \dfrac\{x_\{i-1\}-2x_i+x_\{i+1\}\}\{\Delta t^2\}\\
\frac\{dx\}\{dt\} &=  \dfrac\{x_\{i+1\}-x_i\}\{\Delta t\};
\end\{aligned\}$$

This approximation can be substituted into the governing equation to
give $$\begin\{aligned\}
m\left (\dfrac\{x_\{i-1\}-2x_i+x_\{i+1\}\}\{\Delta t^2\}\right) &= F - k\left (\dfrac\{x_\{i+1\}-x_i\}\{\Delta t\}\right )\\
-mx_\{i-1\} + (k\Delta t + 2m)x_i - (k\Delta t + m)x_\{i+1\} &= -F\Delta t^2
\end\{aligned\}$$

If the domain is divided into $n$ time steps then $x_0$ and $x_n$ values
are specified by the boundary conditions. Therefore, the problem reduces
to solving $n-1$ simultaneous linear algebraic equations for the $n-1$
unknowns. The time steps are numbered consecutively, and since each
equation consists of a time step $(i)$ and its adjoining neighbors
($i-1$ and $i+1$), the resulting set of linear algebraic equations will
be tridiagonal.\

## Tridiagonal Matrix Algorithm

This method, also known as the Thomas algorithm, is a simplified form of
Gaussian elimination that can be used to solve tridiagonal systems of
equations. A tridiagonal system for $n$ unknowns may be written as

$$\begin\{aligned\}
a_ix_\{i-1\} + b_ix_i + c_ix_\{i+1\} = d_i
\end\{aligned\}$$ where $a_1 = 0$ and $c_n = 0$.

::: margintable
$$\begin\{bmatrix\}
b_1 & c_1 &  &  & 0\\ 
a_2 & b_2 & c_2 &  & 0 \\ 
  & a_3 & b_3 & \ddots & \\
 &  & \ddots & \ddots & c_\{n-1\}\\
0  &  &  & a_n & b_n
\end\{bmatrix\} 
\left \{
\begin\{tabular\}\{c\}
$x_1$ \\
$x_2$ \\
$x_3$  \\
$\vdots$ \\
$x_n$
\end\{tabular\}
\right \}
=
\left \{
\begin\{tabular\}\{c\}
$d_1$ \\
$d_2$ \\
$d_3$ \\ 
$\vdots$\\
$d_n$
\end\{tabular\}
\right \}$$
:::

For such systems, the solution can be obtained in *O(n)* operations
instead of *$O(n^3)$* required by Gaussian elimination. A first sweep
eliminates the $a_i$'s, and then an (abbreviated) backward substitution
produces the solution.\
The equations to be solved are: $$\begin\{aligned\}
 b_0 x_0 + c_0x_1 = d_0 \qquad &i = 0\\
a_ix_\{i-1\} + b_ix_i + c_ix_\{i+1\} = d_i \qquad &i = 1,....,n-2\\
a_\{n-1\}x_\{n-2\} + b_\{n-1\}x_\{n-1\} = d_\{n-1\}\qquad  &i = n - 1
\end\{aligned\}$$ Multiplying the first equation with the fraction
($a_1/b_0$) and subtracting it from the second equation $(i = 1)$ to
eliminate $x_0$ gives $$\begin\{aligned\}
\left (b_1 - \frac\{a_1\}\{b_0\}c_0\right )x_1 + c_1x_2 = d_1 - \frac\{a_1\}\{b_0\}d_0
\end\{aligned\}$$

Next modify the third equation $(i = 2)$ with the initial second
equation to eliminate $x_1$. This procedure is repeated until the
$(N-1)^\{th\}$ row; the (modified) $(N-1)^\{th\}$ equation will involve only
one unknown, $x_\{N-1\}$. This may be solved for and then used to solve
the $(N-2)^\{th\}$ equation, and so on until all of the unknowns are
solved for. The coefficients of the modified equations can be defined as
follows: $$\begin\{aligned\}
b_i^\{'\} &= b_i - \left (\dfrac\{a_i\}\{b_\{i-1\}\}\right )c_\{i-1\}\\
c_i^\{'\} &= c_i\\
d_i^\{'\} &= d_i - \left (\dfrac\{a_i\}\{b_\{i-1\}\}\right )d_\{i-1\}
\end\{aligned\}$$

where the system is redefined as: $$\begin\{aligned\}
b'_ix_i + c_ix_\{i+1\} &= d'_i \ \mathrm\{for\}\ i = 1,\dotsc,N-2\\
b'_\{N-1\}x_\{N-1\} &= d'_\{N-1\} \ \mathrm\{for\}\  i = N-1
\end\{aligned\}$$ The last equation involves only one unknown. Solving it
in turn reduces the next last equation to one unknown, so that this
backward substitution can be used to find all of the unknowns:
$$\begin\{aligned\}
b'_ix_i + c_ix_\{i+1\} &= d'_i \  \mathrm\{for\}\  i = n-1,n-2,\dotsc,1
\end\{aligned\}$$ Let us now consider how to implement the Tridiagonal
matrix algorithm in Physika

``` \{.python language="python"\}
def tridiagonal(F: $\mathbb\{R\}$, m: $\mathbb\{R\}$, k: $\mathbb\{R\}$, (xi, xe) : $\mathbb\{R\}^2$, (ti,te): $\mathbb\{R\}^2$, dt: $\mathbb\{R\}$):
  N = (te - ti)/dt- 1
  a = -m*ones(N)
  b = (k*dt + 2*m)*ones(N)
  c = -(k*dt + m)*ones(N)
  d = (-F*dt$^2$)*ones(N)
  d[0] = -F*dt$^2$ + m*xi
  d[N-1] = -F*dt$^2$ + xe*(k*dt + m)
  x = ones(N)

  for i:
    b[i] = b[i] - (a[i]/b[i-1])*c[i-1]
    d[i] = d[i] - (a[i]/b[i-1])*d[i-1]  
  x[N-1] = d[N-1]/b[N-1] # Perform the backsubstitution
  for j = N-2:-1:0:
    x[j] = (d[j] - x[j+1]*c[j])/b[j]
  x
```

We have implemented this equation for the specific equation
$$\begin\{aligned\}
    m\frac\{d^2x\}\{dt^2\} &= f - k \frac\{dx\}\{dt\}.
\end\{aligned\}$$ This implementation will not directly generalize to
higher order equations, which will require a more complex equation
solution method.

## Derivative Boundary Conditions

There are three types of boundary conditions commonly encountered in the
solution of ordinary or partial differential equations:

-   *Dirichlet boundary condition*: It specifies the values that a
    solution needs to take on along the boundary of the domain.

-   *Neumann boundary condition*: It specifies the values that the
    derivative of a solution is to take on the boundary of the domain.

-   *Robin boundary condition*: It is a specification of a linear
    combination of the values of a function and the values of its
    derivative on the boundary of the domain.

The derivation in the previous section can be modified to work for
Neumann boundary conditions.

## The Finite Difference Method for Nonlinear ODEs

For nonlinear ODEs, the substitution of finite differences yields a
system of nonlinear simultaneous equations. Thus, the most general
approach to solving such problems is to use root-location methods for
systems of equations such as the Newton-Raphson method which will be
introduced in next lecture. An adaptation of successive substitution can
sometimes provide a simpler alternative.

Consider the tube problem solved above. If the pressure in the tube is
not close to vacuum then the air drag comes into picture.
$$\begin\{aligned\}
m\dfrac\{d^2x\}\{dt^2\} = F - k\dfrac\{dx\}\{dt\} - \dfrac\{1\}\{2\}\left (\dfrac\{dx\}\{dt\}\right )^2C_DA
\end\{aligned\}$$ We can convert this differential equation into algebraic
form by writing it for a time step $i$ and substitute for the second
derivative. $$\begin\{aligned\}
m\left (\dfrac\{x_\{i-1\}-2x_i+x_\{i+1\}\}\{\Delta t^2\}\right) = F - k\left (\dfrac\{x_\{i+1\}-x_i\}\{\Delta t\}\right )-\frac\{1\}\{2\}\left (\dfrac\{x_\{i+1\}-x_i\}\{\Delta t\}\right)^2C_DA
\end\{aligned\}$$ For routinely encountered problems in engineering and
science, if we assume that the unknown nonlinear term is equal to its
value from the previous iteration, then the equation can be solved and
iterate until the process converges to an acceptable tolerance. Although
this approach will not work for all cases, it converges for many ODEs
derived from physically based systems.


# Runge-Kutta Method \{#chap:runge_kutta\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\},
[\[chap:ode\]](#chap:ode)\{reference-type="ref+label"
reference="chap:ode"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter we will learn how to understand truncation errors for
Runge-Kutta methods. We will then study the representation of second and
fourth order Runge-Kutta methods. We will end by solving the
Cahn-Hilliard model using Runge-Kutta's fourth-order method.

## Runge-Kutta Methods

In previous chapters, we have discussed Euler's method which is accurate
to the first two terms of a Taylor series expansion. By evaluating the
slope at multiple places (instead of just the initial value), a better
estimate can be obtained. Runge-Kutta methods use this methodology and
achieves the accuracy of a Taylor series approach without requiring the
calculation of higher derivatives. Many variations exist but all can be
cast in the generalized form of $$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + \phi h,
\end\{aligned\}$$ where $\phi$ is called an increment function, which can
be interpreted as a representative slope over the interval. The
increment function can be written in general form as

$$\begin\{aligned\}
\phi = a_\{1\}k_\{1\} + a_2k_2 + \dots + a_nk_n
\end\{aligned\}$$ where the $a$'s are constants and the $k$'s are
$$\begin\{aligned\}
    \begin\{split\}
        k_1 &= f(t_i,y_i) \\
        k_2 &= f(t_i + p_1h,y_i + q_\{11\}k_1h)\\
        k_3 &= f(t_i + p_2h,y_i + q_\{21\}k_1h+ q_\{22\}k_2h)\\ 
        \vdots&\\
        k_n &= f(t_i + p_\{n-1\}h,y_i + q_\{n-1,1\}k_1h + \dots + q_\{n-1,n-1\}k_\{n-1\}h)
        \vspace\{15pt\}
    \end\{split\}
\end\{aligned\}$$ where the $p$'s, $q$'s are constants and $f$ is the
derivative function $dy/dt$. Notice that the $k$'s are recurrence
relationships. That is, $k_1$ appears in the equation for $k_2$, which
appears in the equation for $k_3$, and so forth.

Various types of Runge-Kutta methods can be devised by employing
different numbers of terms in the increment function as specified by
$n$. It can be noted that the first-order RK method with $n$ = 1 is, in
fact, Euler's method. Once $n$ is chosen, values for the $a$'s, $p$'s,
and $q$'s are evaluated by setting the above equation equal to terms in
a Taylor series expansion. The second-order RK methods (uses an
increment function with two terms) and the fourth-order RK methods (uses
an increment function with four terms) will be discussed herewith. For
second-order methods, because terms with $h^3$ and higher are dropped
during the derivation, the local truncation error is O($h^3$) and the
global error is O($h^2$), Similarly, for fourth-order methods, the
global truncation error is O($h^4$).

## Second-Order Runge-Kutta Methods

The classical second-order Runge-Kutta method is simply obtained from
the Taylor series expansion of $y$. It is simplified as

$$\begin\{aligned\}
y_\{i+1\} &= y_i + f(t_i,y_i)h + \frac\{1\}\{2!\}f'(t_i,y_i)h^2 + O(h^3)  \\
y_\{i+1\} &= y_i + f(t_i,y_i)\frac\{h\}\{2\} + \left (\frac\{1\}\{2\}f(t_i,y_i) + \frac\{1\}\{2\}f'(t_i,y_i)h \right )h + O(h^3)\\
y_\{i+1\} &= y_i + \left (\frac\{1\}\{2\}f(t_i,y_i) + \frac\{1\}\{2\}f(t_i+h,y_i + f(t_i,y_i)h)\right )h + O(h^3)
\end\{aligned\}$$

::: marginfigure
Using Taylor series expansion, $$\begin\{aligned\}
    \begin\{split\}
f(t_i+h,y_i + f(t_i,y_i)h))& = f(t_i,y_i) + \frac\{\partial f\}\{\partial t\}h \\
& + \frac\{\partial f\}\{\partial y\}f(t_i,y_i)h+ O(h^2) \\
& = f(t_i,y_i) + \frac\{\partial f\}\{\partial y\}\frac\{\partial y\}\{\partial t\}h \\
& = f(t_i,y_i) + f'(t_i,y_i)h 
\end\{split\}
\end\{aligned\}$$
:::

This variant of the second-order Runge-kutta method is known as the
*Heun Method*. It can be re-written as

$$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + \left (\frac\{1\}\{2\}k_1 + \frac\{1\}\{2\}k_2\right )h
\end\{aligned\}$$ where $$\begin\{aligned\}
k_1 = f(t_i,y_i);\hspace\{15pt\}  k_2 = f(t_i + h,y_i + k_1h) 
\end\{aligned\}$$ However, other variants of these second order RK-methods
exist. The general second-order version of the RK equation
$y_\{i+1\} = y_\{i\} + \phi h$ is given as $$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + (a_1k_1 + a_2k_2)h
\end\{aligned\}$$ where

$$\begin\{aligned\}
k_1 = f(t_i,y_i) ; \hspace\{15pt\} k_2 = f(t_i + p_1h,y_i + q_\{11\}k_1h)       
\end\{aligned\}$$

The values of $a$'s, $p$ and $q$ are obtained by setting the above
equation equal to a second-order Taylor series. The first three terms of
the Taylor series expansion of y is written as $$\begin\{aligned\}
y_\{i+1\} = y_i + f(t_i,y_i)h + \frac\{1\}\{2!\}f'(t_i,y_i)h^2 + O(h^3)   
\end\{aligned\}$$

It can be noted that $dy/dt = f(t,y)$. Also, $$\begin\{aligned\}
f'(t_i,y_i) =\frac\{\partial f\}\{\partial t\} + \frac\{\partial f\}\{\partial y\}\frac\{\partial y\}\{\partial t\}
\end\{aligned\}$$

Substituting this back into the Taylor series expansion, we get
$$\begin\{aligned\}
y_\{i+1\} = y_i + f(t_i,y_i)h + \frac\{1\}\{2\}\left (\frac\{\partial f\}\{\partial t\} + \frac\{\partial f\}\{\partial y\}\frac\{\partial y\}\{\partial t\}\right )_\{|_\{(t_i,y_i)\}\}h^2 + O(h^3)
\end\{aligned\}$$

In a similar fashion, $k_2$ can be expanded as $$\begin\{aligned\}
k_2 = f(t_i + p_1h,y_i + q_\{11\}k_1h) = f(t_i,y_i) + p_1h\frac\{\partial f\}\{\partial t\} +  q_\{11\}k_1h\frac\{\partial f\}\{\partial y\} + O(h^2)   
\end\{aligned\}$$

Now, $y_\{i+1\} = y_\{i\} + (a_1k_1 + a_2k_2)h$ is simplified as
$$\begin\{aligned\}
\begin\{split\}
y_\{i+1\} & = y_i + \{a_1f(t_i,y_i) +  a_2(f(t_i,y_i) + p_1h\frac\{\partial f\}\{\partial t\} +  q_\{11\}k_1h\frac\{\partial f\}\{\partial y\} + O(h^2))\}h \\
 & = y_i + (a_1+a_2)hf(t_i,y_i) + a_2p_1h^2\frac\{\partial f\}\{\partial t\} + a_2q_\{11\}f(t_i,y_i)h^2\frac\{\partial f\}\{\partial y\}+ O(h^3)
\end\{split\}
\end\{aligned\}$$

On comparing the above equation with (1), three equations can be derived
to evaluate the four unknown constants. $$\begin\{aligned\}
a_1 + a_2 = 1 ; \hspace\{10pt\}a_2p_1 = 1/2; \hspace\{10pt\} a_2q_\{11\} = 1/2
\end\{aligned\}$$

These equations are *underdetermined*, since we have 3 equations with 4
unknowns. Therefore, a value is assumed for one of the unknowns to
determine the other three. Suppose that we specify a value for $a_2$,
then the equations can be solved as

$$\begin\{aligned\}
        a_1 = 1 - a2; \hspace\{10pt\} p_1 = q_\{11\} = \dfrac\{1\}\{2a_2\}
\end\{aligned\}$$

Because we can choose an infinite number of values for $a_2$, there are
an infinite number of second-order RK methods. Every version would yield
exactly the same results if the solution to the ODE were quadratic,
linear, or a constant. However, they yield different results when (as is
typically the case) the solution is more complicated. Two of the most
commonly used and preferred versions apart from the Heun's method are
discussed here.

### The Midpoint Method ($a_2 = 1$)

In this method, $a_2$ is assumed to be 1 and can be solved for $a_1$ = 0
and $p_1$ = $q_\{11\}$ = 1/2 to obtain $$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + k_2h
\end\{aligned\}$$ where $$\begin\{aligned\}
k_1 = f(t_i,y_i); \hspace\{15pt\}k_2 = f(t_i + \frac\{1\}\{2\}h,y_i + \frac\{1\}\{2\}k_1h)    
\end\{aligned\}$$

### Ralston's Method ($a_2 = 2/3$)

Ralston and Rabinowitz (1978) determined that choosing $a_2$ = 2/3
provides a minimum bound on the truncation error for the second-order RK
algorithms. In this method, $a_2$ is assumed to be 2/3 and can be solved
for $a_1$ = 1/3 and $p_1$ = $q_\{11\}$ = 3/4 to obtain $$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + (\frac\{1\}\{3\}k_1 + \frac\{2\}\{3\}k_2)h
\end\{aligned\}$$ where $$\begin\{aligned\}
k_1 = f(t_i,y_i);\hspace\{15pt\}  k_2 = f(t_i + \frac\{3\}\{4\}h,y_i + \frac\{3\}\{4\}k_1h)   
\end\{aligned\}$$

## 3-8th Rule Fourth-Order Runge-Kutta Method

The most popular RK methods are fourth order. As with the second-order
approaches, there are an infinite number of versions. These can be
solved for by using a similar approach discussed in the second-order
methods. The following variant is one of the most commonly used form.
The 3/8th rule fourth-order RK method is similar to Simpson's 3/8 rule
in case of ODEs that are a function of *t* alone. It is given as

$$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + \frac\{1\}\{8\}(k_1 + 3k_2 + 3k_3 + k_4)h
\end\{aligned\}$$ where $$\begin\{aligned\}
\begin\{split\}
k_1 &= f(t_i,y_i) \\
k_2 &= f(t_i + \frac\{1\}\{3\}h,y_i + \frac\{1\}\{3\}k_1h)\\
k_3 &= f(t_i + \frac\{2\}\{3\}h,y_i - \frac\{1\}\{3\}k_1h + k_2h)\\ 
k_4 &= f(t_i + h,y_i + k_1h - k_2h + k_3h)
\end\{split\}
\end\{aligned\}$$

::: marginfigure
Classical Fourth order Runge-Kutta method:
$$y_\{i+1\} = y_\{i\} + \frac\{1\}\{6\}(k_1 + 2k_2 + 2k_3 + k_4)h
    \vspace\{-6pt\}$$ where $$\begin\{aligned\}
        \begin\{split\}
            k_1 &= f(t_i,y_i) \\
            k_2 &= f(t_i + \frac\{1\}\{2\}h,y_i + \frac\{1\}\{2\}k_1h)\\
            k_3 &= f(t_i + \frac\{1\}\{2\}h,y_i + \frac\{1\}\{2\}k_2h)\\    
            k_4 &= f(t_i + h,y_i + k_3h)
        \end\{split\}
    
\end\{aligned\}$$
:::

It can also be noted that a variant of third-order Runge-Kutta method is
also similar to the Simpson's 1/3 rule. In addition, the fourth-order RK
method is similar to the Heun approach in that previous estimates of the
slope are considered to come up with an improved average slope for the
interval. It can also be observed that each of these $k$'s represent a
slope and a weighted average of these is considered to arrive at the
final improved slope. Another most commonly used variant, is the
*classical fourth-order method*.

## Solving Cahn-Hilliard with the Runge-Kutta Method

Let's consider the Cahn-Hilliard Equation which is an example of a
non-linear differential equation describing the process of phase
separation, by which the two components of a binary fluid spontaneously
separate and form domains which are pure in each component. The
Cahn-Hilliard Equation is given as $$\begin\{aligned\}
\dfrac\{\partial c\}\{\partial t\} = D \nabla^2 (c^3 - c - \gamma \nabla^2c )
\end\{aligned\}$$ Let's solve the Cahn-Hilliard equation given above using
the 3/8th rule fourth order Runge-Kutta's method from $t=0$ to $t=5$
with initial conditions as randomized values of -1 and 1 scattered over
a grid. We use a time step $(h)$ of $0.002$ s, a diffusion coefficient
of $20$ and gamma of $0.5$.

``` \{.python language="python"\}
def $\frac\{\partial c\}\{\partial t\}$(c, dx, dy, D, gamma):
  return D*$\nabla^2$(c$^3$-c-$\gamma$*$\nabla^2$c, dx, dy)
```

Note that above, by specifying `dx,dy`, we are constraining the
Laplacian $\nabla^2$ to use a numeric approximation rather than the
analytical calculation. In particular, we use a central difference to
approximate the second-order derivative at a grid element. We start by
initializing a grid.

``` \{.python language="python"\}
def initialize_grid(N: $\mathbb\{N\}$) $\to \mathbb\{R\}^\{N \times N\}$:
  C = zeros(N, N)
  for i j:
    temp = randi(2)
    if (temp==1):
      C[j,i] = -1
    else 
      C[j,i] = 1
C: $\mathbb\{R\}^\{N, N\}$ = initialize_grid(N)
```

And then we can define a general Runge-Kutta operation

``` \{.python language="python"\}
def runge_kutta($\gamma$ : $\mathbb\{R\}$, D: $\mathbb\{R\}$, dt: $\mathbb\{R\}$, $t_0$: $\mathbb\{R\}$, $t_e$: $\mathbb\{R\}$):
  t = $t_0$
  while t < $t_e$:
    k1 = $\frac\{\partial c\}\{\partial t\}$(C, dx, dy, D, $\gamma$)  
    $C_2$ =  C + dt/3*k1
    k2 = $\frac\{\partial c\}\{\partial t\}$($C_2$, dx, dy, D, $\gamma$)
    $C_3$ =  C + 2*dt/3*k2
    k3 = $\frac\{\partial c\}\{\partial t\}$($C_3$, dx, dy, D, $\gamma$)
    $C_4$ =  C + dt*k3
    k4 = $\frac\{\partial c\}\{\partial t\}$($C_4$, dx, dy, D, $\gamma$)
    C = C + dt*(k1 + 3*k2 + 3*k3 + k4)/8
    C = bound_conc(C)
    t = t + dt
```

The function $\frac\{\partial c\}\{\partial t\}$ estimates the derivative at
the specified concentration. The function `bound\_conc`() ensures that
the concentration lies within \[-1,1\]. A slightly generalized version
of the equation above can be constructed that applies Runge-Kutta to any
differential equation system rather than the Cahn-Hilliard equation.


# Systems of Ordinary Differential Equations \{#chap:systems_ode\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter we will learn how to implement Euler's method and
Runge-Kutta's fourth order method for solving a general system of
ordinary differential equations. We study the specific examples of the
spring-mass damper system and the Lotka-Volterra system and solve these
systems with Euler's method and Runge-Kutta's fourth order method.

In this chapter, we will look at solving a system of ODEs using the
Euler's and Runge-Kutta methods. In the previous lectures, we have
solved a single ODE using either Euler or Runge-Kutta methods. However,
in practice, many problems in engineering and science require the
solution of a system of simultaneous ordinary differential equations.
Here, we will look at extending our knowledge on solving single ODEs to
this generalized system of ODEs.

## Systems of Equations

Many practical problems in engineering and science require the solution
of a system of simultaneous ordinary differential equations rather than
a single equation. Such systems may be represented generally as
$$\begin\{aligned\}
\frac\{\partial y_1\}\{\partial t\} = f_1(t,y_1,y_2,\dots,y_n)\\
\frac\{\partial y_2\}\{\partial t\} = f_2(t,y_1,y_2,\dots,y_n)\\
\vdots \\
\frac\{\partial y_n\}\{\partial t\} = f_n(t,y_1,y_2,\dots,y_n)
\end\{aligned\}$$

::: marginfigure
![image](figures/part1b/ode_systems/rk4_0.png)\{width="2.5in"\}
:::

The solution of such a system requires that $n$ initial conditions be
known at the starting value of $t$. All the methods discussed earlier
for single equations can be extended to systems of ODEs. Engineering
applications can involve thousands of simultaneous equations. In each
case, the procedure for solving a system of equations simply involves
applying the one-step technique for every equation at each step before
proceeding to the next step.

Note that any of the higher-order RK methods discussed earlier can also
be applied to systems of equations. However, care must be taken in
determining the slopes. These slopes can be observed for the classical
fourth order method from the figure. That is, we first develop slopes
for all variables at the initial value. These slopes (a set of $k1$'s)
are then used to make predictions of the dependent variable at the
midpoint of the interval. These midpoint values are in turn used to
compute a set of slopes at the midpoint (the $k2$'s). These new slopes
are then taken back to the starting point to make another set of
midpoint predictions that lead to new slope predictions at the midpoint
(the k3's). These are then employed to make predictions at the end of
the interval that are used to develop slopes at the end of the interval
(the $k4$'s). Finally, the $k$'s are combined into a set of increment
functions that are brought back to the beginning to make the final
predictions. The following example illustrates the approaches of Euler
and Runge-Kutta methods.

## A Mass-Damped Spring System

A spring-mass damper system's amplitude is given by the following second
order differential equation

$$m\frac\{d^2x\}\{dt^2\} + c\frac\{dx\}\{dt\} + kx = F(t)$$

The above equation can be written as two first order ODEs by using two
state variables namely $$\begin\{aligned\}
x_1 &= x\\ 
x_2 &= \frac\{dx\}\{dt\}
\end\{aligned\}$$ By doing so, the second order ODE can be re-written as
$$\begin\{aligned\}
    \begin\{split\}
        \frac\{dx_1\}\{dt\} &= x_2\\
        \frac\{dx_2\}\{dt\} &=  -\frac\{k\}\{m\}x_1 - -\frac\{c\}\{m\}x_2 + \frac\{F\}\{m\}
    \end\{split\}
\end\{aligned\}$$

Now, the problem is reduced to solving a system of first order ODEs.
Let's solve the above dynamical system to determine the state-variables
at every time-step first using Euler's method

::: marginfigure
![image](figures/part1b/ode_systems/Spring mass damper.png)\{width="2.75in"\}
:::

``` \{.python language="python"\}
def mass_damped_system(m : $\mathbb\{R\}$, k : $\mathbb\{R\}$, c : $\mathbb\{R\}$, F: $\mathbb\{R\}$, $x_i: \mathbb\{R\}$, $dx_i: \mathbb\{R\}$, 
                       $(t_i, t_f)$: $\mathbb\{R\}^2$, dt: $\mathbb\{R\}$)
  N = (t$_f$ - t$_i$)/dt
  x = zeros(N+1), dx = zeros(N+1)
  x[0] = $x_i$, dx[0] = $dx_i$
  for i = 1:N+1
    x[i] = x[i-1] + dx[i-1]*dt
    dx[i] = dx[i-1] + (-k/m*x[i-1] - c/m*dx[i-1] + F/m)*dt
  x, dx

```

Now let's try again using Runge-Kutta. The implementation is only a
little more complicated

``` \{.python language="python"\}
def mass_damped_system(m : $\mathbb\{R\}$, k : $\mathbb\{R\}$, c : $\mathbb\{R\}$, F : $\mathbb\{R\}$, $x_i$, $dx_i$, 
                       $(t_i, tf)$: $\mathbb\{R\}^2$, dt: $\mathbb\{R\}$)
  N = (t$_f$ - t$_i$)/dt
  x = zeros(N+1), dx = zeros(N+1)
  x[0] = $x_i$, dx[0] = $dx_i$
  for i = 1:N+1
    k11 = dx[i-1]
    k12 = -k/m*x[i-1] - c/m*dx[i-1] + F/m
    x_mid =  x[i-1] + dt/2*(k11)
    dx_mid =  dx[i-1] + dt/2*(k12)
    k21 = dx_mid
    k22 = -k/m*x_mid  - c/m*dx_mid + F/m
    x_mid =  x[i-1] + dt/2*(k21)
    dx_mid =  dx[i-1] + dt/2*(k22)
    k31 = dx_mid
    k32 = -k/m*x_mid  - c/m*dx_mid + F/m
    x_mid =  x[i-1] + dt*(k31)
    dx_mid =  dx[i-1] + dt*(k32)
    k41 = dx_mid
    k42 = -k/m*x_mid - c/m*dx_mid + F/m
    x[i] = x[i-1] + dt*(k11 + 2*k21 + 2*k31 + k41)/6
    dx[i] = dx[i-1] + dt*(k12 + 2*k22 + 2*k32 + k42)/6
  x, dx
```

## The Lotka-Volterra Equations

The *Lotka--Volterra* equations, also known as the predator--prey
equations, are a pair of first-order, nonlinear, differential equations
frequently used to describe the dynamics of biological systems in which
two species interact, one as a predator and the other as prey. The
populations change through time according to the pair of equations:
$$\begin\{aligned\}
\frac\{dx\}\{dt\} &= ax-bxy\\
\frac\{dy\}\{dt\} &= -cy+dxy
\end\{aligned\}$$ where $x$ and $y$ are the number of prey and predators,
respectively, $a$ is the prey growth rate, $c$ is the predator death
rate, and $b$ and $d$ are the rates characterizing the effect of the
predator-prey interactions on the prey death and the predator growth,
respectively. The multiplicative terms (i.e., those involving xy) are
what make such equations nonlinear.

Let's solve the above system of equations using Euler's and Runge-Kutta
methods. First let's look at the implementation with Euler's equation

``` \{.python language="python"\}
def lotka_volterra(a: $\mathbb\{R\}$, b: $\mathbb\{R\}$, c: $\mathbb\{R\}$, d: $\mathbb\{R\}$,
                   x$_i$: $\mathbb\{R\}$, y$_i$ : $\mathbb\{R\}$, $(t_i, t_f): \mathbb\{R\}^2$, dt: $\mathbb\{R\}$):
  N = (t$_f$-t$_i$)/dt
  x = zeros(N+1), y = zeros(N+1)
  x[0] = x$_i$, y[0] = y$_i$
  for i = 1:n+1:
    x[i] = x[i-1] + (a*x[i-1] - b*x[i-1]*y[i-1])*dt
    y[i] = y[i-1] + (-c*y[i-1] + d*x[i-1]*y[i-1])*dt
  x, y
```

Now let's try again using Runge-Kutta

``` \{.python language="python"\}
def lotka_volterra(a: $\mathbb\{R\}$, b: $\mathbb\{R\}$, c: $\mathbb\{R\}$, d: $\mathbb\{R\}$,
                   x$_i$: $\mathbb\{R\}$, y$_i$: $\mathbb\{R\}$, $(t_i, t_f): \mathbb\{R\}^2$, dt: $\mathbb\{R\}$): 
  N = (t$_f$-t$_i$)/dt
  x = zeros(N+1), y = zeros(N+1)
  x[0] = x$_i$, y[0] = y$_i$
  for i = 1:n+1:
    k1x = a*x[i-1] - b*x[i-1]*y[i-1]
    k1y = -c*y[i-1] + d*x[i-1]*y[i-1]
    x_13 =  x[i-1] + dt/3*(k1x)
    y_13 =  y[i-1] + dt/3*(k1y)
    k2x = a*x_13 - b*x_13*y_13
    k2y = -c*y_13 + d*x_13*y_13
    x_23 =  x[i-1] - 1*dt/3*(k1x) + dt*(k2x)
    y_23 =  y[i-1] - 1*dt/3*(k1y) + dt*(k2y)
    k3x = a*x_23 - b*x_23*y_23
    k3y = -c*y_23 + d*x_23*y_23
    x_h =  x[i-1] + dt*(k1x-k2x+k3x)
    y_h =  y[i-1] + dt*(k1y-k2y+k3y)
    k4x = a*x_h - b*x_h*y_h
    k4y = -c*y_h + d*x_h*y_h
    x[i] = x[i-1] + dt*(k1x + 3*k2x + 3*k3x + k4x)/8
    y[i] = y[i-1] + dt*(k1y + 3*k2y + 3*k3y + k4y)/8
  x, y
```

::: marginfigure
![image](figures/part1b/ode_systems/predator_prey.png)\{width="2.75in"\}
:::

The results obtained from the Euler's and Runge-kutta methods are shown.
The time series of Euler's method indicates that the amplitudes of the
oscillations are expanding. These results indicate that the crude Euler
method would require a much smaller time step to obtain accurate
results.

In contrast, because of its much smaller truncation error, the fourth
order Runge-Kutta method yields good results with the same time step.
The same time series of Runge-Kutta's method portrays a cyclical pattern
emerging in time. Because the predator population is initially small,
the prey grows exponentially. At a certain point, the prey become so
numerous that the predator population begins to grow. Eventually, the
increased predators cause the prey to decline. This decrease, in turn,
leads to a decrease of the predators. Eventually, the process repeats.
Notice that, as expected, the predator peak lags the prey. Also, observe
that the process has a fixed period - that is, it repeats in a set time.
The phase-plane representation for the accurate RK4 solution indicates
that the interaction between the predator and the prey amounts to a
closed counterclockwise orbit.


# Euler's Method \{#chap:ode_euler\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:linear_systems\]](#chap:linear_systems)\{reference-type="ref+label"
reference="chap:linear_systems"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter, we learn how to implement the Euler's method for
solving a single ordinary differential equation. We learn about the
meanings of local and global truncation errors and their relationships
to step sizes for one-step methods for solving ODEs. We then solve the
Cahn-Hilliard model using Euler's method and end by explaining the use
of adaptive step-sizing for solving ODEs.

## First Order ODEs

First-order ODEs form the basis for solving governing equations of
engineering systems. Let us first consider different approaches for
solving first-order ODEs. One-step methods and multi-step methods are
two widely used methods for solving these first order ODEs.

*One-Step methods* attempt to trace out the trajectory of the solution
into the future by extrapolating from an old value $y_i$ to a new value
$y_\{i+1\}$ over a distance $h$, given the value of the increment function
(first-derivative of the function) at a single point $i$. They are also
referred to as Runge-Kutta methods after the two applied mathematicians
who first discussed them in the early 1900s. *Multi-Step methods* use
information from several previous points as the basis for extrapolating
to a new value. We focus on studying solution methods for first order
ODEs since higher order ODEs can be reduced to a system of first order
ODEs.

Runge-Kutta methods are a family of iterative methods, usually employed
for solving first order ODEs. Among these methods, the simplest approach
to solve a first order ODE is to use the differential equation to
estimate the slope in the form of the first derivative at $t_i$. In
other words, the slope at the beginning of the interval is taken as an
approximation of the average slope over the whole interval. This
approach is called the *Euler's method*. It is the most basic method for
numerical integration of ordinary differential equations and is the
simplest Runge--Kutta method.

## Euler's Method \{#eulers-method\}

Let's look at the first order ODEs of the general form
$\frac\{dy\}\{dt\} = f(t,y)$. The first derivative of the function provides
a direct estimate of the slope at the initial value $(t_i,y_i)$. This is
given as $$\begin\{aligned\}
\phi = \frac\{dy\}\{dt\} = f(t_i,y_i),
\end\{aligned\}$$ where $f(t_i,y_i)$ is the differential equation
evaluated at $t_i$ and $y_i$. The estimate from Euler's method can be
obtained by moving over by a step size along this slope estimate from
the initial value. The estimate at the next step is given as
$$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + f(t_i,y_i)h
\end\{aligned\}$$

::: marginfigure
![image](figures/part1b/euler_method/Euler_0.png)\{width="2.5in"\}
:::

This formula is referred to as *Euler's method* (or the Euler-Cauchy or
point-slope method). A new value of y is predicted using the slope
(equal to the first derivative at the original value of t) to
extrapolate linearly over the step size $h$. It can be observed that the
Euler's method primarily draws its motivation from the Taylor's series
by utilizing the first two-terms of the function's approximation about
the initial value.

## The Cahn-Hilliard Equation

Let's consider the Cahn-Hilliard Equation which is an example of a
non-linear differential equation describing the process of phase
separation, by which the two components of a binary fluid spontaneously
separate and form domains which are pure in each component. The
Cahn-Hilliard Equation is given as $$\begin\{aligned\}
\dfrac\{\partial c\}\{\partial t\} = D \nabla^2 (c^3 - c - \gamma \nabla^2c )
\vspace\{-6pt\}
\end\{aligned\}$$ Let's try to solve the Cahn-Hilliard equation given
above using the Euler's method from $t=0$ to $t=3$ with initial
conditions as the normalized intensity values of an image. Suppose time
step $dt$ of $0.002$ s, a diffusion coefficient $D$ of $20$ and $\gamma$
of $0.5$.

The Cahn-Hilliard equation can be re-written as $$\begin\{aligned\}
\dfrac\{\partial c\}\{\partial t\} = D (\nabla^2c^3 - \nabla^2 c - \nabla^2 \gamma \nabla^2 c)
\end\{aligned\}$$

We assume for modeling purposes that we can model $c$ by a $N\times N$
sized grid.

``` \{.python language="python"\}
def cahn_hilliard_eulers(C : $\mathbb\{R\}^\{N\times N\}$, $\gamma: \mathbb\{R\}$, D : $\mathbb\{R\}$, dt: $\mathbb\{R\}$, ($t_0$, $t_e$): $\mathbb\{R\}^2$, tol: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}^\{N\times N\}$:
  out = zeros(N, N)
  $t$ = $t_0$ 
  while $t$ < $t_e$:
    dc$^2$ = $\nabla^2$(C, dx) + $\nabla^2$(C, dy)
    dc$^3$ = $\nabla^2$(C$^3$,dx) + $\nabla^2$(C$^3$,dy)
    $\frac\{\textrm\{d\}\gamma\}\{\textrm\{dc\}\}$ = $\nabla^2$(gamma*dc$^2$, dx) + $\nabla^2$(gamma*dc$^2$, dy)
    $\frac\{\textrm\{dc\}\}\{\textrm\{dt\}\}$ = D*(dc$^3$ - dc$^2$ - $\frac\{\textrm\{d\}\gamma\}\{\textrm\{dc\}\}$)
    C = C + $\frac\{\textrm\{d\}\gamma\}\{\textrm\{dc\}\}$*dt
    if $|\|\textrm\{out\}\|-\|C\|)|$ < tol:
      break
    out = C
    time = time + dt
  return out
```

We use the notation $\nabla^2$ above, but in practice we need to use the
central difference to approximate the second-order derivative at a grid
element.

## Error Analysis for Euler's Method

The numerical solution of ODEs involves two types of error

1.  *Truncation* or Discretization Errors - These are caused by the
    nature of the techniques employed to approximate values of y.

2.  *Roundoff* Errors - These are caused by the limited numbers of
    significant digits that can be retained by a computer.

The truncation errors are composed of two parts. The first is a *local
truncation error* that results from an application of the method in
question over a single step. The second is a *propagated truncation
error* that results from the approximations produced during the previous
steps. The sum of the two is the total error. It is referred to as the
*global truncation error*.

Insight into the magnitude and properties of the truncation error can be
gained by deriving Euler's method directly from the Taylor series
expansion. To do this, realize that the differential equation being
integrated will be of the general form of $\frac\{dy\}\{dt\} = f(t,y)$ with
an initial value of ($t_i,y_i$). If the solution, i.e. the function
describing the behavior of y has continuous derivatives, it can be
represented by a Taylor series expansion about a starting value
($t_i ,y_i$), as in $$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + y_\{i\}'h + \frac\{y_i''\}\{2!\}h^2 + \dots + \frac\{y_i^\{n\}\}\{n!\}h^n + R_n
\end\{aligned\}$$

where $h = t_\{i+1\} - t_\{i\}$ and $R_n =$ the remainder term defined as
$$\begin\{aligned\}
R_n = \frac\{y^\{n+1\}(\xi)\}\{(n+1)!\}h^\{n+1\}
\end\{aligned\}$$ where $\xi$ lies somewhere in the interval from $t_i$ to
$t_\{i+1\}$. The above Taylor Series expansion can also be re-written as
$$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + f(t_i,y_i)h + \frac\{f'(t_i,y_i)\}\{2!\}h^2 + \dots + \frac\{f^\{(n-1)\}(t_i,y_i)\}\{n!\}h^n + O(h^\{n+1\})
\end\{aligned\}$$

where $O(h^\{n+1\})$ specifies that the local truncation error is
proportional to the step size raised to the $(n + 1)$th power. From the
above equation, it can be seen that the Euler's method covers only the
first two terms of the Taylor's expansion. Thus, the truncation error in
Euler's method is attributable to the remaining terms in the Taylor
series expansion that were not included and is given as
$$\begin\{aligned\}
E_t = \frac\{f'(t_i,y_i)\}\{2!\}h^2 + \dots + \frac\{f^\{(n-1)\}(t_i,y_i)\}\{n!\}h^n + O(h^\{n+1\})
\end\{aligned\}$$

where $E_t$ is the *true local truncation error*. For sufficiently small
$h$, the higher-order terms are usually negligible, and the result is
often represented as $$\begin\{aligned\}
E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2 = O(h^2)
\end\{aligned\}$$

where $E_a$ is the *approximate local truncation error*.

It can also be shown that the global truncation error is $O(h)$, i.e. it
is proportional to the step size. These observations lead to some useful
conclusions:

1.  The global error can be reduced by decreasing the step size.

2.  The method will provide error-free predictions if the underlying
    function is linear, because for a straight line the second
    derivative would be zero.

This latter conclusion makes intuitive sense because Euler's method uses
straight-line segments to approximate the solution. Hence, Euler's
method is referred to as a *first-order method*.

## Stability of Euler's Method

The stability of a solution method is another important consideration
that must be considered when solving ODEs. A numerical solution is said
to be unstable, if errors grow exponentially for a problem for which
there is a bounded solution. The stability of a particular application
can depend on three factors: the differential equation, the numerical
method, and the step size.

Insight into the step size required for stability can be examined by
studying the following simple ODE: $$\begin\{aligned\}
\frac\{dy\}\{dt\} = -ay
\end\{aligned\}$$

If $y(0)=y_0$, the true solution can be determined using calculus as

$$\begin\{aligned\}
y = y_0e^\{-at\}
\end\{aligned\}$$

The above solution starts at $y_0$ and asymptotically approaches zero.
Solving the same problem using Euler's method gives the following
result,

$$\begin\{aligned\}
y_\{i+1\} &= y_i + \frac\{dy_i\}\{dt\}\\
h &= y_i - ay_ih = y_i(1-ah)
\end\{aligned\}$$

The parenthetical quantity $1-ah$ is called an *amplification factor*.
If its absolute value is greater than unity, the solution will grow in
an unbounded fashion. So clearly, the stability depends on the step size
*h*. Based on this analysis, Euler's method is said to be *conditionally
stable*.

Note that there are certain ODEs where errors always grow regardless of
the method. Such ODEs are called *ill-conditioned*. Inaccuracy and
instability are often confused. This is usually because both represent
situations where the numerical solution breaks down or both are affected
by step size.

## Adaptive Step-Sizing

We have seen how to solve ODEs numerically using Euler's method using a
fixed step size. However, it is noticed that the local trunctation error
is proportional to the step-size chosen. By changing this step-size
dynamically at every step, an upper bound on the local truncation error
can be imposed with minimum number of steps. *Adaptive step-sizing*
methods make use of this to come up with a new step-size at every step.

The local truncation error for Euler's method is given by
$E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2$. The step-size is simply obtained by
establishing an upper bound on this truncation error ($e_\{max\}$).
$$\begin\{aligned\}
    E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2 <= e_\{max\} \implies h <= \sqrt\{\dfrac\{2e_\{max\}\}\{f'(t_i,y_i)\}\}
    \vspace\{-6pt\}
    
\end\{aligned\}$$

This condition is used for limiting the local truncation error while
achieving greater performance on segments where the derivative of $f$ is
small.

As an example, let's try to estimate $y$ from the ODE $$\begin\{aligned\}
\frac\{dy\}\{dt\} = t-\log(1+t)
\end\{aligned\}$$ using adaptive step-sizing and Euler's method.

::: marginfigure
![image](figures/part1b/euler_method/Euler_0.png)\{width="2.5in"\}
:::

``` \{.python language="python"\}
def adaptive_euler(f: $\mathbb\{R\} \to \mathbb\{R\}$, $(t_0, t_e)$: $\mathbb\{R\}^2$, tol: $\mathbb\{R\}$) $\to \mathbb\{R\}[]$:
  t_adp, y, dt: $\mathbb\{R\}[]$
  t_adp[0] = $t_0$, i = 0
  y[0] = 0
  dt[0] = sqrt(2*tol/$\nabla$f(t_adp[0]))
  while (t_adp[i] < $t_e$):
    y[i+1] = y[i] + f(t_adp[i])*dt[i]
    dt[i+1] = sqrt(2*tol/$\nabla$f(t_adp[i]))
    t_adp[i+1] = t_adp[i] + dt[i+1]
    i = i + 1
  y
```

It can be observed that initially the step-size was high, which
gradually decreases because of the increasing value of $f'$.
