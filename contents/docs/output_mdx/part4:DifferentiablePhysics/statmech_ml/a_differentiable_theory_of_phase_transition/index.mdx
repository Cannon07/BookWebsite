# A Differentiable Physics Theory of Phase Transitions

Modeling phenomena involving phase transitions is tricky for today's
deep learning systems due to a few points. Phase transitions involve
non-analytic or discontinuous behavior which can be difficult to model
appropriately in a deep network. They also involve subtle effects with
potentially arbitrary large correlation scales. Deep architecture for
molecular or materials property prediction typically use length cutoffs
which can make the modeling of long range phenomena potentially
challenging.

In this note, I suggest a first step towards constructing a
differentiable model of phase transitions. The model is based on the
Landau theory of phase transitions and uses the formalism of the Landau
functional.

Suppose that a system undergoes a phase transition at a point $T_c$. At
this transition point, some structured symmetry or order of the system
is broken. This is formalized by order parameter $\eta$. For simple
systems, we usually specify that $$\begin\{aligned\}
\eta(T) &= \begin\{cases\}
\geq 0 & T > T_c \\
= 0 & T \leq T_c 
\end\{cases\}
\end\{aligned\}$$

More generally, $\eta$ could be a field $\eta(x, T)$ which gives the
local order parameter for each point in space rather than a global
measurement of order. $\eta$ can also be a complex quantity or tensor
quantity for more complex systems.

For example, the order parameter $\eta$ could be the density of system
near the boiling point. Above this temperature, the density falls to
nearly $0$ with a sharp discontinuity from the earlier nonzero density.
Recall that the partition function $Z$ of a thermodynamic system is
given by $$\begin\{aligned\}
Z &= \sum_\{\mu\} e^\{-H[\mu]/k_B T\}
\end\{aligned\}$$ Where the sum $\mu$ is over the microstates of the
system. As the sum goes to infinity in the thermodynamic limit $Z$ can
gain discontinuities. The Helmholtz free energy is given by
$$\begin\{aligned\}
F &= -k_B T \ln Z \\
&= -k_B T \ln \left [ \sum_\{\mu\} e^\{-H[\mu]/k_B T\}\right ]
\end\{aligned\}$$ Since $Z$ may contain discontinuities, $F$ may as well.
The core insight of Landau theory is to make the simplifying assumption
that the free energy can be rewritten as a function of the order
parameter $\eta$ and the temperature $T$ $$\begin\{aligned\}
F &= F(T, \eta)
\end\{aligned\}$$ Typically, $\eta(T)$ is a scalar value in simple cases
and is often written down. For example, $\eta(T)$ would be the density
for a liquid-vapor transition system. Assuming for simplicity that
$\eta(T)$ is a scalar constant and not a field, we can expand the Taylor
series and write $$\begin\{aligned\}
F &= a_0 + a_1(T) \eta(T) + a_2(T) \eta^2(T) + \dotsc 
\end\{aligned\}$$ If $\eta(x, T)$ is a field, then we write instead
$$\begin\{aligned\}
F(x, T) &= \int_x \left [ a_0 + a_1(T) \eta(x, T) + a_2(T) \eta^2(x, T) + \dotsc \right ]
\end\{aligned\}$$

Landau's second core insight is to note that the form of $F$ must be
constrained by symmetries that the system obeys. For example, if the
system is an even function of the order parameter (true for the Ising
model), the sum would only be given by even powers $$\begin\{aligned\}
F &= a_0 + a_2(T) \eta^2(T) + a^4(T) \eta^4(T) + \dotsc
\end\{aligned\}$$ In general, we can require that $F$ must be invariant
(as a scalar quantity) or equivariant (as a field) to a given group $H$.
For example, for the Heisenberg model of a magnet, $H$ would be $O(3)$.
A common way to enforce this constraint is to require that $\eta(T)$
must be invariant to group $H$. For example for $O(3)$, specifying that
$\eta(T) \in \mathbb\{R\}^3$ and that $F$ is a function of powers of
$\eta(T)^T \eta(T)$ would make it rotationally invariant.

To solve this equation, we in general seek to minimize the Landau
functional $F$ with respect to the order parameter $\eta$
$$\begin\{aligned\}
\frac\{d F\}\{d \eta\} &= 0
\end\{aligned\}$$

By making simplifying assumptions on the forms of the $a_i(T)$ it is
possible for many simple systems to solve for $\eta(T)$. With a closed
for of $\eta(T)$, a closed form for $F$ can be derived from the defining
Taylor series. From this form for $F$, various thermodynamic properties
can subsequently be derived.

In order to convert this recipe into a differentiable program, we must
be able to determine $\eta(T)$ in an automatic or learnable fashion. Our
recipe must also not depend on the specifics of a particular toy system
but must extend to a broad range of systems in order to allow for enough
flexiblity to learn from experimental data. Suppose that $m$ is a
description of the underlying system for which we seek to derive the
free energy. For example $m$ could be a molecular structure, material
unit cell, a field or possibly a mixture of molecules. $\eta$ by
necessity will depend on the choice of $m$.

$$\begin\{aligned\}
\eta = \eta(m, T)
\end\{aligned\}$$

We take the ansatz that $\eta$ can be computed by a suitable
differentiable program. For example, in the case $m$ is a molecule, we
could write $$\begin\{aligned\}
\eta &= \textrm\{GNN\}_\{\theta\}(m, T)
\end\{aligned\}$$

This simple parameterization has the downside that it may not be able to
directly represent non-analytic or discontinuous behavior at $T_c$. To
fix this problem, we may choose to make this parameterization depend
directly on $T_c$. For example $$\begin\{aligned\}
\eta &= \textrm\{GNN\}_\{\theta_1\}(m, T)\sigma(T \leq T_c) + \textrm\{GNN\}_\{\theta_2\}(m, T)\sigma(T > T_c)
\end\{aligned\}$$ Here we use the notation $\sigma(T \leq T_c)$ to denote
a smooth approximation to a heaviside step function that is $1$ only if
$T \leq T_c$. More generally, we note though that $\eta$ may have a
known structure from a universality class, so we will want to constrain
the output of $eta$ to follow this known universality relationship. For
example, we may known that the system in equation has multiple critical
points $T_c^1,T_c^2,\dotsc$ or that the output must obey a power law.

Note that in this framing, $T_c$ has become a learnable parameter which
can be learned by gradient descent and trained against known critical
temperatures. For functional $F$, we can simplify specify that $F$ is an
equivariant network which takes as input $\eta$ $$\begin\{aligned\}
F_\theta(m, T) &= \textrm\{Equiv\}_\{H,\theta'\}(\eta(m, T)) \\
&= \textrm\{Equiv\}_\{H,\theta'\}(\textrm\{GNN\}_\{\theta_1\}(m, T)\sigma(T \leq T_c) + \textrm\{GNN\}_\{\theta_2\}(m, T)\sigma(T > T_c))
\end\{aligned\}$$ For certain physical systems, we may already know a good
ansatz for the form of $F$ which we can directly encode as the
equivariant network in question. In general, different groups $H$ will
require different equivariant constructions to make sense.

The construction above provides a direct way to compute $F$ for any
given system $m$, but requires that we be able to learn values for
parameters $\theta', \theta_1, \theta_2$. Assume that we have data for
some thermodynamic property $P(m, T)$ and that we know that
$$\begin\{aligned\}
P(m, T) &= f(F_\theta(m, T))
\end\{aligned\}$$ where $f$ is some thermodynamic function. We can then
train the entire system directly from data. At the end of training, we
will have a learned form for $F_\theta(m, T)$. By construction,
$F_\theta$ will satisfy known universality and structural properties on
$\eta$ (including known discontinuity structure) and known invariances
for $H$. We can also add terms to the loss function for known
experimental values of $T_c$. The trained network can then predict $T_c$
for new materials/molecules.

I believe that this framework can also be extended to a renormalization
group analysis of phase transitions and in a different vein, to create a
differentiable Ginzberg-Landau functional for modeling
superconductivity, but that's left for future refinements. It also
remains to implement the concept and test it out on datasets like heat
of vaporization or fusion as Venkat suggested.

## High Dimensional Order Parameters

In the classical derivation above, we assume that the order parameter is
typically given by a 1 dimensional or more generally low dimensional
quantity. This choice is typically made for analytical convenience.
However, for a differentiable free energy functional, there is no strict
requirement that order parameters must be low dimensional. Suppose that
our order parameter is now a potentially high dimensional embedding
vector $$\begin\{aligned\}
\eta(T) \in \mathbb\{R\}^D
\end\{aligned\}$$ We can still represent the free energy functional as
$$\begin\{aligned\}
F &= F(T, \eta)
\end\{aligned\}$$

A direct Taylor expansion for a high dimensional vector can be written
as $$\begin\{aligned\}
F &= a_0 + a_1(T)^T \eta(T) + \frac\{1\}\{2\} a_2(T)^T (\eta(T) \otimes \eta(T)) a_2(T) + \dotsc
\end\{aligned\}$$ Here $\otimes$ denotes the outer product. Computing
terms in this expansion will rapidly become expensive due to the growth
in the number of interaction terms. We can potentially directly jumpt
then to the equivariant formulation we saw earlier

$$\begin\{aligned\}
F_\theta(m, T) &= \textrm\{Equiv\}_\{H,\theta'\}(\eta(m, T)) 
\end\{aligned\}$$

To make further progress, we need extra information about $\eta(T)$.
Physically, there are still 2 phases that we are separating, so we can
again condition on critical temperature $T_c$

$$\begin\{aligned\}
\eta(T) &= \textrm\{GNN\}_\{\theta_1\}(m, T)\sigma(T \leq T_c) + \textrm\{GNN\}_\{\theta_2\}(m, T)\sigma(T > T_c)
\end\{aligned\}$$ There are two differences from before. First the graph
neural networks now output empbedding vectors rather than scalars.
$$\begin\{aligned\}
\textrm\{GNN\}_\{\theta_i\}(m, T) \in \mathbb\{R\}^D
\end\{aligned\}$$ Next, we construct an ansatz that these two embedding
spaces are disjoint. Let $\mathcal\{M\}$ be the space of all possible
input system states. We assume the ansatz that the learned embedding for
the two phases must rest in different parts of phase space
$$\begin\{aligned\}
\textrm\{GNN\}_\{\theta_1\}(\mathcal\{M\}, T < T_c) \cap \textrm\{GNN\}_\{\theta_2\}(\mathcal\{M\}, T \geq T_c) &= \emptyset
\end\{aligned\}$$ We now write as before

$$\begin\{aligned\}
F_\theta(m, T) &= \textrm\{Equiv\}_\{H,\theta'\}(\eta(m, T)) \\
&= \textrm\{Equiv\}_\{H,\theta'\}(\textrm\{GNN\}_\{\theta_1\}(m, T)\sigma(T \leq T_c) + \textrm\{GNN\}_\{\theta_2\}(m, T)\sigma(T > T_c))
\end\{aligned\}$$ The non-overlap ansatz means we can factor the outer
network through the two disjoint states $$\begin\{aligned\}
F_\theta(m, T) &= \textrm\{Equiv\}_\{H,\theta'\}(\textrm\{GNN\}_\{\theta_1\}(m, T))\sigma(T \leq T_c)+ \textrm\{Equiv\}_\{H,\theta'\}(\textrm\{GNN\}_\{\theta_2\}(m, T))\sigma(T \leq T_c)
\end\{aligned\}$$

We can then simplify the expression by conjoining the networks

$$\begin\{aligned\}
F_\theta(m, T) &= \textrm\{EquivGNN\}_\{H,\theta'_1\}(m, T)\sigma(T \leq T_c)+ \textrm\{EquivGNN\}_\{H,\theta'_2\}(m, T)\sigma(T \leq T_c)
\end\{aligned\}$$ This final form is very simple and simply outputs the
free energy as a switch between two equivariant graph neural networks.
This conjoining trick also provides us a method to avoid having to
enforce the disjointness ansatz explicitly since it is implicitly
encoded by the smooth Heaviside functions $\sigma$.

We can view this network as learning the free energy of the two phases
separately and using a continuous switching function to swap between the
two phases. We can also represent the full phase diagram more generally
by using a more general cutoff rule $$\begin\{aligned\}
\sigma(h(P, T, V) > 0)
\end\{aligned\}$$ where $h$ is some learnable curve in the
pressure-temperature-volume space. This more general scheme would allow
for combining data from different parts of the phase diagram for a
system.

## Venkat Voice Mail and Discussions

Hey bharath um ok this derivation is very interesting. I wonder if
there's a way in which we could connect the phase transition to the
activation function since you could imagine these two phases being the
last layer and then we optimally choose that last piece and then once
you get back then basically taking the derivative of that gets us the
phase transition temperature and other properties as well. I think
there's a way we can do that pretty easily or very nicely. We just need
to figure out what kind of stuff to do to do first order and second
order phase transition. Anyway super cool derivation, let's talk soon.

Notes: The idea is that we can make the order parameter high
dimensional. This high dimensional order parameter can then feed into a
polynomial. This draws into classical concepts in phase field modeling
which also starts from functionals

Eventually, we can just view the system as. a continuous switch function
between two graph neural networks. Roughly, the graph neural network is
a type of parameterized Ising model for the system. The graph neural
networks can just directly encode the two equivariances of the system.

This concept draws from classical methods of using molecular dynamics to
find the $g(T)$ of the system at the two different phases and see where
the functions have a a discontinuity.

TODO: Flesh out these derivations in formal detail. Read references on
molecular dynamics for computing properties

Notes: You can represent symmetry breaking by having differeng
equivariance groups $H$ for the two different sides of the two phases.

Notes: To represent first order phase transitions, we need to enforce a
discontinuity in the enthalpy between the two phases. May be required to
add a repulsion term to the loss. What are the standard repulsion term
losses out there?

Notes: To represent second order phase transitions, enforce a continuity
condition on the enthalpy and add a discontinuity condition on the
derivative. Can do this by directly leveraging the derivative operator
to work with the system.

Notes: The last step remaining is to derive the renormalization group
setting. I think the three steps of the renormalization group process
(coarse graining, rescaling, etc) can all be represented as
differentiable layers. Finding the fixed point of the RG equations is
just an implicit layer. I believe we can directly plug in the Ginzburg
functional into the RG equations to get a model of the critical
parameters? Need to think about this some more.

Note: Differentiable physics seems to require a smoothening. Take the
equations, add on extra terms to allow them to represent a broad class
of architectures. Turn the core operations into differentiable layer.
Solving the free case isn't interesting, but providing infrastructure to
systematically solve the other cases. Leverage symmetries to create
correct by construction solutions (or close to correct).
