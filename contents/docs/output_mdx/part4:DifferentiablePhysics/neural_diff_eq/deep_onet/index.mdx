# Applying Operator Learning to Solve Differential Equations \{#chap:deep_onet\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Deep networks satisfy a powerful universal approximation capability that
allows them to learn arbitrary functions. It is less known that deep
networks satisfy a universal approximation theorem with respect to
learning operators and not just scalar functions. Complex nonlinear
partial or ordinary differential equations provide a natural source of
nonlinear operators. By using the universal operator approximation
capability of deep networks, it is possible to learn the operators
associated with these equations. The DeepONet [@lu2019deeponet] approach
uses two sub-networks, one which encodes $m$ input values
$x_1,\dotsc, x_m$ and another encodes $n$ output location $y$. The
results of the two networks are combined to predict that value of the
solution at $y$.

## The Universal Approximation Theorem for Operators

Let $X$ be a Banach space. Let $K_1 \subset X$ be a compact subset. Let
$K_2 \subset \mathbb\{R\}^d$. $V$ is a compact set in $C(K_1)$. Let
$u \in V$ and $y \in K_2$. Let $G$ be some operator we seek to learn
(usually $G$ is the solution operator to some potentially non-linear
partial differential equation). The type of $G$ is given by

$$\begin\{aligned\}
    G: V \to C(K_2)
\end\{aligned\}$$

The universal approximation theorem for operator learning then states
that for any given approximation accuracy $\epsilon > 0$. there exist
constants $c_i^k, \xi_\{ij\}^k, \theta_i^k$, $\zeta_k$ that satisfy the
approximation equation

$$\begin\{aligned\}
    \left | G(u)(y) - \sum_\{k=1\}^p \sum_\{i=1\}^n c_i^k \sigma \left ( \sum_\{j=1\}^m \xi^k_\{ij\} u(x_j)+ \theta^k_i \right ) \sigma(w_k \cdot y + \zeta_k) \right | < \epsilon 
\end\{aligned\}$$

In this equation, there are $m$ input points, $n$ hidden units in the
input encoding, and $p$ hidden units in the output encoding.

## Basic Structure of the Deep ONet

Let $x_1,\dotsc, x_m$ be the input points. Let $u$ be some input
function. Network inputs are given by

$$\begin\{aligned\}
u(x_1),\dotsc, u(x_m)
\end\{aligned\}$$

Here, we place a constraint that positions $x_1,\dotsc,x_m$ must be
fixed in the training data. The universal approximation theorem in
itself doesn't tell us how to learn an operator $G$, but we can use the
statement of the bound to define \"branch\" and \"trunk\" elements

$$\begin\{aligned\}
\textrm\{trunk\}_k &= \sigma(w_k \cdot y + \zeta_k) \\
\textrm\{branch\}_k &= \sum_\{i=1\}^n c^k_i \sigma \left ( \xi^k_\{ij\} u(x_j) + \theta^k_i \right )
\end\{aligned\}$$

The core idea of the Deep ONet architecture is to approximate $G$ as an
inner product of branchh and trunk computed elements.

$$\begin\{aligned\}
G(u)(y) &= \sum_\{k=1\}^p \textrm\{branch\}_k \textrm\{trunk\}_k
\end\{aligned\}$$

The Deep ONet technique can be used for both ODEs and PDEs with
different choice of operator.
