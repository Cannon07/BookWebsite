# Hopfield Networks \{#chap:behler_parinello\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Hopfield networks are a type of neural network that is not trained with
gradient descent. Instead, a neuroscience-inspired update, the Hebbian
rule, is used to update weights. Informally, the rule used is \"neurons
that fire together wire together.\"


# Machine Learning Basics \{#ch:background\}

[]\{#chap:ml_basics label="chap:ml_basics"\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:optimization\]](#chap:optimization)\{reference-type="ref+label"
reference="chap:optimization"\},
[\[chap:probability\]](#chap:probability)\{reference-type="ref+label"
reference="chap:probability"\},
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

This chapter introduces some fundamentals of modern machine learning and
establishes common mathematical language used in the remainder of this
book.

## Introduction to Machine Learning

Machine learning theory can be framed in terms of datasets and losses.
Let $X \in \mathbb\{R\}^\{N\times M\}$ be a matrix. Let $y \in \mathbb\{R^N\}$
be a vector. We say that $X$ is the data matrix and consists of $N$
datapoints. That is, each $x_i \in \mathbb\{R\}^M$ is a vector of
dimension $M$. Then we say that $y$ is the set of labels associated with
these datapoints. $y_i \in \mathbb\{R\}$ is the label associated with data
point $x_i$. To a simplified approximation, the act of learning is to
find a function $f$ which maps inputs $x_i$ to labels $y_i$. For
example, we could construct the mathematical optimization problem.

$$\begin\{aligned\}
f := \argmin_\{g \in \mathcal\{G\}\} \sum_\{i=1\}^N |g(x_i) - y_i|^2 
\end\{aligned\}$$

Here, $\mathcal\{G\}$ is some subset of the functions mapping
$\mathbb\{R\}^M \to \mathbb\{R\}$. We use the squared error to penalize
mismatches between $g(x_i)$ and $y_i$. Then $f$ is the solution to the
optimization problem $$\begin\{aligned\}
\argmin_\{g \in \mathcal\{G\}\} \sum_\{i=1\}^N |g(x_i) - y_i|^2,
\end\{aligned\}$$ of finding the function $g \in \mathcal\{G\}$ which
minimizes the prediction error mapping datapoints $x_i$ to labels $y_i$.
We introduce an additional piece of terminology. The loss function
$\mathcal\{L\}(g, X, y)$ is the function which determines how far a given
function $g$ is from mapping all the datapoints in matrix $X$ to labels
$y$. In our case, we have

$$\mathcal\{L\}(g, X, y) = \sum_\{i=1\}^N |g(x_i) - y_i|^2$$

When learning, we seek a function $g$ that minimizes loss $\mathcal\{L\}$
for given $X$ and $y$. However, underneath this definition there lies
considerable complexity. What is the function space we are optimizing
over? If we allow arbitrary non-continuous functions, the \"optimal\"
function we find could be a lookup table:

$$\begin\{aligned\}
f(x) = \begin\{cases\}
y_i & x = x_i \\
0 & \textrm\{otherwise\} \\
\end\{cases\}
\end\{aligned\}$$

We have to be more restrictive in our choice of function somehow. We
need a way to mathematically model the intuition that the learned
function shouldn't just be a lookup table. One common framework for
handling this is that of data distributions. Mathematically, we say that
$\mathcal\{D\}$ is a data distribution if it's a probability distribution
which samples data points from $\mathbb\{R\}^M$ and labels from
$\mathbb\{R\}$. Then we can view each data point, label pair as being
sampled from the data distribution as $(x_i, y_i) \sim \mathcal\{D\}$. We
won't be too formal with the definition of $\mathcal\{D\}$, but the basic
idea is that we have some probability density function $p_\{\mathcal\{D\}\}$
over $\mathbb\{R\}^M \times \mathbb\{R\}$ which lets us sample data points
at random. Then our original dataset $(X, y)$ is just a set of $N$
samples drawn from $\mathbb\{D\}$. What's the optimization problem we're
trying to solve then?

$$\begin\{aligned\}
 f := \argmin_\{g \in \mathcal\{G\}\} \mathbb\{E\}_\{(x, y) \sim \mathcal\{D\}\} \left [\mathcal\{L\}(g, x, y) \right] 
\end\{aligned\}$$

This definition captures our intuition that $f$ must be a general
solution. Note that the lookup table we defined above would not be the
minimum of this new function (assuming that the data distribution
$\mathcal\{D\}$ is realistic).

The challenge of course is that this optimization problem isn't actually
solvable in the real world. We often don't have access to the true
distribution $\mathcal\{D\}$ (for scientific applications, $\mathcal\{D\}$
could be some complex measured property that must be experimentally
gathered). Rather, we only have the finite dataset
$X, y \in \mathbb\{R\}^\{N\times M\} \times \mathbb\{R\}^N$ with $N$ data
points. And if we try to directly minimize the loss on our dataset, we
usually run into the *phenomenon* of overfitting again, and run the risk
of getting a lookup function instead of something that captures the true
data distribution $\mathcal\{D\}$. Empirically, the common solution people
take is to split our dataset into subsets for *training* and
*validation*. We might choose say 80% of our $N$ data points to minimize
the loss on. Then we evaluate the goodness of fit of our model by
measuring the loss on the validation set. In actual practice, people
often go further and split into training, validation, and test sets,
where the validation set is used for experimentation and the test set is
only viewed at the end of model development.

We've still left one question open here: what's the space $\mathcal\{G\}$.
A mathematically elegant answer might be to consider something like
$C(\mathbb\{R\}^M, \mathbb\{R\})$, the space of all continuous functions.
However, we run into a wall of infinities here.
$C(\mathbb\{R\}^M, \mathbb\{R\})$ is an enormous space and there will be all
sorts of functions in there. At the end of the day, we have to wrestle
with the problem of *algorithmic complexity*. That is, we need to come
up with solutions that can reasonably run on actual computers. This
suggests we need to consider a smaller space $\mathcal\{G\}$ that is more
feasible to work with. If you're a mathematician, you might point to the
Stone-Weierstrass theorem and suggest a space of polynomials. Or perhaps
sums of trigonometric functions if you're a fan of Fourier analysis. And
there is indeed considerable work learning with these classes of
function. But interestingly, most of modern machine learning focuses
instead on *parametric function classes*.

What do we mean by a parametric class of functions? Well let's start
with a simple example. Suppose that we have a vector
$W \in \mathbb\{R\}^\{M\}$ and a scalar $b \in \mathbb\{R\}$. Then for each
such tuple $(w, b)$ we can define a function
$g[w,b]: \mathbb\{R\}^M \to \mathbb\{R\}$ as follows

$$g[w,b](x) = w^T x + b$$

That is, the space of functions we consider is the space of linear
functions from $\mathbb\{R\}^M\to \mathbb\{R\}$. With a bit of creativity,
you should be able to see how we could specify a parameteric space of
the set of polynomials of degree $\leq n$ or a similar space of
parameteric sinusoidal summations. However, as we shall see in the next
few sections, there are other classes of functions less explored by
classical mathematics that prove exceedingly interesting to the machine
learner. Let's introduce some Physika code to pair with this definition.

``` \{.python language="python"\}
class g(w : $\mathbb\{R\}^M$, b : $\mathbb\{R\}$):
  def $\lambda$(x: $\mathbb\{R\}^M$) $\to \mathbb\{R\}$:
    return w$^T$ x + b
```

This is a parameterized class definition that parameterizes `g` with
parameters `w, b`.

### Data Requirements

The space of distributions over $\mathbb\{R\}^M \times \mathbb\{R\}$ is a
large space and $N$ samples may not be sufficient to learn the true
distribution $\mathcal\{D\}$. This is the *low data problem*. In
applications of learning techniques to the internet or consumer
technologies, it's possible to gather very large datasets with millions,
billions, or even trillions of datapoints. This means that the true
distribution $\mathcal\{D\}$ can often be understood quite well. However,
our data points must be drawn ultimately from physical experiments. For
complex systems, we may consequently have$N$ in the dozens, hundreds, or
low thousands. In subsequent chapters we will discuss various techniques
for wrestling with low data. For example, classical simulators can be
leveraged to generate cheap additional training data. Another common
idea is to place additional restrictions on the function class
$\mathcal\{G\}$ to enable learning of correct functions with fewer
samples.

## Linear Models

A linear model is a learning system where we restrict the parametetric
class $\mathcal\{G\}$ to be the space of linear functions. As before, each
$g$ is parameterized by $(w, b) \in \mathbb\{R\}^M \times \mathbb\{R\}$ and

$$g[w,b](x) = w^T x + b$$

For a given dataset $X, y$, the loss function is then

$$\begin\{aligned\}
\mathcal\{L\}\left (g[w,b], X, y \right) &= \sum_\{i=1\}^N |w^T x + b - y|^2 
\end\{aligned\}$$

In practice though, minimizing this loss leads to *overfitting*. That
is, we find a function which is spiritually akin to our lookup-table and
fails to fit the true data distribution $\mathcal\{D\}$ well. One
technique that helps this problem is *regularization*. The core idea of
regularization is that the loss function $\mathcal\{L\}$ is modified to
penalize excessive complexity in the learned function. The idea here is
that simpler functions are more likely to generalize well. One type of
regularization is called *ridge regression* or *Tikhonov regularization*
and consists of adding a $L^2$ loss term to the loss

$$\mathcal\{L\}\left (g[w,b], X, y\right ) = \sum_\{i=1\}^N |w^T x + b - y|^2  + \alpha \left (\sum_\{i=1\}^N w_i^2 + b^2 \right)$$

Here $\alpha$ is a hyperparameter that controls the amount of weight
penalization. The idea here is straightforward. Values of $w_i$ or $b$
that get too large will increase the loss. Then any learning algorithm
that seeks to minimize the loss will avoid solutions with large $w$ or
$b$. An alternative regularization uses the $L^1$ loss instead of the
$L^2$. This technique is commonly referred to as LASSO (least absolute
shrinkage and selection operator).

$$\mathcal\{L\}\left (g[w,b], X, y\right ) = \sum_\{i=1\}^N |w^T x + b - y|^2  + \lambda \left (\sum_\{i=1\}^N |w_i| + |b| \right)$$

We can transform this loss into Physika code

``` \{.python language="python"\}
def L(g: $(\mathbb\{R\}^M \to \mathbb\{R\})[\mathbb\{R\}^M, \mathbb\{R\}]$, X: $\mathbb\{R\}$[N, M], y: $\mathbb\{R\}$[N], $\lambda$: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}$:
  sum = 0
  for i: sum += (g(X[i]) - y[i])$^2$
  for j: sum += $\lambda$ * |g.w[j]|
  sum += $\lambda$ * |b|
  sum
```


# Stochastic Gradient Descent \{#chap:grad_descent\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\},
[\[chap:multidimensional\]](#chap:multidimensional)\{reference-type="ref+label"
reference="chap:multidimensional"\},
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

The previous chapters have defined the structure of simple machine
learning models but have not explained how to learn model weights from
data. The techniques for model learning draw heavily on the optimization
and numerical methods we have seen in earlier chapters. The core idea is
to use gradient descent to find local optima of loss functions. Consider
the simple model $$\begin\{aligned\}
h[w, b] = w^T x + b,
\end\{aligned\}$$ paired with a $L^2$ loss function:

$$\begin\{aligned\}
    \mathcal\{L\}(h[w, b], x, y) &= | w^T x + b - y| ^2 
\end\{aligned\}$$

Each learning update is to take one step towards a lower loss by
following the gradient:

$$\begin\{aligned\}
 w &\leftarrow w - \alpha \nabla_w \mathcal\{L\} \\
 b &\leftarrow b - \alpha \nabla_b \mathcal\{L\} \\
\end\{aligned\}$$

Here the parameter $\alpha$ is called the step size. Let's define this
system in Physika

``` \{.python language="python"\}
class LinearModel(w: $\mathbb\{R\}^m$, b: $\mathbb\{R\}^m$):
  def $\lambda$(x: $\mathbb\{R\}^m$) $\to$ $\mathbb\{R\}$:
    w*x + b
 
def L(model: $(\mathbb\{R\}^m\to\mathbb\{R\})[\mathbb\{R\}^m, \mathbb\{R\}^m]$, x: $\mathbb\{R\}^m$, y: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}$:
  (model(x) - y)$^2$
```

The full type signature for `LinearModel` can get unwieldy to read at
times, so we will often use the convenient shortcut of referring to the
type as `LinearModel` directly. With this convention, one step of the
learning update can then be computed with

``` \{.python language="python"\}
$\alpha$: $\mathbb\{R\}$
def step(model: LinearModel, x: $\mathbb\{R\}^m$, y: $\mathbb\{R\}$):
  model.w = model.w - $\alpha$ * $\nabla$(L(model, x, y), model.w)
  model.b = model.b - $\alpha$ * $\nabla$(L(model, x, y), model.b)
```

To learn on a dataset $X \in \mathbb\{R\}^\{N\times m\}$,
$y \in \mathbb\{R\}^N$, you can simply repeatedly take gradient descent
steps. We tie this together into a simple learning algorithm for
\"stochastic gradient descent\"

``` \{.python language="python"\}
def sgd(model: $(\mathbb\{R\}^m\to\mathbb\{R\})[\mathbb\{R\}^m, \mathbb\{R\}^m]$, X: $\mathbb\{R\}^\{N \times m\}$, y: $\mathbb\{R\}^N$, num_epochs: $\mathbb\{N\}$):
  for _ in num_epochs:
    for i: step(model, $\alpha$, X[i], y[i])
```

The code snippet above introduces the new concept of epochs. An epoch is
a complete pass of gradient descent steps over the entire dataset. We've
tied the implementation above to the particular linear model at hand,
but this technique can be applied to arbitrary models. There are a few
important variants of this technique. The first is to performed batched
updates on a batch of gradients at a time.

``` \{.python language="python"\}
w: $\mathbb\{R\}$[m], b: $\mathbb\{R\}$
def minibatch(X: $\mathbb\{R\}[B, m]$, y: $\mathbb\{R\}[B]):$
  $\Sigma_\{\nabla w\}$, $\Sigma_\{\nabla b\}$ = zeros(m), zeros(1)
  for i:
    $\Sigma_\{\nabla w\}$, $\Sigma_\{\nabla b\}$ += $\nabla$(loss, X[i], y[i], [w, b])
  w = w - $\alpha$ * (1/B)*$\Sigma_\{\nabla w\}$
  b = b - $\alpha$ * (1/B)*$\Sigma_\{\nabla b\}$
```

In practice, more efficient hardware operations are used than the
for-loop above, but conceptually the same operation is performed.

It is an amazing empirical fact that many different architectures and
models can be trained with this basic algorithm. One of the biggest
mathematical mysteries of deep learning is explaining why gradient
descent works in practice. Most loss functions are non-convex, without a
theoretically guaranteed unique minimum, but simple gradient descent
often does surprisingly well at optimizing these systems. Theorists
don't have a definitive understanding of this phenomenon yet.

## Exercises

1.  Derive and implement the gradient descent step in Physika for a
    quadratic model $$\begin\{aligned\}
            f(x) &= (w^T x + b)^2
        
    \end\{aligned\}$$
