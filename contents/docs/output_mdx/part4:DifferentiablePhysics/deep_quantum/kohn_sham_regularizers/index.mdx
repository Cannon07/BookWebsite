# Kohn-Sham Equations as Regularizers \{#chap:dft_e3nn\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},
[\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

A basic idea when applying differentiable programming techniques to
density functional theory is to use a neural network to approximate the
exchange correlation functional. A vanilla approximation though is
underspecified and will require large amounts of training data. We can
adapt an idea from techniques like physics inspired neural networks
though and use the Kohn-Sham equations to constrain the form of the
function we seek to learn. This technique is called a Kohn-Sham
regularizer [@li2021kohn], [@kalita2021generalizability]. The chosen
regularization scheme allows for learned models to generalize to new
types of molecules past those in a training distribution due to the
richer inductive prior.

## Kohn-Sham as Regularizer

The Kohn-Sham equations are given by

$$\begin\{aligned\}
    \left [ -\frac\{\nabla^2\}\{2\} + v_s[n](r) \right] \phi_i(r) &= \epsilon_i \phi_i(r)
\end\{aligned\}$$ Here $n$ is the electron density $$\begin\{aligned\}
    n(r) &= \sum_i |\phi_i(r)|^2
\end\{aligned\}$$ and $\phi_i$ is a molecular orbital. $v_s$ is the
Kohn-Sham potential and is given by $$\begin\{aligned\}
v_s[n](r) &= v(r) + v_H[n](r) + v_\{xc\}[n](r)
\end\{aligned\}$$ Here $v$ is the external one-body potential, $v_H$ is
the Hartree potential.

$$\begin\{aligned\}
    v_\{xc\}[n](r) &= \frac\{\delta E_\{xc\}\}\{\delta n(r)\}
\end\{aligned\}$$ where $E_\{xc\}$ is the exchange correlation energy
$$\begin\{aligned\}
    E_\{xc\}[n] &= \int \epsilon_\{xc\}[n](r) n(r) dr
\end\{aligned\}$$ where $\epsilon_\{xc\}[n](r)$ is the exchange correlation
energy per electron. This quantity is approximated by a neural network
$$\begin\{aligned\}
    \epsilon_\{xc,\theta\}[n]
\end\{aligned\}$$ The eigenvalue problem for the Kohn-Sham equation is
solved by iterating the self-consistent field equations. The loss
function is given by the deviation from a reference code
$$\begin\{aligned\}
L(\theta) &= \mathbb\{E\}_\{train\}\left [\int  (n_\{ks\}-n_\{DMRG\})^2/N_e dx \right ] + \mathbb\{E\}_\{train\}\left [\sum_\{k=1\}^K w_k (E_k - E_\{DMRG\})^2/N_e\right ]
\end\{aligned\}$$

We use global convolution layers as in the Schnet architecture we
studied earlier in the book

$$\begin\{aligned\}
G(n(x), \xi_p) &= \frac\{1\}\{2\xi_p\} \int  n(x') \exp(-|x-x'|/\xi_p) dx'
\end\{aligned\}$$
