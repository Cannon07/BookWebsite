# Matrix Calculus \{#chap:matrix_calculus\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Matrix calculus extends basic calculus to functions on matrices (and on
tensors). This chapter works through examples of taking matrix
derivatives, starting by deriving derivatives manually, working through
the steps in detail, followed by a more general treatment with the chain
rule and Jacobians.

Let's start with a simple function $f$ $$\begin\{aligned\}
    f(\vec\{w\}, \vec\{x\}) = \vec\{w\}^T \vec\{x\} = \sum_\{i=1\}^N w_i x_i 
\end\{aligned\}$$ Where
$f: \mathbb\{R\}^N \times \mathbb\{R\}^N \to \mathbb\{R\}$. Let's treat
$\vec\{w\}$ as constants. From the definition of the Jacobian, we have
$\mathcal\{J\}_\{\vec\{x\}\}(f) \in \mathbb\{R\}^\{1 \times N\}$. That is,
$\mathcal\{J\}_\{\vec\{x\}\}(f)$ is a row vector that can be written as

$$\begin\{aligned\}
\mathcal\{J\}_\{\vec\{x\}\}(f) &= \begin\{bmatrix\}
     \frac\{\partial f\}\{\partial x_1\} & \cdots & \frac\{\partial f\}\{ \partial x_N\}  \\
    \end\{bmatrix\} \\
    &= \begin\{bmatrix\}
    w_1 & \cdots & w_N \\
    \end\{bmatrix\} \\
    &= \vec\{w\}^T
\end\{aligned\}$$

We will for convenience write
$\mathcal\{J\}_\{\vec\{x\}\}(f) = \frac\{\partial f\}\{\partial \vec\{x\}\} \in \mathbb\{R\}^\{1 \times N\}$
but this is purely a syntactic representation. You should mentally
expand this shorthand to the Jacobian when you read it. Let's now treat
$\vec\{x\}$ as constants and take the derivative with respect to
$\vec\{w\}$. We get $$\begin\{aligned\}
\mathcal\{J\}_\{\vec\{w\}\}(f) &= \begin\{bmatrix\}
     \frac\{\partial f\}\{dw_1\} & \cdots & \frac\{\partial f\}\{\partial w_N\}  \\
    \end\{bmatrix\} \\
    &= \begin\{bmatrix\}
    x_1 & \cdots & x_N \\
    \end\{bmatrix\} \\
    &= \vec\{x\}^T 
\end\{aligned\}$$ For convenience, we can write
$\mathcal\{J\}_\{\vec\{w\}\}(f) = \frac\{\partial f\}\{\partial \vec\{w\}\} \in \mathbb\{R\}^\{1 \times N\}$.
It's very useful to keep careful tracks of shapes when doing these types
of derivatives.

Let's now start a more complex example. Let
$W \in \mathbb\{R\}^\{M \times N\}$ be a matrix and let
$\vec\{x\} \in \mathbb\{R\}^N$ be a vector. Let's define a function

$$\begin\{aligned\}
    f(W, \vec\{x\}) &= \textrm\{sum\}(W \vec\{x\}^T) \\
    &= \sum_\{i=1\}^M \sum_\{j=1\}^N W_\{ij\} x_j \\
\end\{aligned\}$$

Here $\textrm\{sum\}(\vec\{v\}) = \sum_\{i=1\}^M v_i$ is the summation
function. Then we have that
$f: \mathbb\{R\}^\{M \times N\} \times \mathbb\{R\}^N \to \mathbb\{R\}$.

Let's now return to our function $f$ and take some partial derivatives.
Let's first find the partial derivative with respect to $\vec\{x\}$ (so
we're treating $W$ as a constant). Let's check our shapes first. We have

$$\begin\{aligned\}
\frac\{\partial f\}\{\partial \vec\{x\}\} &= \mathcal\{J\}_\{\vec\{x\}\}(f) \in \mathbb\{R\}^\{1 \times N\} \\
&= \begin\{bmatrix\}
\frac\{\partial f\}\{x_1\} & \cdots & \frac\{\partial f\}\{x_N\} \\
\end\{bmatrix\} 
\end\{aligned\}$$ Let's expand out these inner terms. We can use the fact
that partial derivatives factor through summation to write
$$\begin\{aligned\}
    \frac\{\partial f\}\{\partial x_k\} &= \sum_\{i=1\}^M \frac\{\partial\}\{\partial x_k\} \left ( \sum_\{j=1\}^N W_\{ij\} x_j \right ) \\
    &= \sum_\{i=1\}^M  \sum_\{j=1\}^N W_\{ij\} \frac\{\partial x_j \}\{\partial x_k\}  \\
    &= \sum_\{i=1\}^M  W_\{ik\}  \\
    &= \textrm\{sum\}(W[:, k]) 
\end\{aligned\}$$ Here we have used the fact that
$\frac\{\partial x_j\}\{\partial x_j\}$ is $1$ if $j = k$ and $0$ otherwise.
We introduce a new bit of notation here. We use $W[:, k]$ to denote the
$k$-th column of $W$. We can now write the full Jacobian out as

$$\begin\{aligned\}
\frac\{\partial f\}\{\partial \vec\{x\}\} &= \begin\{bmatrix\}
\textrm\{sum\}(W[:, 1]) & \cdots & \textrm\{sum\}(W[:, N]) \\
\end\{bmatrix\} 
\end\{aligned\}$$

Let's now assume that $\vec\{x\}$ is a constant and take derivatives. We
have that

$$\begin\{aligned\}
    \frac\{\partial f\}\{\partial W\} &= \mathcal\{J\}_\{W\}(f) \in \mathbb\{R\}^\{1 \times M \times N\} 
\end\{aligned\}$$ We can view an element of
$\mathbb\{R\}^\{1 \times M \times N\}$ as a matrix in
$\mathbb\{R\}^\{M \times N\}$ but for pedagogical reasons, let's keep the
full shape. We write $$\begin\{aligned\}
\left (\frac\{\partial f\}\{\partial W\}\right )_\{1, k, \ell\} &= \frac\{\partial f\}\{\partial W_\{k \ell\}\} \\
&= \sum_\{i=1\}^M \sum_\{j=1\}^N \frac\{\partial W_\{ij\}\}\{\partial W_\{k \ell\}\} x_j \\
&= x_\{\ell\} 
\end\{aligned\}$$ We can visualize the full derivative as
$$\begin\{aligned\}
\frac\{\partial f\}\{\partial W\} &=
    \begin\{bmatrix\}
    \begin\{bmatrix\}
    \vec\{x\}^T \\
    \vdots \\
    \vec\{x\}^T
    \end\{bmatrix\} 
    \end\{bmatrix\} \\
    &= \begin\{bmatrix\} \textrm\{ones\}(N) \otimes \vec\{x\} \end\{bmatrix\} 
\end\{aligned\}$$ Note that we're using the notation for multidimensional
arrays we introduced previously. We also introduce two new bits of
notation here. First, we define $\textrm\{ones\}(N)$ to be
$$\textrm\{ones\}(N) = \begin\{bmatrix\}
1 \\
\vdots \\
1 \\ 
\end\{bmatrix\}$$ That is, $\textrm\{ones\}(N)$ is the column vector made up
of ones. We also introduce the outer product denoted as $\otimes$. The
outer product of two vectors is defined as
$$\vec\{u\} \otimes \vec\{v\} = \begin\{bmatrix\}
u_1 v_1 & \cdots & u_1 v_N \\
& \vdots & \\
u_M v_1 & \cdots & u_M v_N \\
\end\{bmatrix\}$$

As we can see, these calculations of matrix derivatives are starting to
get a little tricky. Let's introduce a bit of machinery that will help
facilitate calculations of more complex derivatives. Let
$f: \mathbb\{R\}^N \to \mathbb\{R\}^M$ be a differentiable function. Let
$g:\mathbb\{R\}^M \to \mathbb\{R\}^O$ be a second differentiable function.
Let's say that we know the Jacobians of $g$ and $f$, $\mathcal\{J\}(g)$
and $\mathcal\{J\}(f)$. Can we compute the Jacobian of $g \circ f$, the
composition in terms of the component Jacobians? Let's expand out the
definition.

$$\begin\{aligned\}
    \mathcal\{J\}(g \circ f) &= \begin\{bmatrix\}
    \frac\{\partial (g \circ f)_1\}\{\partial x_1\} & \cdots & \frac\{\partial (g \circ f)_1\}\{\partial x_N\} \\
    & \vdots & \\
    \frac\{\partial (g \circ f)_O\}\{\partial x_1\} & \cdots & \frac\{\partial (g \circ f)_O\}\{\partial x_N\} \\
    \end\{bmatrix\}
\end\{aligned\}$$

The challenge here seems to be finding a clean derivation of
$\frac\{\partial (g \circ f)_i\}\{\partial x_j\}$. (Here $(g\circ f)_i$ is
the $i$-th component of the output of $g \circ f$.). One challenge here
is that there are a number of \"hidden\" intermediate variables. That
is,

$$(g \circ f)_i(\vec\{x\}) = g_i(f_1(\vec\{x\}), \dotsc, f_M(\vec\{x\}))$$

The values $f_i(\vec\{x\})$ are the \"hidden\" variables here. We can use
the chain rule for scalar variables to expand out the partial derivative
we're working with.

$$\begin\{aligned\}
    \frac\{\partial (g \circ f)_i\}\{\partial x_j\} &= \sum_\{k=1\}^M \frac\{\partial (g \circ f)_i\}\{\partial f_k\} \frac\{\partial f_k\}\{\partial x_j\} \\
    &= \sum_\{k=1\}^M \frac\{\partial g_i\}\{\partial y_k\} \frac\{\partial f_k\}\{\partial x_j\} \\
    &= \mathcal\{J\}(g)[i, :]^T \mathcal\{J\}(f)[:, j]
\end\{aligned\}$$

That is, $\frac\{\partial (g \circ f)_i\}\{\partial x_j\}$ is simply the dot
product of the $i$-th row of $\mathcal\{J\}(g)$ with the $j$-th column of
$\mathcal\{J\}(g)$. From the definition of matrix multiplication, it
follows that

$$\begin\{aligned\}
    \mathcal\{J\}(g \circ f) &= \mathcal\{J\}(g) \mathcal\{J\}(f)
\end\{aligned\}$$

That is, composition of functions reduces to the products of the
Jacobians! This is a powerful generalization of the chain rule which can
help us compute matrix derivatives for complex functions by breaking
them down into simpler Jacobians. Let's introduce a more complex
function

$$\begin\{aligned\}
f(\vec\{v\}, W, \vec\{b\}, \vec\{x\}) &= \vec\{v\}^T \sigma\left (W \vec\{x\} + \vec\{b\} \right )
\end\{aligned\}$$

Here,
$\vec\{v\} \in \mathbb\{R\}^M, W \in \mathbb\{R\}^\{M \times N\}, \vec\{b\} \in \mathbb\{R\}^M, \vec\{x\} \in \mathbb\{R\}^N$.
We also define the function $\sigma: \mathbb\{R\} \to \mathbb\{R\}$ to be
the logistic function as follows $$\begin\{aligned\}
    \sigma(x) &= \frac\{1\}\{1 + \exp(-x)\}
\end\{aligned\}$$ You might note that
$W \vec\{x\} + \vec\{b\} \in \mathbb\{R\}^M$. What does
$\sigma(W \vec\{x\} + \vec\{b\})$ mean then? Our definition above operates
on real numbers. Simply put, we apply the function $\sigma$ element-wise
to the vector $W \vec\{x\} + \vec\{b\}$. Our function $f$ is in fact a
simple neural network with one \"hidden\" layer. We will often want to
take the derivative of functions such as $f$. How can we do this
cleanly? Doing these calculations by hand can become complicated so at
this point, we will usually fall back to an automated algorithm. If you
would like a challenge, try computing the Jacobians associated with this
function by hand.

### Exercises

1.  The directional derivative $\nabla_\{\vec\{u\}\}(f)$ is defined as
    $$\nabla_\{\vec\{u\}\}(f) = \lim_\{h \to 0\} \frac\{f(x + h\vec\{u\}) - f(x)\}\{h\}$$
    Prove that
    $$\nabla_\{\vec\{u\}\}(f) = \frac\{df\}\{dx_1\}u_1 + \cdots + \frac\{df\}\{dx_N\} u_N$$

2.  Let $\vec\{u\}$ be a unit vector. Prove that the directional
    derivative $\nabla_\{\vec\{u\}\}(f)$ is maximized when
    $\vec\{u\} = \frac\{\nabla f\}\{\|\nabla f\|\}$.

3.  Consider the function $$\begin\{aligned\}
        g(W, \vec\{x\}) = W \vec\{x\}
    \end\{aligned\}$$ Note that
    $g: \mathbb\{R\}^\{M \times N\} \times \mathbb\{R\}^N \to \mathbb\{R\}^N$.
    Compute the Jacobians associated with this function.
