# Tensor Field Networks \{#chap:neural_equivariant_potentials\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

The molecular representations we have learned thus far do not respect
known physical constraints about molecular systems. In particular,
learned representations for a molecule should update correctly under
three dimensional rotations, translations and permutations. That is,
representations should update in an equivariant fashion. Respecting
equivariance natively in an architecture lowers the requirement for data
augmentation techniques which manually add rotated datapoints to
training data but at the cost of higher learning complexity.

Tensor field networks [@thomas2018tensor] use spherical harmonics to
encode rotational equivariance correctly. Tensor field networks
introduce some powerful equivariant primitives that are used in other
architectures like neural equivariant potentials. In particular, tensor
field networks track scalar, vector, and higher order tensors for every
atom in the system.

## A Brief Review of Equivariance

A representation $\rho$ for a group $G$ is a function
$\rho: G \to \mathbb\{R\}^\{N \times N\}$ such that $$\begin\{aligned\}
    \rho(g)\rho(h) &= \rho(gh)
\end\{aligned\}$$ Suppose that $X$ and $Y$ are vector spaces with
representations $\rho^X$, $\rho^Y$. Then a function $L: X \to Y$ is
equivariant with respect to group $\rho$ and representations $\rho^X$
and $\rho^Y$ if the following operators are equal $$\begin\{aligned\}
L \circ \rho^X(g) &= \rho^Y(g) \circ L
\end\{aligned\}$$ A function $L$ is invariant if $\rho^Y(g) = I$ for all
$g \in G$. In the exercises, we ask you to prove some composition
properties of equivariant networks.

## Tensor Field Networks \{#tensor-field-networks\}

Each tensor field network layer takes in a set of vectors
$S = \{\vec\{r\}_a\}$ in $\mathbb\{R\}^3$, along with a vector $x_a \in X$
for each $\vec\{r\}_a$ in $S$ and outputs a vector $y_a \in Y$.
$$\begin\{aligned\}
    L(\vec\{r\}_a, x_a) &= (\vec\{r\}_a, y_a)
\end\{aligned\}$$ Here the $x_a, y_a$ should be interpreted as feature
vectors, and $X$ and $Y$ are spaces of appropriate features.

### Permutation Equivariance

We want our layer $L$ to be equivariant to the action of permutations on
the atoms. Formally, we can define permutation

$$\begin\{aligned\}
    P_\sigma(\vec\{r\}_a, x_a) &= (\vec\{r\}_\{\sigma(a)\}, x_\{\sigma(a)\})
\end\{aligned\}$$

Here $P_\sigma$ permutes the atoms, where $$\begin\{aligned\}
\sigma: \{1,\dotsc, n\} \to \{1,\dotsc, n\}
\end\{aligned\}$$ is a bijective map. For $L$ to be equivariant to
permutations, we must have that $$\begin\{aligned\}
    P_\{\sigma\} \circ L &= L \circ P_\{\sigma\}
\end\{aligned\}$$

### Translation Equivariance

We define translations by operators that shift the position of all atoms
without changing their feature vectors $$\begin\{aligned\}
    T_\{\vec\{t\}\}(\vec\{r\}_a, x_a) &= (\vec\{r\}_a + \vec\{t\}, x_a)
\end\{aligned\}$$ For $L$ to be equivariant to translations, we impose the
condition that $$\begin\{aligned\}
    T_\{\vec\{t\}\} \circ L &= L \circ T_\{\vec\{t\}\}
\end\{aligned\}$$

### Rotation Equivariance

Recall that $SO(3)$ is the group of three dimensional rotations. Note
that $g \in SO(3)$ can act on vector $\vec\{r\} \in \mathbb\{R\}^3$ by
rotating this vector. We write this action as $$\begin\{aligned\}
    R(g)\vec\{r\} \in \mathbb\{R\}^3
\end\{aligned\}$$ where $R$ is the representation of $SO(3)$ on
$\mathbb\{R\}^\{3 \times 3\}$. Suppose that we have representation $\rho^X$
of $SO(3)$ on vector space $X$ and $\rho^Y$ on vector space $Y$. Then
note that $g \in SO(3)$ can act on a feature vector $x_a$ as
$$\begin\{aligned\}
    \rho^X(g)x_a \in X
\end\{aligned\}$$ We define a joint operator $$\begin\{aligned\}
    [R(g) \oplus \rho^X(g)](\vec\{r\}_a, x_a) &= (R(g)\vec\{r\}_a, \rho^X(g)x_a)
\end\{aligned\}$$ The rotational equivariance condition is then given by
$$\begin\{aligned\}
    L \circ [R(g) \oplus \rho^X(g)] &= [R(g) \oplus \rho^Y(g)] \circ L
\end\{aligned\}$$ We will make use of a particular family of
representations, the Wigner $D$-matrices, $$\begin\{aligned\}
    D^\{(l)\}: SO(3) \to \mathbb\{R\}^\{(2l+1)\times(2l+1)\}
\end\{aligned\}$$ which maps elements of $SO(3)$ to square matrices of
side $2l+1$. We define $$\begin\{aligned\}
    D^\{(0)\} &= I \\
    D^\{(1)\} &= R(g)
\end\{aligned\}$$ The number $l$ is referred to as the rotation order of
the representation.

## Constructing Tensor Field Layers

As mentioned earlier, each tensor field network layer takes in a
collection $S = \{\vec\{r\}_a\}$ of points in $\mathbb\{R\}^3$ and a
collection of associated feature vectors $\{x_a\}$ that lives in a
vector space $X$ with a representation of $SO(3)$. This representation
can be uniquely decomposed as a sum of irreducible representations of
$SO(3)$ on this space. We may have multiple copies of a given
irreducible representation (say of order $l$) in this decomposition.
These copies are called \"channels\", in analogy with channels in
convolutional networks. We define the dictionary

$$\begin\{aligned\}
    V^\{(l)\}_\{acm\} &= \{l: A_l \in  \mathbb\{R\}^\{[|S|, n_l, 2l+1]\} \}
\end\{aligned\}$$ Here $V^\{(l)\}_\{acm\}$ maps rotation order $l$ to a
multidimensional array $A_l$ of inputs. Here we follow the following
conventions on variables

1.  $a$: The point index referring to a given atom

2.  $c$: The channel index. Recall that a channel refers to a copy of an
    irreducible representation of order $l$. $n_l$ is used to refer to
    the number of channels.

3.  $m$: The representation index, ranging from $1$ to $2l+1$,
    corresponding to the dimensions of the Wigner $D$-matrix

![The dictionaries $V^\{(l)\}_\{acm\}$ contain equivariant features used by
the tensor field network and other related
architectures.](figures/Differentiable Physics/Molecular ML/Tensor Field Networks/Vacm dictionaries.png)\{#fig:acm_features
width="70%"\}

In the reminder of this chapter, we will explicitly construct some
equivariant tensor field layers that operate on these dictionary
objects.

## Point Convolutions

We define point convolutions as objects that take as input dictionaries
$V^\{(l)\}_\{acm\}$. These convolutions can be viewed as a generalization of
a standard convolutional layer (with the usual convolution corresponding
to the case where the atoms are arranged in a regular grid).

### Spherical Filters

Recall that the spherical harmonics are functions defined on the sphere

$$\begin\{aligned\}
    Y^\{(l)\}_m: S^2 \to \mathbb\{R\}
\end\{aligned\}$$ Here $l$ is again the rotation order. For order $0$ and
order $1$, we have that the spherical harmonics are functions

$$\begin\{aligned\}
    Y^\{(0)\}(\hat\{r\}) &\propto 1 \\
    Y^\{(1)\}(\hat\{r\}) &\propto \hat\{r\} 
\end\{aligned\}$$

Spherical convolutions are equivariant to $SO(3)$ and for all
$g \in SO(3)$, satisfy the equivariance relationship

$$\begin\{aligned\}
    Y^\{(l)\}_m(R(g)\hat\{r\}) &= \sum_\{m'\} D^\{(l)\}_\{mm'\}(g)Y^\{(l)\}_\{m'\}(\hat\{r\})
\end\{aligned\}$$

Here $D$ is the Wigner $D$-matrix. Spherical harmonic functions provide
useful primitives to define rotation equivariant filters. Let $l_i$
denote the rotation order of the input and $l_f$ the rotation order of
the filter. We mandate that filter $F^\{(l_f, l_i)\}$ takes the functional
form

$$\begin\{aligned\}
    F^\{(l_f, l_i)\}_\{cm\}(\hat\{r\}) &= R_c^\{(l_f, l_i)\}(r) Y^\{(l_f)\}_m(\hat\{r\})
\end\{aligned\}$$ where $R_c^\{(l_f, l_i)\}$ depends only on the radius $r$
of $\hat\{r\}$ $$\begin\{aligned\}
R_c^\{(l_f, l_i)\}: \mathbb\{R\}^+ \to \mathbb\{R\}
\end\{aligned\}$$ is a learnable function in this filter. This filter by
construction is also equivariant to $SO(3)$.

### Combining Representations with Tensor Products

The tensor product provides a useful tool for combining representations.
Suppose $\rho^X$ and $\rho^Y$ are two representations. Then
$\rho^X \otimes \rho^Y$ is a representation on $X \otimes Y$. Suppose
that that we have an irreducible representation $u^\{(l_1)\}$ of rotation
order $l_1$ and an irreducible representation $v^\{(l_2)\}$ of rotation
order $l_2$. Then $u^\{(l_1)\} \otimes v^\{(l_2)\}$ is a representation of
order $l_1 \times l_2$. The coefficients of this combined representation
are given explicitly by the Clebsch-Gordon coefficients
$$\begin\{aligned\}
(u \otimes v)^\{(l)\}_m &= \sum_\{m_1=-l_1\}^\{l_1\} \sum_\{m_2=-l_2\}^\{l_2\} C^\{(l,m)\}_\{(l_1,m_1),(l_2,m_2)\} u^\{(l_1)\}_\{m_1\} v^\{(l_2)\}_\{m_2\}
\end\{aligned\}$$

### Combining Clebsch-Gordon and Spherical Filters

Given this definition, we define the point convolution layer. The
learned spherical filters inhabits one representation, and the input
inhabits another. To combine the filter representation and the input
representation, we have to make use of the Clebsch-Gordon rule. Let
$l_o$ denote the rotation order of the output. Then we define the point
convolution layer by the rule

$$\begin\{aligned\}
L^\{(l_0)\}_\{acm_o\}(\vec\{r\}_a, V^\{(l_i)\}_\{acm_i\}) &= \sum_\{m_f, m_i\} C^\{(l_o, m_o)\}_\{(l_f, m_f), (l_i, m_i)\} \sum_\{b \in S\} F^\{(l_f, l_i)\}_\{cm_f\}(\vec\{r\}_\{ab\})V^\{(l_i)\}_\{bcm_i\}
\end\{aligned\}$$

The rule is dense to parse, but intuitively, it combines each filter
across all pairs of atoms against the input to produce an output. This
joint construction is equivariant to rotations.

## Self-Interaction Layers

Self interaction layers are defined by the equation $$\begin\{aligned\}
V^\{(l)\}_\{acm\} &= \sum_\{c'\} 
 W^\{(l)\}_\{cc'\}V^\{(l)\}_\{ac'm\}
\end\{aligned\}$$ These layer blend information across channels of
$V^\{(l)\}_\{acm\}$. Because $W$ does not depend on the representation index
$m$, it commutes with the Wigner $D$-matrices.

## Nonlinearities

The nonlinear layers defined here act along the $m$ dimension. We define
the norm operation $$\begin\{aligned\}
\|V\|^\{(l)\}_\{ac\} &= \sqrt\{\sum_m |V^\{(l)\}_\{acm\}|^2\}
\end\{aligned\}$$ Suppose we have some learnable function for each $l$
$$\begin\{aligned\}
\eta^\{(l)\}: \mathbb\{R\} \to \mathbb\{R\}
\end\{aligned\}$$ We define the nonlinearity layer as $$\begin\{aligned\}
V^\{(l)\}_\{acm\} &= \eta^\{(l)\}(\|V\|^\{(l)\}_\{ac\} + b^\{(l)\}_c) V^\{(l)\}_\{acm\}
\end\{aligned\}$$ where $b^\{(l)\}_c$ is a bias offset. The Wigner
$D$-matrices are unitary, so they do not change the norm $\|V\|$, which
means that our nonlinearities commute with the $D$-matrices.

## Building Tensor Field Architectures

The point-convolution, self-interaction, and nonlinear layers can be
combined and recombined as in standard convolutional architectures.
Unlike standard convolutional architectures, features $V^\{(l)\}_\{acm\}$
hold information for different orders $l$ allowing for manipulation of
scalar, vector, matrix, and higher order features.

![An illustration of information flow in a 3-layer tensor field
architecture with scalar and vector features
<https://arxiv.org/pdf/1802.08219.pdf>](figures/Differentiable Physics/Molecular ML/Tensor Field Networks/Information flow_tf arch.png)\{#fig:my_label\}

## Exercises

1.  Prove that the composition of two equivariant networks is
    equivariant.

2.  Prove that if a network is equivariant with respect to
    transformation $h$ and transformation $g$ separately, it is
    equivariant with respect to composed transformation $hg$.

3.  Prove that filter $F_c^\{(l_f, l_i)\}$ is equivariant to $SO(3)$.
