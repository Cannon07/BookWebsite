# Basic Root Finding \{#chap:newton_raphson\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:math_basics\]](#chap:math_basics)\{reference-type="ref+label"
reference="chap:math_basics"\}\
**Difficulty Level:** \*\

------------------------------------------------------------------------

One of the foundational problems of numerical analysis is finding the
roots of functions. We start with one dimensional functions, and in
later chapters will generalize to higher dimensional systems.

In particular, we explain how to solve and implement a root finding
problem with the Newton-Raphson method and how to appreciate the concept
of quadratic convergence. We will then cover numeric methods for
estimating roots and how to manipulate and determine the roots of
polynomials.

## Open Methods

::: marginfigure
![image](figures/part1b/newton_raphson/openmethod0.png)\{width="2.4in"\}
:::

In the bracketing methods of root-finding, the root is located within an
interval prescribed by a lower and an upper bound. In contrast, open
methods require only a single starting value or two starting values that
do not necessarily bracket the root. In *bracketing methods*, the
estimate of the root gets better with every iteration. This cannot be
ensured in the case of *open methods*. However, when the open methods
converge, they converge much faster than the bracketing methods. The
distinction between bracketing methods and open methods is shown in
Figure [\[fig:1\]](#fig:1)\{reference-type="ref" reference="fig:1"\} by
depicting Bisection (bracketing method) and Newton-Raphson (open
method).

## Newton-Raphson Method

::: marginfigure
![image](figures/part1b/newton_raphson/newtonraphson0.png)\{width="2.4in"\}
:::

The most widely used of all root locating formulas amongst the open
methods is the *Newton-Raphson method*. The method starts with an
initial guess of the root at $x_i$ and the tangent to the function is
drawn at the point $[x_i,f(x_i)]$. The point where this tangent crosses
the x-axis usually represents an improved estimate of the root. This can
be observed from Figure [\[fig:2\]](#fig:2)\{reference-type="ref"
reference="fig:2"\}. Using the definition of the derivative (slope of
tangent), this can be derived as $$\begin\{aligned\}
f'(x_i) &= \dfrac\{f(x_i) - 0\}\{x_i - x_\{i+1\}\} \\
x_\{i+1\} &= x_i - \dfrac\{f(x_i)\}\{f'(x_i)\}
\end\{aligned\}$$

The above equation is called the *Newton-Raphson formula* which iterates
over the root search. In root-finding problems in many variables, the
equation becomes:
$$\vec\{x\}_\{i+1\} = \vec\{x\}_i - \mathcal\{J\}(x_i)^\{-1\} \mathcal\{F\}(\vec\{x_i\})$$
where the product of the Jacobian times the function value at the point
$\vec\{x\}_i$

Although the Newton-Raphson method is often very efficient, there are
situations where it performs poorly. For example, let's try to evaluate
the root of $f(x) = x^\{10\} - 1$ using Newton-Raphson method with an
initial guess of x= 0.5. The Newton-Raphson formula for this function is
given by $$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{x_i^\{10\} - 1\}\{10x_i^9\}
 
\end\{aligned\}$$ This can be used to compute the next root estimate. The
root estimates with the iterations are given in the table.

::: margintable
   **i**   **$x_i$**   **$|\epsilon_a|$ in %**
  ------- ----------- -------------------------
     0        0.5     
     1       51.65             99.032
     2      46.485             11.111
     3      41.8365            11.111
     4      37.6585            11.111
     .                
     .                
     .                
     .                
    40     1.002316             2.130
    41     1.000024             0.229
    42         1                0.002
:::

Since, the first guess is near a region where the slope is near zero,
flings the solution far away from the initial guess to a new value
($x = 51.65$) where $f(x)$ has an extremely high value. The solution
then plods along for over 40 iterations until converging on the root
with adequate accuracy.

There are some additional cases where the Newton-Raphson exhibits poor
performance. Some of these are depicted in Figure
[\[fig:3\]](#fig:3)\{reference-type="ref" reference="fig:3"\}. In (a), if
the root is near the inflection point (i.e. $f'(x) = 0$), the iterations
beginning at $x_0$ progressively diverge from the root. In (b), the
tendency of the Newton-Raphson technique to oscillate around a local
maximum or minimum is demonstrated. (c) shows how an initial guess that
is close to one root can jump to a location several roots away. This is
observed when near-zero slopes are encountered. This scenario can be
observed well in (d), where the slope is zero and the next root estimate
never hits the x-axis. Thus, there is no general convergence criterion
for Newton-Raphson. Its convergence depends on the nature of the
function and on the accuracy of the initial guess. The only remedy is to
have an initial guess that is sufficiently close to the root.

::: marginfigure
![image](figures/part1b/newton_raphson/newtonraphsonpoor0.png)\{width="2in"\}
:::

Let's go back to the example function that was used to demonstrate the
other root finding methods and re-solve it using the Newton-Raphson
method. Let's implement the same relative convergence criterion. We
start with the function we seek to optimize

``` \{.python language="python"\}
def f(x: $\mathbb\{R\}$):
  return $x^3$ - 3.7*x$^2$ + 4.54*x - 1.848    
```

Next we implement Newton-Raphson

``` \{.python language="python"\}
def newton_raphson(f: $\mathbb\{R\} \to \mathbb\{R\}$, start: $\mathbb\{R\}$ = 0): $\mathbb\{R\}$  
  old = start
  while True:
    new = old - f(old)/grad(f)(old)                 
    # Convergence criterion                              
    if abs((new-old)/new) < 10**(-8):           
      break
    old = new
  return new
```

Note that Newton Raphson converges much faster than the false position
method for the same function. As an exercise, try the function $x^3-x-3$
with $x=0$ as the starting point.

## Secant Method

The Newton-Raphson method requires the evaluation of the derivative for
its implementation. Although this is not inconvenient for polynomials
and many other functions, there are certain functions whose derivatives
may be difficult or inconvenient to evaluate. For these cases, the
derivative can be approximated by a backward finite divided difference:
$$\begin\{aligned\}
 f'(x_i) \cong \dfrac\{f(x_\{i-1\}) - f(x_\{i\})\}\{x_\{i-1\} - x_\{i\}\}
\end\{aligned\}$$ The approximation is substituted into the
*Newton-Raphson formula* to obtain the following iterative equation:
$$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{f(x_i)(x_\{i-1\} - x_\{i\})\}\{f(x_\{i-1\}) - f(x_\{i\})\}
\end\{aligned\}$$ The above equation is called the *Secant-method
formula*. Notice that the approach requires two initial estimates of x.
However, because f (x) is not required to change signs between the
estimates, it is not classified as a bracketing method.

Rather than using two arbitrary values to estimate the derivative, an
alternative approach involves a fractional perturbation of the
independent variable to estimate the derivative. $$\begin\{aligned\}
 f'(x_i) \cong \dfrac\{f(x_\{i\} + \delta) - f(x_\{i\})\}\{\delta\}
\end\{aligned\}$$ where $\delta$ is a small perturbation. The
approximation is substituted into the *Newton-Raphson formula* to obtain
the following iterative equation: $$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{\delta \times f(x_i)\}\{f(x_\{i\} + \delta) - f(x_\{i\})\}
\end\{aligned\}$$ We call this the *modified secant method*. It provides a
nice means to attain the efficiency of Newton-Raphson without having to
compute derivatives.

Let's use the same example and find a root using the secant method. Note
that this requires two starting points.

``` \{.python language="python"\}
def secant_method(f: $\mathbb\{R\} \to \mathbb\{R\}$, now: $\mathbb\{R\}$ = 120, prev: $\mathbb\{R\}$ = 100): $\mathbb\{R\}$                                            
  while True: 
    f_prime = (f(prev)-f(now))/(prev-now)        
    new = now-f(now)/f_prime                       
    # Convergence criterion 
    if abs((new-now)/new) < 10**(-8):           
      break 
    prev = now
    now = new
  return new
```

Note that the secant method also converges much faster than the false
position method in this case for the same tolerance level.

## Optimization as Root Finding

In this section, we will learn how to use root-finding techniques to
find local optima. We will understand how Newton's method for
on-dimensional optimization is an extension from the Newton-Raphson
method for root finding.

## Newton's Method

The same techniques learnt in root-finding can be extended to
optimization by changing the function under consideration to $f'(x)$
from $f(x)$ i.e. the values of x where $f'(x) = 0$ are local optima (or)
inflection-points.

Considering the Newton-Raphson method, the update rule for root-finding
is:

$$\begin\{aligned\}
 x_\{i+1\} = x_i - \dfrac\{f(x_i)\}\{f'(x_i)\}
\end\{aligned\}$$

Using the same Newton-Raphson method, the update rule for optimization
is:

$$\begin\{aligned\}
x_\{i+1\} = x_i - \dfrac\{f'(x_i)\}\{f''(x_i)\}
\end\{aligned\}$$

This approach to optimization is called the Newton's Method in one
dimension. It is worth highlighting that this is an open method for
optimization.

``` \{.python language="python"\}
def newton_raphson_opt(f: $\mathbb\{R\} \to \mathbb\{R\}$, start: $\mathbb\{R\}$, tol:$\mathbb\{R\}$ = 1e-8): 
  old = start
  while True: 
    new = old-grad(f)(old)/grad(grad(f))(old)
    if abs((new-old)/new) < tol:
      break
    old = new
  return new
```

In a similar manner, other root finding techniques can also be used for
optimization to converge to local optima where the derivative is zero.

## Exercises

1.  Apply the Newton-Raphson method to the function $x^3 - x - 3$ with
    $x=0$ as the starting point

2.  Apply the Secant method to the function $x^3 - x - 3$ with $x=0$ and
    $x=2$ as the two starting points.

3.  Determine a local optima of the function $f(x) = \exp(x)-2x$ using
    the Newton-Raphson method
