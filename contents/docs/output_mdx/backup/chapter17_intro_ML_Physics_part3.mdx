# Introduction to Scientific Machine Learning \{#ch:intro-molecular-ml\}

While applying machine learning algorithms to physical systems, there
typically are additional constraints in the form of respecting any given
laws of physics often described by general nonlinear partial
differential equations. In this chapter, we will introduce approaches to
incorporate such additional constraints into machine learning
algorithms. We will also discuss approaches to discover and learn the
underlying partial differential equations for systems where the
underlying physics can emerge from collected data.

## Equivariance in Neural Networks

Understanding symmetry's role in the physical sciences is critical for
choosing an appropriate machine learning method. While invariant models
are the most prevalent symmetry-aware models, equivariant models can
more faithfully represent physical interactions. Until recently,
equivariant models had been absent in the literature due to their
technical complexity. Now, after two years of active development,
fully-equivariant Euclidean neural networks are ready to take on
challenges across the physical sciences.

### Symmetry enables free choice of coordinate system

There is no inherent way to orient physical systems; yet, we still need
to choose a coordinate system to articulate their geometry and physical
properties. Unless coded otherwise, machine learned models make no
assumption of symmetry and will be sensitive to an arbitrary choice of
coordinate system. In order to be able to recognize a 3D pattern in any
orientation, such a model will need to be shown roughly 500 rotated
versions of the same example, a process called data augmentation. One of
the motivations of explicitly treating symmetry in machine learning
models is to eliminate the need for data augmentation, so the model can
instead focus on e.g. learning quantum mechanics. In 3D space, we can
transform between coordinate systems using elements of Euclidean
symmetry (3D rotations, 3D translations, and inversion
$(x, y, z) \rightarrow (-x, -y, -z)$ which includes mirror symmetry);
hence we say that 3D space has Euclidean symmetry. One useful way of
categorizing machine learning models applied in the physical sciences is
by whether they employ symmetry and if so where they use invariant vs.
equivariant operations. Between the two types of symmetry-aware models,
invariant models get rid of coordinate systems by only dealing with
quantities that are invariant to the choice of coordinate system
(scalars), while equivariant models preserve how quantities predictably
change under coordinate transformations.

### Invariance vs. Equivariance

There is good reason for the popularity of invariant scalar features for
machine learning; you can give scalars to any machine learning algorithm
without violating symmetry. More practically, scalars are easier to
handle than geometric tensors and invariant models are high (if not top)
performers on many existing benchmarks. It is common practice to employ
equivariant operations to generate invariant features for machine
learning models. For example, SOAP descriptors are equivariant functions
that operate on the geometry and atom types of local atomic environments
to produce invariant scalars. Geometry in invariant models is reduced to
pairwise distances; Figure below describes the invariant vs. equivariant
properties of 3D vectors. Invariant models can yield equivariant
quantities, but only through gradients of the equivariant operations
used in featurization; this may not be practical or possible depending
on the quantity of interest.

### When is Equiavariance Critical

Physical systems and their interactions are inherently equivariant. With
an invariant model, you have to invent a means of representing your
naturally equivariant physical system in terms of invariant features.
With an equivariant model, you represent your system in the same way you
might articulate it to a physical simulation: with geometric coordinates
and any relevant quantities you need to describe the system, e.g.
external fields or atom-wise properties such as velocities. Even if you
want to predict a scalar, not all physical interactions can be
represented as scalars; e.g. the only way to interact a moving charged
particle with an external magnetic field is to use the equivariant
cross-product,$\vec\{F\}=q\vec\{v\}\times \vec\{B\}$, or the difference
between momentum and the charge weighted vector potential
$H=|\vec\{p\}-q\vec\{A\}|^2/2m$. To predict quantities that are
fundamentally generated from equivariant interactions you must either

1.  Include these equivariant interactions in the scalar featurization
    used for an invariant model (which requires knowing to include those
    interactions)

2.  use an equivariant model, which may make more accurate or effcient
    predictions because it has more expressive operations. The more
    exotic or complex a property, the more likely there are non-trivial
    geometric tensor interactions at play (multipole interactions,
    antisymmetric exchange, etc).

### Euclidean Neural Networks Are Equivariant Models

Neural networks are one of the most exible machine learning methods
since the only requirement is that the network be differentiable. A
neural network is a function $f$ that takes in inputs x and trainable
parameters $W$ to produce outputs y, $f(x;W) = y$. Given pairs of inputs
$x$ and target output $y_\{true\}$, you train a neural network by
computing derivatives of the loss, e.g. $\mathcal\{L\}$ with respect to
trainable parameters W and update according to a learning rate $\eta$,
$$W = W + \eta\frac\{\partial \mathcal\{L\}\}\{\partial W\}$$

First proposed in 2018, Euclidean neural networks (tensor field
networks, Clebsch-Gordan nets, 3d steerable CNNs, and their descendants)
are a exible, general framework for learning 3D Euclidean symmetry
equivariant functions that can be trained on the context of a given
dataset. To achieve equivariance in Euclidean Neural Networks, scalar
multiplication is replaced by the more general tensor product and
convolutional kernels are restricted to be composed of spherical
harmonics and learnable radial functions,
$$W(\vec\{r\})=R(|r|)Y_\{lm\}(\hat\{r\})$$ Scalar nonlinearities must also be
replaced with equivariant equivalents.

Due to these mathematical complexities, Euclidean neural networks can be
challenging to implement; however, there are open-source implementations
e.g. e3nn is an open-source PyTorch library that combines the
implementations of There are implementations of a variety of additional
equivariant layers and utility functions for converting and visualizing
geometric tensors. Equivariance has also been employed in other machine
learning approaches such as kernel methods by using equivariant kernels
and an equivariant definition of covariance. Euclidean neural networks
can be used to learn equivariant functions that generate scalar
invariants or equivariant kernels for use with traditional machine
learning methods. With Euclidean neural networks, you can build
end-to-end models for predicting physical properties (e.g. molecular
dynamics forces) from atomic geometries and initial atomic features
(e.g. atom types). You can use them to manipulate atomic geometries and
hierarchical features. The only difference is how you choose to compose
equivariant operations and learnable equivari- ant modules.

Euclidean Neural Networks provides a mathematically rigorous framework
for articulating scientific questions in terms of geometric tensors and
their tensor interactions. Inputs, outputs, and intermediate data are
completely specified by their transformation properties. Geometric
tensors take many forms and can represent many different quantities:
numerical geometric tensors, atomic orbitals, or projections of
geometry; Another particularly useful aspect of handling Euclidean
symmetry in full generality is that you get all subgroup symmetries
(e.g. point groups and space groups) for free. Illustration below shows
an example of how the output of even randomly initialized Euclidean
Neural Networks will have equal or higher symmetry than the input. Since
these networks intrinsically uphold any and all selection rules that
occur in physical systems, they act as \"symmetry compilers\" that check
your thinking about the data types of your physical system. Using these
models requires more forethought than a traditional neural network; in
exchange, they cannot learn to do something that doesn't symmetrically
make sense.

### Sperical Harmonics

### Cormorant

Risi Kondor

Cormorant is a rotationally covariant neural network architecture for
learning the behavior and properties of complex many-body physical
systems. These networks have been applied to molecular systems with two
goals: learning atomic potential energy surfaces for use in Molecular
Dynamics simulations, and learning ground state properties of molecules
calculated by Density Functional Theory. Some of the key features of our
network are that (a) each neuron explicitly corresponds to a subset of
atoms; (b) the activation of each neuron is covariant to rotations,
ensuring that overall the network is fully rotationally invariant.
Furthermore, the non-linearity in our network is based upon tensor
products and the ClebschGordan decomposition, allowing the network to
operate entirely in Fourier space. Cormorant significantly outperforms
competing algorithms in learning molecular Potential Energy Surfaces
from conformational geometries in the MD-17 dataset, and is competitive
with other methods at learning geometric, energetic,

### 3D Steerable CNNs

Taco Cohen, Max Welling.

3D Steerable CNNs are convolutional networks that are equivariant to
rigid body motions. The model uses scalar-, vector-, and tensor fields
over 3D Euclidean space to represent data, and equivariant convolutions
to map between such representations. These SE(3)-equivariant
convolutions utilize kernels which are parameterized as a linear
combination of a complete steerable kernel basis, which have been
derived analytically. It can be proven that equivariant convolutions are
the most general equivariant linear maps between fields over R3.
Experimental results confirm the effectiveness of 3D Steerable CNNs for
the problem of amino acid propensity prediction and protein structure
classification, both of which have inherent SE(3) symmetry.

### Tensor field networks

Li Li, Tess and Pfr

Tensor field neural networks are locally equivariant to 3D rotations,
translations, and permutations of points at every layer. 3D rotation
equivariance removes the need for data augmentation to identify features
in arbitrary orientations. Tensor field neural networks use filters
built from spherical harmonics; due to the mathematical consequences of
this filter choice, each layer accepts as input (and guarantees as
output) scalars, vectors, and higher-order tensors, in the geometric
sense of these terms.

### Pseudocode: E3NN

    def R(r: $\mathbb\{R\}$, rmax: $\mathbb\{R\}$, params: $\mathbb\{R\}^3$) -> $\mathbb\{R\}$
      if r > rmax:
        return 0

      c, pow = params[1:2]
      res = r ≈ 0 ? 0 : c / r^pow

      c = params[3]
      res += r ≈ 0 ? c : 0
      return res



    REPS = (scalar = (0, 1), vector = (1, -1), pseudovector = (1, 1))

    function Ylm(u, l)
        x, y, z = u
        r2 = norm(u)^2
        if l == 0
            return [1]
        elseif l == 1
            return [u[1], u[3], u[2]]
        elseif l == 2
            return [x * y / r2, y * z / r2, (x^2 - y^2 + 2z^2) / (2 * sqrt(3) * r2), z * x / r2, (x^2 - y^2) / (2r2)]
        end

        # TODO higher orders

        return zeros(2l + 1)
    end

    function product(u, Y, dimout)
        dim1 = length(u)
        dim2 = length(Y)

        v = Y
        if dim2 == 3
            v = [Y[1], Y[3], Y[2]]
        end

        if dim1 == 1
            return u[1] * v
        elseif dim2 == 1
            return u * v[1]
        elseif dim1 == dim2 && dimout == 1
            return [dot(u, v)]
        elseif dim1 == dim2 == dimout == 3
            return cross(u, v)
        end

        # clebsch-gordan for higher orders
        return zeros(dimout)
    end



    function EquivConv(intypes, outtypes, rmax; paramdensity = 8, σ = tanh)
        # numout=lmax=0
        # ignore() do
        intypes = [typeof(x) == Symbol ? REPS[x] : x for x in intypes]
        outtypes = [typeof(x) == Symbol ? REPS[x] : x for x in outtypes]
        l_i = [x[1] for x in intypes]
        l_o = [x[1] for x in outtypes]
        nin = length(intypes)
        nout = length(outtypes)

        paths_l = Dict([(li, sort(unique(vcat([abs(lo - li):lo+li for lo in unique(l_o)]...)))) for li in unique(l_i)])

        start_i = cumsum([1, [2l_i[i] + 1 for i = 1:nin-1]...])
        start_o = cumsum([1, [2l_o[i] + 1 for i = 1:nout-1]...])
        numout = start_o[end] + 2l_o[end]

        lfmax = maximum(l_i) + maximum(l_o)
        lfrange_io = [abs(l_i[i] - l_o[o]):(l_i[i]+l_o[o]) for i = 1:nin, o = 1:nout]
        nlf_io = length.(lfrange_io)

        radparamspos_io = zeros(nin, nout)
        pos = 0
        for i = 1:nin
            for o = 1:nout
                radparamspos_io[i, o] = pos
                pos += nlf_io[i, o]
            end
        end
        radparams = rand((pos) * paramdensity)
        nlparams = rand(2nout)

        lolist = 0
        Zygote.ignore() do
            lolist = unique(l_o)
        end


        function getradparams(i, o, lf)
            start = 1 + Int(paramdensity * (radparamspos_io[i, o] + lf - abs(l_i[i] - l_o[o])))
            return radparams[start:start+paramdensity-1]
        end

        function conv(X, points)
            npoints = size(points, 1)
            Xout = zeros(npoints, numout)


            Y_l_ab = rvec_ab = r_ab = rhat_ab = 0
            Zygote.ignore() do
                # rvec_ab = [[points[b, i] - points[a, i] for a = 1:npoints] for b = 1:npoints, i = 1:3]
                # r_ab = [norm(rvec_ab[a, b, :]) for a = 1:npoints, b = 1:npoints]
                # rhat_ab = [a == b ? 0 : rvec_ab[a, b, i] / r_ab[a, b] for a = 1:npoints, b = 1:npoints, i = 1:3]
                rvec_ab = [points[b, i] - points[a, i] for a = 1:npoints, b = 1:npoints, i = 1:3]
                r_ab = [norm(rvec_ab[a, b, :]) for a = 1:npoints, b = 1:npoints]
                rhat_ab = [a == b ? 0 : rvec_ab[a, b, i] / r_ab[a, b] for a = 1:npoints, b = 1:npoints, i = 1:3]

                Y_l_ab = Dict()
                for l = 0:lfmax
                    Y_l_ab[l] = [Ylm(rhat_ab[a, b, :], l) for a = 1:npoints, b = 1:npoints]
                    # Y_l_ab[l] = [Ylm(rhat_ab[a, b, :], l) for a = 1:npoints, b = 1:npoints]
                end
            end

            product_i_lf_lo_ab = Dict()
            for i = 1:nin
                product_i_lf_lo_ab[i] = Dict()
                li = l_i[i]
                for lf in paths_l[li]
                    product_i_lf_lo_ab[i][lf] = Dict()
                    for lo = abs(li - lf):li+lf
                        if lo in lolist
                            product_i_lf_lo_ab[i][lf][lo] = [[product(X[b, start_i[i]:start_i[i]+2li], Y_l_ab[lf][a, b], 2lo + 1) for b = 1:npoints] for a = 1:npoints]
                            # product_i_lf_lo_ab[i][lf][lo] = [product(X[b, start_i[i]:start_i[i]+2li], Y_l_ab[lf][a, b], 2lo + 1) for a = 1:npoints, b = 1:npoints]
                        end
                    end
                end
            end

            result = Dict()
            for o = 1:nout
                result[o] = [sum([sum([sum([product_i_lf_lo_ab[i][lf][l_o[o]][a][b] * R(r_ab[a, b], rmax, getradparams(i, o, lf)) for lf in paths_l[l_i[i]]]) for i = 1:nin]) for b = 1:npoints]) for a = 1:npoints]
                # result[o] = [sum([sum([sum([product_i_lf_lo_ab[i][lf][l_o[o]][a, b] * R(r_ab[a, b], rmax, getradparams(i, o, lf)) for lf in paths_l[l_i[i]]]) for i = 1:nin]) for b = 1:npoints]) for a = 1:npoints]
            end
            o_p = m_p = 0
            Zygote.ignore() do
                o_p = [searchsortedfirst(start_o, p + 1) - 1 for p = 1:numout]
                m_p = [p - start_o[o_p[p]] for p = 1:numout]
            end
        
            result = [[result[o_p[p]][a][o_p[p]+m_p[p]] for a = 1:npoints] for p = 1:numout]
            result = hcat(result...)
            radparams
            nlparams
            return result
        end
        return conv
    end
