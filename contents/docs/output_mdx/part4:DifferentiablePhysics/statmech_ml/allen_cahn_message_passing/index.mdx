# Allen-Cahn Message Passing with Particle-Phase Message Passing \{#chap:allen_cahn\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:eft\]](#chap:eft)\{reference-type="ref+label"
reference="chap:eft"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Message passing neural networks can become difficult to train as
networks get very deep.

$$\begin\{aligned\}
m^\{t+1\}_v &= AGG_\{w \in N(v)\}(M(h^t_v, h^t_w, h^t_\{e(v,w)\}))\\
h^\{t+1\}_v &= U(h^t_v, m^\{t+1\}_v) \\
h_G &= READOUT(\{h^L_v | v \in G \})
\end\{aligned\}$$

## Adddressing Oversmoothing

Graph convolutions in general can be rewritten as message passing. These
models in general suffer from \"over-smoothing.\"

The GRAND model can help improve over-smoothing. We can borrow ideas
from the study of interacting particle systems. Such systems can be
modeled used differential equations. Here, let $x_i$ be the $i$-th
particle and $a_\{i,j\}$ the attractive force strength

$$\begin\{aligned\}
x_i &= \sum_\{j: (i, j) \in X\} a_\{i,j\}(x_j - x_i) \\
\end\{aligned\}$$

You can also add phase transitions here. The graph diffusion scheme can
be viewed as the gradient flow of the Dirichlet energy.

$$\begin\{aligned\}
E(x) &= \frac\{1\}\{N\} \sum_\{i \in V\} \sum_\{j \in N(i)\} a_\{i,j\} \|x_i - x_j\|^2
\end\{aligned\}$$

## Defining Allen-Cahn Message Passing Equations

The Allen-Cahn message passing (ACMP) equation is given by
$$\begin\{aligned\}
\frac\{\partial\}\{\partial t\} x_i(t) &= \alpha \odot \sum_\{j \in N(i)\} a_\{i,j\} (x_j - x_i) + \delta \odot x_i (1 - x_i \odot x_i)
\end\{aligned\}$$ The repulsive structure prevents over-smoothing while
the Allen-Cahn structure prevents blow-up. The model is learned using a
neural-ODE solver.

The ACMP method can do better on \"heterophilic\" datasets as opposed to
\"homophilic\" datasets. In homophilic datasets, nodes with similar
labels cluster together while heterophilic datasets don't usually have
neighbors have the same label.

$$\begin\{aligned\}
x(T) &= x(0) + \int_0^T \frac\{\partial x(t)\}\{\partial t\} dt \\
x(0) &= MLP(x^\{in\})
\end\{aligned\}$$

The Dormand-Prince5 integrator is used to stably implement the
integration. Oversmoothing is defined as collapse of Dirichlet energy in
deeper layers.
