# Tensor Network Machine Learning \{#chap:quantum_rl\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},
[\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\},
[\[chap:hartree_fock\]](#chap:hartree_fock)\{reference-type="ref+label"
reference="chap:hartree_fock"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Tensor networks offer a framework for representing high dimensional
tensors effectively. These techniques have found traction in quantum
many-body physics where they are used to represent composite states in a
computationally efficient representation. The techniques of tensor
networks can also be used to define architectures suitable for broader
machine learning. In effect, these methods provide a structured method
for constructing a very large feature space that can be coupled with a
downstream classification architecture [@stoudenmire2016supervised],
[@convy2021mutual]

## Mathematical Foundations

A linear model can be represented abstractly by the equation

$$\begin\{aligned\}
    f(x) &= W \cdot \Phi(x)
\end\{aligned\}$$ where $\Phi(x)$ is a feature vector. For many of the
differentiable programming applications we have seen in the book, $\Phi$
is a fully connected neural network. We have leveraged the universal
appproximation capabilities of neural networks for a variety of
applications, and in particular for providing rich ansatzes for quantum
calculations.

In a separate line of work, physicists have spent decades developing
complex ansatzes for multibody quantum systems. In particular, tensor
networks provide a family of functions that have proven useful
decompositions of wave functions. A simple tensor network computes a
per-coordinate breakdown of the wave function. A very simple example of
a tensor network (with no interactions) is given below:

$$\begin\{aligned\}
    \Phi^\{s_1\dotsc s_N\}(x) &= \phi^\{s_1\}(x_1)\dotsc \phi^\{s_N\}(x_N)
\end\{aligned\}$$

![The Matrix Product State
Decomposition](figures/Differentiable Physics/Deep Quantum/Tensor Network Machine Learning/MPS Decomposition.png)\{#fig:enter-label\}

More complex interaction structures can be computed systematically for
larger tensor networks. Recent work has realized that the physical
notion of tensor network can feedback into machine learning
applications. Tensor networks provide a rich mathematical structure for
feature extraction from arbitrary data thatt can prove useful in
different contexts.

## Choice of Network Structure

The choice of network decomposition can dramatically influence the
effectiveness of a tensor network algorithm. The matrix product state
(MPS) approximation provides one standard ansatz for efficiently
representing a tensor decomposition.

Other anzatses included the projected entangled pair states (PEPS) and
the multiscale entanglement renormalization ansatz (MERA).
