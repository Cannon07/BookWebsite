# Gaussian Processes \{#chap:gaussian_processes\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\},
[\[chap:multidimensional\]](#chap:multidimensional)\{reference-type="ref+label"
reference="chap:multidimensional"\},
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Gaussian processes provide a powerful class of methods for modeling time
series data that competes broadly with RNNs but offers much tighter
theoretical control

## Intuition

The core idea of a Gaussian process is that a dataset $$\begin\{aligned\}
\mathcal\{D\} &= \{(x_1, y_1),\dotsc, (x_n, y_n)\}
\end\{aligned\}$$ corresponds to a family of functions which pass through
these points. By using as assumption of Gaussianity, we can place a
reasonable prior distribution on the family of all functions which pass
through the points in $\mathcal\{D\}$. In order for this prior to make
sense at points not in $\{x_i\}$, we need to make a choice of a kernel
function $K$ that tells us how to compute the probability of values $y$
at some point $x$ not in the training distribution.

In order to do prediction at $x$, we have to compute the conditional
Gaussian distribution given $\mathcal\{D\}$ and $K$. This computation can
become expensive, so there are number of methods to compute approximate
Gaussian process regressors.


# Kernel Methods \{#chap:kernel_methods\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:hilbert\]](#chap:hilbert)\{reference-type="ref+label"
reference="chap:hilbert"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Kernel methods offer a structured method of adding nonlinear features to
linear regression. The basic intuition behind a kernel method is to
compare a given query point $x$ against known datapoints $x_i$ to
determine prediction $y$. For example, suppose that we have the training
dataset

$$\begin\{aligned\}
\mathcal\{D\} &= \{(x_i, y_i) \}
\end\{aligned\}$$

Then to make a prediction at query point $x$, we predict
$$\begin\{aligned\}
    y &= \sum_i K(x, x_i) y_i
\end\{aligned\}$$ here $K$ is some distance function capturing the notion
of distance between $x$ and $x_i$. Points closer to $x_i$ will make
predictions closer to $y_i$. Many possible families of kernel functions
$K$ exist. Note that in the case $K$ only depends on $x$ and not $x_i$,
kernel methods are exactly the same as linear regression.

## Radial Basis Function Kernels

The radial basis function (RBF) kernels are a broadly used family of
nonlinear kernels

$$\begin\{aligned\}
K(x, x') &= \exp \left ( - \frac\{\|x - x'\|^2\}\{2 \sigma^2\} \right )
\end\{aligned\}$$ Here $\sigma$ is a hyperparameter for the kernel. RBF
kernels are closely related to Gaussians, and arise in many physical
systems.

    def rbf($X: \mathbb\{R\}^\{M \times N\}$, $\sigma$) $\to$ $\mathbb\{R\}^\{M \times M\}$:
      K: $\mathbb\{R\}^\{M \times M\}$
      for i j
        K[i,j] = exp(-$\|X[i]-X[j]\|^2$/($2\sigma^2$))
      return K

## Efficiency Considerations

Kernel methods require the computation of $K$ for every datapoint in the
training dataset. For large datasets, the expense can become
prohibitive. For this reason, a vast literature of approximate kernel
methods have emerged. These methods typically attempt to factor the
matrix $K(x_i, x_j)$ into smaller matrices and use these matrix
factorizations to allow for faster query answers.

## Exercises

1.  Prove that linear regression is a special case of kernel regression
    for a suitable kernel $K$.


# Multilayer Perceptrons \{#chap:mlpd\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

The limitation of linear models is expressivity. There are many complex
interactions which can't be modeled simply with linear interactions.
Practitioners often bypass this limitation by adding in new \"features\"
that encode pairwise interactions, but ideally we would like a model
that can natively learn more complex interactions. We want to add some
nonlinearity to our function class $\mathcal\{G\}$. Let $\sigma$ be some
nonlinear function. Then we can define a model by

$$\begin\{aligned\}
h[w,b](x) = \sigma(w^T x + b) 
\end\{aligned\}$$

There are many choices commonly made for the function $\sigma$. One
common choice is the sigmoidal function

$$\begin\{aligned\}
\sigma(x) = \frac\{1\}\{1+ e^\{-x\}\}
\end\{aligned\}$$

Note that this function ranges from 0 to 1 (see
FigureÂ [\[fig:logistic\]](#fig:logistic)\{reference-type="ref"
reference="fig:logistic"\}). We can turn this definition into code.

::: marginfigure
![image](figures/Differentiable Models/multilayer_perceptrons/Logisticurve0.png)
:::

``` \{.python language="python"\}
def $\sigma$(x: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}$:
  1/(1 + exp(-x))

class h(w: $\mathbb\{R\}^n$, b: $\mathbb\{R\}$):
  def $\lambda$(x: $\mathbb\{R\}^n$) $\to$ $\mathbb\{R\}$:
    $\sigma$(w$^T$ * x + b)
```

With this choice of $\sigma$, the function $h_\{(w,b)\}$ is commonly
referred to as a \"perceptron.\" It corresponds to a very crude model of
a neuron. The core idea is that this function corresponds to the
following \"firing\" rule

$$\begin\{aligned\}
 h[b](x) = \begin\{cases\}
    1 & \sigma(w^T x + b) > 0.5 \\
    0 & \textrm\{otherwise\} \\
    \end\{cases\} 
\end\{aligned\}$$

Perceptrons spurred a lot of excitement when they were first introduced
in the 1950s. Minsky and Papert showed in their book \"Perceptrons\"
[@minsky2017perceptrons] that perceptrons were incapable of learning
simple functions such as XOR. This caused a broad loss of interest in
Perceptrons and neural networks more broadly. Interest only revived when
researchers realized that many of the limitations of perceptrons could
be overcome by chaining them into multiple layers.

To implement multilayer perceptrons, we alter our function $h[w, b]$ to
return a vector of outputs rather than a scalar. Let
$W \in \mathbb\{R\}^\{P \times M\}$ and let $c \in \mathbb\{R\}^\{P\}$. Then we
defined $h[W, c]: \mathbb\{R\}^M \to \mathbb\{R\}^P$ as

$$\begin\{aligned\}
 h[W, c](x) = W x + c 
\end\{aligned\}$$

We then introduce a convenient bit of notation. If $x$ is a vector, we
let $\sigma(x)$ denote the component wise application of $\sigma$ to
every scalar in the vector. We're now ready to introduce a simple
multilayer perceptron

$$\begin\{aligned\}
 h[W_0, c_0, w_1, b_1](x) = \sigma(w_1^T\sigma(W_0 x + c_0) + b_1) 
\end\{aligned\}$$

<figure id="fig:my_label">
<p><img
src="figures/Differentiable Models/multilayer_perceptrons/mlp_example_0.png"
alt="image" /> <span id="fig:my_label"
data-label="fig:my_label"></span></p>
</figure>

We say this this model has one *hidden layer* since the inner layer
$\sigma(W_0x + c_0)$ isn't directly reflected in the output. We can
transform this definition into Physika code in a straightforward
fashion.

    class OneLayerNet($W_0$: $\mathbb\{R\}$[N], $w_1$: $\mathbb\{R\}$, $b_0$: $\mathbb\{R\}$, $b_1$: $\mathbb\{R\}$):
      def $\lambda$(x: $\mathbb\{R\}$[N]) $\to$ $\mathbb\{R\}$:
        $\sigma$($w_1$*$\sigma$($W_0$*x+$b_0$)+$b_1$)

There's no reason we have to stop at just one layer here. We can go
deeper. Here's an example with two hidden layers for example.

$$h[W_0, c_0, W_1, c_1, w_2, b_2](x) = \sigma(w_1^T\sigma(W_1 \sigma(W_0 x + c_0) + c_1) + b_1)$$

You might now start to see where the term *deep learning* originates
from. As we keep adding more and more hidden layers, we make the model
deeper. Let's take a look at some simple Physika code for a $n$ hidden
layer network.

``` \{.python language="python"\}
class FullyConnectedNetwork($\sigma$: $\mathbb\{R\}$ $\to$ $\mathbb\{R\}$, W: $\mathbb\{R\}$[n, N, N], B: $\mathbb\{R\}$[n, N], w: $\mathbb\{R\}^N$, b: $\mathbb\{R\}$):
  def $\lambda$(x: $\mathbb\{R\}$[N]) $\to$ $\mathbb\{R\}$:
    for i: x = $\sigma$(W[i] * x + B[i])
    w$^T$ x + b
```
