# Universal Approximation Theorem \{#chap:univ_approx\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

The universal approximation theorem is one of the most foundational
theoretical results in deep learning. It proves that arbitrary functions
can be approximated by fully connected deep networks. There are a number
of extensions of the universal approximation theorem to other families
of neural networks that are also useful.

## The Universal Approximation Theorem for Fully Connected Networks

Start with the one layer network. Let $C(X, Y)$ denote the set of
continuous functions from $X$ to $Y$, where $X$ is a compact space. Then
it has been shown that sums of the form

$$\begin\{aligned\}
\sum_\{j=1\}^N a_j\sigma(w_jx + b_j)
\end\{aligned\}$$ can approximate any function in $C(X, Y)$ arbitrarily
well for suitable choices of parameters. The proof of the theorem draws
upon the fact that Fourier series offer good approximations to functions
in $C(X, Y)$

## The Universal Approximation Theorem for Convolutional Networks

Convolutional networks form a subset of fully connected networks with a
special Toeplitz matrix structure for the weight matrix. A priori, it is
not necessarily obvious a universal approximation theorem would hold for
such systems, but this has been proven to be the case.

## The Universal Approximation Theorem for Recurrent Neural Networks

Any open dynamic system can be approximated with arbitrary accuracy by a
recurrent neural network. Here an open dynamic system is given by a set
of update equations

$$\begin\{aligned\}
s_\{t+1\} &= g(s_t, x_t)\\
y_t &= h(s_t)
\end\{aligned\}$$

where $x_t$ is the input at time $t$, $s_t$ is the state at time $t$ and
$y_t$ is the output at time $t$.

## The Universal Approximation Theorem for Graph Networks

Function approximation on graphs requires a slightly more complicated
framework to specify precisely. Two different graphs can be isomorphic;
that is there can exist a map $$\begin\{aligned\}
\phi: G \to H
\end\{aligned\}$$ between graphs $G$ and $H$ that preserves edges and
labels on vertices. The universal approximation theorem for graphs
states that any function that has the same value on isomorphic graphs
can be approximated by a suitable graph neural network.


# Deep Networks Tend to Gaussian Processes \{#chap:ntk\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

Consider a fully connected neural network with $L$ hidden layers. Let
the $\ell$-th layer have $N_\ell$ hidden units, bias $b^\ell$ and
nonlinearity $\phi$. $x \in \mathbb\{R\}^d$ is the input and $z^\{\ell\}$ is
the output of the $\ell$-th layer. One of the most important
mathematical theorems in deep learning proves that as
$N_\ell \to \infty$, the deep neural network converges to a Gaussian
process.

## A Single Hidden Layer Network is a Gaussian Process

The $i$-th component output $z_i$ of a single hidden layer network is
given by $$\begin\{aligned\}
a_j &= \sigma \left ( b^0_j + \sum_\{k=1\}^d W^0_\{jk\} x_k \right ) \\
z_i &= b^1_i + \sum_\{j=1\}^\{N\} W^1_\{ij\} a_j 
\end\{aligned\}$$

Assume that weight parameters $W^0_\{ij\}, W^1_\{ij\}$ and bias parameters
$b^0_j, b^1_j$ are given by independent, identically distributed random
draws from a Gaussian distribution. We can treat inputs $x_k$ as
constants. Then note that the sum term $$\begin\{aligned\}
b^0_j + \sum_\{k=1\}^d W^0_\{jk\} x_k
\end\{aligned\}$$ itself follows a Gaussian distribution, and $a_j$ is
drawn from the nonlinear transformation $\sigma$ of this Gaussian. Then
it follows that $a_j$ and $a_\{j'\}$ are independent and identically
distributed when $j \neq j'$ since the summation terms draw from
independent distributions (note that $x_k$ are constant terms so it does
not matter that they are shared.)

The output term $z_i$ is then the sum of i.i.d terms. As $N \to \infty$,
it follows from the central limit theorem that $z_i$ will converge to a
Gaussian distribution (we can ignore the bias term as $N \to \infty$).

It follows then that $$\begin\{aligned\}
z^1_i \sim \mathcal\{GP\}(\mu^1, K^1)
\end\{aligned\}$$ is given by a Gaussian process since any collection of
these terms for different $i$ forms a multivariate Gaussian
distribution. Assuming that random parameters have mean $0$, then,
$\mu^1 = 0$. The covariance is given by

$$\begin\{aligned\}
K(x, x') &= \mathbb\{E\}[z_i(x)z_i(x')] \\
\end\{aligned\}$$

## Deep Neural Networks are Gaussian Processes

An argument by induction can prove that as we add additional layers, a
deeper neural network still converges to a Gaussian process as the width
goes to infinity.


# Neural Tangent Kernel \{#chap:ntk\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

The neural tangent kernel provides a theoretical methodology to study an
\"infinite-width\" approximation of a fully connected network. Even more
interestingly, the evolution of a neural network under gradient descent
can also be described by a suitable kernel.

## Introduction to Neural Tangent Kernels

A recursion relation on neural networks can be mapped over to a relation
on Gaussian processes. For a single layer network, we can write

$$\begin\{aligned\}
f_i^0(x) &= b_i^0 + \sum_j W^0_\{ij\} x_j \\
f_i^1(x) &= b_i^1 + \sum_j \sigma(W^0_\{ij\}) x_j \\
K^0(x, x') &= E[f_j(x)f_j(x')]
\end\{aligned\}$$

Let $f$ denote the output of the full network. For simplicity, assume
that $f$ has a one dimensional output. Let $\theta_j$ denote the $j$-th
learnable parameter. We then define the tangent kernel as

$$\begin\{aligned\}
\Theta(x, x') &= \sum_j \frac\{\partial f(x)\}\{\partial \theta_j\} \frac\{\partial f(x')\}\{\partial \theta_j\}
\end\{aligned\}$$

Note that the type of this kernel is $$\begin\{aligned\}
\Theta: \mathbb\{R\}^n \times \mathbb\{R\}^n \to \mathbb\{R\}
\end\{aligned\}$$ where $x, y \in \mathbb\{R\}^n$.

The neural tangent kernel lets us model the dynamics of the neural
network during training. Let us define a dataset

$$\begin\{aligned\}
D &= \{(x_1, y_i),\dotsc, (x_N, y_N)\}
\end\{aligned\}$$

We can then model the time evolution of the output of the neural network
$f$ by the function

$$\begin\{aligned\}
\frac\{df(x; \theta)\}\{dt\} &= - \sum_\{i=1\}^N \Theta(x, x_i) \frac\{\partial \mathcal\{L\}(f(x_i; \theta), y_i)\}\{\partial f(x_i; \theta)\} 
\end\{aligned\}$$

We can interpret the term $\Theta(x, x_i)$ as a measure of the influence
of the gradient of the loss on the $i$-th datapoint $(x_i, y_i)$ on the
output of the model. To first order, the evolution of the neural network
during training g is given by the linear model $$\begin\{aligned\}
f(x; \theta(t)) &= f(x; \theta(0)) + \sum_j \frac\{\partial f(x; \theta_j(0))\}\{\partial \theta_j\} (\theta_j(t) - \theta_j(0))
\end\{aligned\}$$

As the width of the neural network goes to infinity, its training
behavior converges to this linear model. Note that for a finite width
neural network, the neural tangent kernel $\Theta$ is a random quantity
since it depends on the initialization of $\theta$. However for an
infinitely wide network, the neural tangent kernel converges to a
deterministic quantity.

We can also compute the evolution of the parameters at a general point
$x$. There is a Gaussian process that we can write down that describes
the evolution of this system as a point process. This is called the NNGP
kernel as opposed to the NNTK kernel.

When are NNTK/NNGP methods useful? They serve as a useful starting point
for perturbation theoretic expansions. NNTK methods can also be
competitive with neural network methods in their own right as predictive
models.
