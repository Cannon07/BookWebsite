# Neural Radiance Fields \{#chap:nerfs\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},
[\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\},
[\[chap:hartree_fock\]](#chap:hartree_fock)\{reference-type="ref+label"
reference="chap:hartree_fock"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Neural radiance fields (NeRFs) provide a new methodology for
transforming input coordinates into volume densities and radiances. The
core mathematical transformation is given by

$$\begin\{aligned\}
(x, y, z, \theta, \phi) \mapsto (R,G, B, \sigma)
\end\{aligned\}$$

Here $x,y,z$ are spatial coordinates, and $(\theta, \phi)$ are viewing
angles. This transformation is typically performed by a fully connected
network $f_\theta$. The NeRF provides dramatic improvements in view
synthesis, allowing for a global view to be learned from a collection of
input images with known orientations $$\begin\{aligned\}
\mathcal\{D\} &= \{ x_i\} \\
x_i &\in \mathbb\{R\}^\{m \times n\}
\end\{aligned\}$$ The core idea is that rays of light can be sent
backwards from the image data to compute a representation of the
radiance at any given point. This radiance can be used to project out
new sampled images from new orientations.
