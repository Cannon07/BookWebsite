# Introduction to Scientific Machine Learning \{#ch:intro-molecular-ml\}

While applying machine learning algorithms to physical systems, there
typically are additional constraints in the form of respecting any given
laws of physics often described by general nonlinear partial
differential equations. In this chapter, we will introduce approaches to
incorporate such additional constraints into machine learning
algorithms. We will also discuss approaches to discover and learn the
underlying partial differential equations for systems where the
underlying physics can emerge from collected data.

This chapter will cover the three broad ways in which one can impose the
known physics and symmetries in the physical systems. Often, a
combination of the three broad verticals are employed.

-   Chapter Part-1: Hard-code the known governing equations and boundary
    conditions in the **loss function** while training the
    physics-driven model, which comprises of approaches that fall under
    the category of **physics-informed models**

-   Chapter Part-2: Build in the physics-based equations and constraints
    **within the model architecture** so that intermediate variables are
    physically motivated

-   Chapter Part-3: Impose known symmetries of the system that need to
    be obeyed **within the featurizer(s)** before feeding into the model

## Physics-informed Machine Learning

Let's begin with discussing physics-informed machine learning using
neural networks. The approach by Raissi et al.[@raissi2019physics]
employs deep neural networks to leverage their well known capability as
universal function approximators. This facilitatest the ability to
directly tackle nonlinear problems without the need for committing to
any prior assumptions, linearization, or local time-stepping. In
addition we can exploit recent developments in automatic differentiation
-- one of the most useful but perhaps under-utilized techniques in
scientific computing -- to differentiate neural networks with respect to
their input coordinates and model parameters to obtain physics-informed
neural networks. Such neural networks are constrained to respect any
symmetries, invariances, or conservation principles originating from the
physical laws that govern the observed data, as modeled by general
time-dependent and nonlinear partial differential equations. This simple
yet powerful construction allows us to tackle a wide range of problems
in computational science and introduces a potentially transformative
technology leading to the development of new data-efficient and
physics-informed learning machines, new classes of numerical solvers for
partial differential equations, as well as new data-driven approaches
for model inversion and systems identification. The approach sets the
foundations for a new paradigm in modeling and computation that enriches
deep learning with the longstanding developments in mathematical
physics.

### Problem Formulation

A parametrized and nonlinear partial differential equations of the
following general form are considered
$$u_t + \mathcal\{N\}[u;\lambda]=0, x \in \Omega, t\in [0,T],$$ where
$u(t, x)$ denotes the hidden solution, $\mathcal\{N[\cdot;\lambda]\}$ is a
nonlinear operator parametrized by $\lambda$, and $\Omega$ is a subset
of $\mathbb\{R\}^D$. This formulation applies to a wide range of problems
in physical systems including conservation laws, diffusion processes,
kinetic equations and physical systems.

### Data-driven solutions of partial differential equations

Let's begin with solutions to formulations of the general form
$$u_t + \mathcal\{N\}[u]=0, x \in \Omega, t\in [0,T],
    \label\{eq_generalform\}$$ where $u(t, x)$ denotes the hidden
solution, $\mathcal\{N[\cdot]\}$ is a nonlinear operator, and $\Omega$ is
a subset of $\mathbb\{R\}^D$.

The approach discussed below pertains to an algorithm that can be used
on **continuous time models**.

Let's define $f(t, x)$ to be given by the equation on the left hand
side,

$$f:=u_t + \mathcal\{N\}[u]
    \label\{eq_contTime\}$$

and the approach is to approximate $u(t,x)$ by a deep neural network.
This assumption along with equation
[\[eq_contTime\]](#eq_contTime)\{reference-type="ref"
reference="eq_contTime"\} gives rise to the physics-informed neural
network $f(t,x)$. The network can be derived by applying the chain rule
for differentiating compositions of functions using automatic
differentiation, and has the same parameters as the network representing
$u(t, x)$, albeit with different activation functions due to the action
of the differential operator $\mathcal\{N\}$. The shared parameters
between the neural networks $u(t, x)$ and $f(t, x)$ can be learned by
minimizing the mean-squared error loss

$$MSE=MSE_u + MSE_f$$

where $$MSE_u=\frac\{1\}\{N_u\} \sum_\{i=1\}^\{N_u\}|u(t_u^i,x_u^i)-u^i|^2$$

and $$MSE_f=\frac\{1\}\{N_f\} \sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2$$

In the problem formulation, the initial and boundary training data on
$u(t, x)$ are denoted by $\{\{t^i_u, x^i_u, u^i\}\}^\{N_u\}_\{i=1\}$, and
$\{\{t^i_f, x^i_f\}\}^\{N_f\}_\{i=1\}$ specify the collocation points for
$f(t, x)$. The loss function $MSE_u$ corresponds to the initial and
boundary data while $MSE_f$ enforces the structure imposed by the
general equation (equation
[\[eq_generalform\]](#eq_generalform)\{reference-type="ref"
reference="eq_generalform"\}) at a finite set of collocation points.

::: example
This example aims to highlight the ability of The method to handle
periodic boundary conditions, complex-valued solutions, as well as
different types of nonlinearities in the governing partial differential
equations. The nonlinear Schr$\mathrm\{\ddot\{o\}\}$dinger equation along
with periodic boundary conditions is given by

$$\begin\{aligned\}
\begin\{split\}
i h_t + 0.5 h_\{xx\} + |h|^2 h = 0,\ \ \ x \in [-5, 5],\ \ \ t \in [0, \pi/2],\\
h(0,x) = 2\ \mathrm\{sech\}(x),\\
h(t,-5) = h(t, 5),\\
h_x(t,-5) = h_x(t, 5),
\end\{split\}
\end\{aligned\}$$

where $h(t,x)$ is the complex-valued solution. Let us define $f(t,x)$ to
be given by

$$f := i h_t + 0.5 h_\{xx\} + |h|^2 h,$$

and proceed by placing a complex-valued neural network prior on
$h(t,x)$. In fact, if $u$ denotes the real part of $h$ and $v$ is the
imaginary part, we are placing a multi-out neural network prior on
$h(t,x) = \begin\{bmatrix\}
u(t,x) & v(t,x)
\end\{bmatrix\}$. This will result in the complex-valued (multi-output)
physics informed neural network $f(t,x)$. The shared parameters of the
neural networks $h(t,x)$ and $f(t,x)$ can be learned by minimizing the
mean squared error loss

$$MSE = MSE_0 + MSE_b + MSE_f$$

where

$$MSE_0 = \frac\{1\}\{N_0\}\sum_\{i=1\}^\{N_0\} |h(0,x_0^i) - h^i_0|^2$$

$$MSE_b = \frac\{1\}\{N_b\}\sum_\{i=1\}^\{N_b\} \left(|h^i(t^i_b,-5) - h^i(t^i_b,5)|^2 + |h^i_x(t^i_b,-5) - h^i_x(t^i_b,5)|^2\right)$$

and

$$MSE_f = \frac\{1\}\{N_f\}\sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2.$$

Here, $\{x_0^i, h^i_0\}_\{i=1\}^\{N_0\}$ denotes the initial data,
$\{t^i_b\}_\{i=1\}^\{N_b\}$ corresponds to the collocation points on the
boundary, and $\{t_f^i,x_f^i\}_\{i=1\}^\{N_f\}$ represents the collocation
points on $f(t,x)$. Consequently, $MSE_0$ corresponds to the loss on the
initial data, $MSE_b$ enforces the periodic boundary conditions, and
$MSE_f$ penalizes the Schr$\mathrm\{\ddot\{o\}\}$dinger equation not being
satisfied on the collocation points.
:::

One potential limitation of the continuous time neural network models
considered so far stems from the need to use a large number of
collocation points $N_f$ in order to enforce physics informed
constraints in the entire spatio-temporal domain. Although this poses no
significant issues for problems in one or two spatial dimensions, it may
introduce a severe bottleneck in higher dimensional problems, as the
total number of collocation points needed to globally enforce a physics
informed constrain (i.e., in this case a partial differential equation)
will increase exponentially. In the next section, we put forth a
different approach that circumvents the need for collocation points by
introducing a more structured neural network representation leveraging
the classical
[Runge-Kutta](https://en.wikipedia.org/wiki/Runge–Kutta_methods)
time-stepping schemes.

The approach discussed below pertains to a methodology that can be used
on **discrete-time models**. Let's first revist the classical
Runge-Kutta method (4 steps) before discussing the general case of $q$
steps.

##### The Classical Runge-Kutta Method

Let an initial value problem be specified as follows:

$\frac\{dy\}\{dt\}=f(t,y)$,$\quad y(t_\{0\})=y_\{0\}$

Now pick a step-size h \> 0 and define

$\begin\{aligned\}y_\{n+1\}&=y_\{n\}+\{\frac \{1\}\{6\}\}h\left(k_\{1\}+2k_\{2\}+2k_\{3\}+k_\{4\}\right),\\t_\{n+1\}&=t_\{n\}+h\\\end\{aligned\}$

$\begin\{aligned\}k_\{1\}&=\ f(t_\{n\},y_\{n\}),\\k_\{2\}&=\ f\left(t_\{n\}+\{\frac \{h\}\{2\}\},y_\{n\}+h\{\frac \{k_\{1\}\}\{2\}\}\right),\\k_\{3\}&=\ f\left(t_\{n\}+\{\frac \{h\}\{2\}\},y_\{n\}+h\{\frac \{k_\{2\}\}\{2\}\}\right),\\k_\{4\}&=\ f\left(t_\{n\}+h,y_\{n\}+hk_\{3\}\right).\end\{aligned\}$

##### The General Form of the Runge-Kutta Method \{#the-general-runge-kutta-method\}

The general form of Runge-Kutta methods with $q$ stages is applied to
the governing equation to arrive at the following:

$$\begin\{aligned\}
    \begin\{split\}
        u^\{n+c_i\}=u^n-\Delta t\sum_\{i=1\}^\{q\}a_\{ij\}\mathcal\{N\}[u^\{n+c_j\}], i=1,\cdot\cdot\cdot,q,\\
        u^\{n+1\}=u^n-\Delta t\sum_\{i=1\}^\{q\}b_\{j\}\mathcal\{N\}[u^\{n+c_j\}].
    \end\{split\}
\end\{aligned\}$$

In the formulation, $u^\{n+c_j\}(x)=u(t^n+c_j\Delta t,x)$ for
$j=1,\cdot\cdot\cdot,q$. Depending on the choice of the parameters
$\{a_\{ij\},b_j,c_j\}$.

The above equations can also be expressed as

$$\begin\{aligned\}
    \begin\{split\}
        u^n=u^n_i, i=1,\cdot\cdot\cdot,q,\\
        u^n=u^n_\{q+1\}
    \end\{split\}
\end\{aligned\}$$

where $$\begin\{aligned\}
    \begin\{split\}
        u_i^n:=u^\{n+c_i\}+\Delta t\sum_\{j=1\}^\{q\}a_\{ij\}\mathcal\{N\}[u^\{n+c_j\}], i=1,\cdot\cdot\cdot,q,\\
          u^\{n\}_\{q+1\}=u^\{n+1\}+\Delta t\sum_\{i=1\}^\{q\}b_\{j\}\mathcal\{N\}[u^\{n+c_j\}].
    \end\{split\}
\end\{aligned\}$$

We can place a multi-output neural network prior on
$$\label\{eq_discrete\}
    [u^\{n+c_1\}(x),\cdot\cdot\cdot,u^\{n+c_q\}(x),u^\{n+1\}(x)]$$ The
assumption along with equations
[\[eq_discrete\]](#eq_discrete)\{reference-type="ref"
reference="eq_discrete"\} result in a physics-informed neural network
that takes $x$ as an input and gives as the output
$$[u^\{n\}_1(x),\cdot\cdot\cdot,u^\{n\}_q(x),u_\{q+1\}^\{n+1\}(x)]$$

Below we show an example for the Allen-Cahn equation that appears in
systems with phase separations, e.g., battery electrode-electrolyte
interfaces.\

::: example
The example is used to demonstrate the ability of the proposed discrete
time models to handle different types of nonlinearity in the governing
partial differential equation. Let's consider the Allen--Cahn equation
along with periodic boundary conditions

$$\begin\{aligned\}
     u_t-0.0001u_\{xx\}+5u^3-5u^3-5u &= 0, x\in[-1,1], t \in [0,1]\\
     u(0,x) &=x^2 cos(\pi x)\\
     u(t,-1) &=u(t,1)\\
     u_x(t,-1) &=u_x(t,1)\\
 
\end\{aligned\}$$

For the Allen-Cahn equation, the non-linear operator is given by

$$\mathcal\{N\}[u^\{n+c_j\}]=-0.0001u^\{n+c_j\}_\{xx\}+5(u^\{n+c_j\})^3-5u^\{n+c_j\}$$

and the shared parameters of the neural networks can be learnt by
minimizing the loss function as discussed earlier,

$$SSE=SSE_n+SSE_b$$

where
$$SSE_n=\sum_\{j=1\}^\{q+1\}\sum_\{i=1\}^\{N_n\}|u^n_j(x^\{n,i\})=u^\{n,i\}|^2$$

and $$\begin\{aligned\}
 \begin\{split\}
     SSE_b=\sum_\{i=1\}^q|u^\{n+c_i\}(-1)-u^\{n+c_i\}(1)|^2+|u^\{n+1\}(-1)-u^\{n+1\}(1)|^2\\
     +\sum_\{i=1\}^q|u_x^\{n+c_i\}(-1)-u_x^\{n+c_i\}(1)|^2+|u_x^\{n+1\}(-1)-u_x^\{n+1\}(1)|^2
 \end\{split\}
 
\end\{aligned\}$$

![Allen-Cahn Equation: Discrete Time
Domain](figures/allenCahn_PINN.png)\{#fig:allencahn\}
:::

## Data-Driven Discovery of Nonlinear Partial Differential Equations

We shift our attention to the problem of data-driven discovery of
partial differential equations. To this end, let us consider
parametrized and nonlinear partial differential equations of the general
form $$u_t + \mathcal\{N\}[u;\lambda] = 0,\ x \in \Omega, \ t\in[0,T],$$

where $u(t,x)$ denotes the latent (hidden) solution,
$\mathcal\{N\}[\cdot;\lambda]$ is a nonlinear operator parametrized by
$\lambda$, and $\Omega$ is a subset of $\mathbb\{R\}^D$. Now, the problem
of data-driven discovery of partial differential equations poses the
following question: given a small set of scattered and potentially noisy
observations of the hidden state $u(t,x)$ of a system, what are the
parameters $\lambda$ that best describe the observed data?

In what follows, we will provide an overview of the two main approaches
to tackle this problem, namely continuous time and discrete time models,
as well as a series of results and systematic studies for a diverse
collection of benchmarks. In the first approach, we will assume
availability of scattered and potential noisy measurements across the
entire spatio-temporal domain. In the latter, we will try to infer the
unknown parameters $\lambda$ from only two data snapshots taken at
distinct time instants

### Continuous Time Models

We define $f(t,x)$ to be given by

$$f := u_t + \mathcal\{N\}[u;\lambda],\label\{eq:PDE_RHS\}$$

and proceed by approximating $u(t,x)$ by a deep neural network. This
assumption results in a [physics informed neural
network](https://arxiv.org/abs/1711.10566) $f(t,x)$. This network can be
derived by the calculus on computational graphs:
[Backpropagation](http://colah.github.io/posts/2015-08-Backprop/). It is
worth highlighting that the parameters of the differential operator
$\lambda$ turn into parameters of the physics informed neural network
$f(t,x)$.

### Navier-Stokes Equation

The next example involves a realistic scenario of incompressible fluid
flow as described by the Navier-Stokes equations. Navier-Stokes
equations describe the physics of many phenomena of scientific and
engineering interest. They may be used to model the weather, ocean
currents, water flow in a pipe and air flow around a wing. The
Navier-Stokes equations in their full and simplified forms help with the
design of aircraft and cars, the study of blood flow, the design of
power stations, the analysis of the dispersion of pollutants, and many
other applications. Let us consider the Navier-Stokes equations in two
dimensions (2D) given explicitly by

$$\begin\{aligned\}
\begin\{split\}
u_t + \lambda_1 (u u_x + v u_y) = -p_x + \lambda_2(u_\{xx\} + u_\{yy\}),\\
v_t + \lambda_1 (u v_x + v v_y) = -p_y + \lambda_2(v_\{xx\} + v_\{yy\}),    
\end\{split\}
\end\{aligned\}$$

where $u(t, x, y)$ denotes the $x$-component of the velocity field,
$v(t, x, y)$ the $y$-component, and $p(t, x, y)$ the pressure. Here,
$\lambda = (\lambda_1, \lambda_2)$ are the unknown parameters. Solutions
to the Navier-Stokes equations are searched in the set of
divergence-free functions; i.e.,

$$u_x + v_y = 0.$$

This extra equation is the continuity equation for incompressible fluids
that describes the conservation of mass of the fluid. We make the
assumption that

$$u = \psi_y,\ \ \ v = -\psi_x,$$

for some latent function $\psi(t,x,y)$. Under this assumption, the
continuity equation will be automatically satisfied. Given noisy
measurements

$$\{t^i, x^i, y^i, u^i, v^i\}_\{i=1\}^\{N\}$$

of the velocity field, we are interested in learning the parameters
$\lambda$ as well as the pressure $p(t,x,y)$. We define $f(t,x,y)$ and
$g(t,x,y)$ to be given by

$$\begin\{aligned\}
f &:= u_t + \lambda_1 (u u_x + v u_y) + p_x - \lambda_2(u_\{xx\} + u_\{yy\})\\
g &:= v_t + \lambda_1 (u v_x + v v_y) + p_y - \lambda_2(v_\{xx\} + v_\{yy\})        
\end\{aligned\}$$

and proceed by jointly approximating $$\begin\{bmatrix\}
\psi(t,x,y) & p(t,x,y)
\end\{bmatrix\}$$ using a single neural network with two outputs. This
prior assumption results into a physics informed neural network
$$\begin\{bmatrix\}
f(t,x,y) & g(t,x,y)
\end\{bmatrix\}$$ The parameters $\lambda$ of the Navier-Stokes operator
as well as the parameters of the neural networks $\begin\{bmatrix\}
\psi(t,x,y) & p(t,x,y)
\end\{bmatrix\}$ and $\begin\{bmatrix\}
f(t,x,y) & g(t,x,y)
\end\{bmatrix\}$ can be trained by minimizing the mean squared error loss

$$\begin\{aligned\}
    \begin\{split\}
    MSE :=& \frac\{1\}\{N\}\sum_\{i=1\}^\{N\} \left(|u(t^i,x^i,y^i) - u^i|^2 + |v(t^i,x^i,y^i) - v^i|^2\right) \\
    +& \frac\{1\}\{N\}\sum_\{i=1\}^\{N\} \left(|f(t^i,x^i,y^i)|^2 + |g(t^i,x^i,y^i)|^2\right)
    \end\{split\}
\end\{aligned\}$$

The approach so far assumes availability of scattered data throughout
the entire spatio-temporal domain. However, in many cases of practical
interest, one may only be able to observe the system at distinct time
instants. In the next section, we introduce a different approach that
tackles the data-driven discovery problem using only two data snapshots.
We will see how, by leveraging the classical Runge-Kutta time-stepping
schemes, one can construct discrete time physics informed neural
networks that can retain high predictive accuracy even when the temporal
gap between the data snapshots is very large.

### Discrete Time Models

We begin by employing the general form of Runge-Kutta methods with $q$
stages and obtain

$$\begin\{aligned\}
u^\{n+c_i\} &= u^n - \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q\\
u^\{n+1\} &= u^\{n\} - \Delta t \sum_\{j=1\}^q b_j \mathcal\{N\}[u^\{n+c_j\};\lambda].        
\end\{aligned\}$$

Here, $u^\{n+c_j\}(x) = u(t^n + c_j \Delta t, x)$ for $j=1, \ldots, q$.
This general form encapsulates both implicit and explicit time-stepping
schemes, depending on the choice of the parameters $\{a_\{ij\},b_j,c_j\}$.
The above equations can be equivalently expressed as

$$\begin\{aligned\}
u^\{n\} &= u^n_i, \ \ i=1,\ldots,q\\
u^\{n+1\} &= u^\{n+1\}_\{i\}, \ \ i=1,\ldots,q
\end\{aligned\}$$

where

$$\begin\{aligned\}
u^n_i &:= u^\{n+c_i\} + \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q,\\
u^\{n+1\}_\{i\} &:= u^\{n+c_i\} + \Delta t \sum_\{j=1\}^q (a_\{ij\} - b_j) \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q.
\end\{aligned\}$$

We proceed by placing a multi-output neural network prior on

$$u^\{n+c_1\}(x), \ldots, u^\{n+c_q\}(x)$$

This prior assumption result in two physics informed neural networks

$$u^\{n\}_1(x), \ldots, u^\{n\}_q(x), u^\{n\}_\{q+1\}(x)$$

and

$$u^\{n+1\}_1(x), \ldots, u^\{n+1\}_q(x), u^\{n+1\}_\{q+1\}(x)$$

Given noisy measurements at two distinct temporal snapshots
$\{\mathbf\{x\}^\{n\}, \mathbf\{u\}^\{n\}\}$ and
$\{\mathbf\{x\}^\{n+1\}, \mathbf\{u\}^\{n+1\}\}$ of the system at times $t^\{n\}$
and $t^\{n+1\}$, respectively, the shared parameters of the neural
networks along with the parameters $\lambda$ of the differential
operator can be trained by minimizing the sum of squared errors

$$SSE = SSE_n + SSE_\{n+1\},$$

where

$$SSE_n := \sum_\{j=1\}^q \sum_\{i=1\}^\{N_n\} |u^n_j(x^\{n,i\}) - u^\{n,i\}|^2,$$

and

$$SSE_\{n+1\} := \sum_\{j=1\}^q \sum_\{i=1\}^\{N_\{n+1\}\} |u^\{n+1\}_j(x^\{n+1,i\}) - u^\{n+1,i\}|^2.$$

Here, $\mathbf\{x\}^n = \left\{x^\{n,i\}\right\}_\{i=1\}^\{N_n\}$,
$\mathbf\{u\}^n = \left\{u^\{n,i\}\right\}_\{i=1\}^\{N_n\}$,
$\mathbf\{x\}^\{n+1\} = \left\{x^\{n+1,i\}\right\}_\{i=1\}^\{N_\{n+1\}\}$, and
$\mathbf\{u\}^\{n+1\} = \left\{u^\{n+1,i\}\right\}_\{i=1\}^\{N_\{n+1\}\}$.

### Korteweg--de Vries Equation

The final example aims to highlight the ability of the proposed
framework to handle governing partial differential equations involving
higher order derivatives. Here, we consider a mathematical model of
waves on shallow water surfaces, the Korteweg-de Vries (KdV) equation.
The KdV equation reads as

$$u_t + \lambda_1 u u_x + \lambda_2 u_\{xxx\} = 0,$$

with $(\lambda_1, \lambda_2)$ being the unknown parameters. For the KdV
equation, the nonlinear operator is given by

$$\mathcal\{N\}[u^\{n+c_j\}] = \lambda_1 u^\{n+c_j\} u^\{n+c_j\}_x - \lambda_2 u^\{n+c_j\}_\{xxx\}$$

and the shared parameters of the neural networks along with the
parameters $\lambda = (\lambda_1, \lambda_2)$ of the KdV equation can be
learned by minimizing the sum of squared errors given above.

## Universal Differential Equations

Universal differential equations (UDEs) are an approach can be utilized
to discover previously unknown governing equations, accurately
extrapolate beyond the original data, and accelerate model simulation,
all in a time and data-efficient manner.

## PINNs

##### Broad Goal:

-   Physical and biological systems: there exists a vast amount of prior
    knowledge that is currently not being utilized in modern machine
    learning practice (general non-linear PDEs)

-   Approach:

    -   Leverage NNs and their well known capability as universal
        function approximators

    -   PINNS: Regularization agent method

-   Example: Compressible fluid dynamics problems by discarding any non
    realistic flow solutions that violate the conservation of mass
    principle

-   Learn the physics (PDEs coefficients)

##### Two Distinct Algorithms:

-   Continuous-time and Discrete-time model

## Data-driven Solutions of Partial Differential Equations \{#data-driven-solutions-of-partial-differential-equations\}

Partial differential equations of the general form

$$u_t + \mathcal\{N\}[u;\lambda] = 0,\ x \in \Omega, \ t\in[0,T],$$

where,

$u(t,x)$ denotes the latent (hidden) solution

$\mathcal\{N\}[\cdot]$ is a nonlinear differential operator parametrized
by $\lambda$

$\Omega$ is a subset of $\mathbb\{R\}^D$.

##### Specifically:

-   **Given fixed model parameters $\lambda$, what can be said about the
    unknown hidden state u(t, x) of the system**

-   **Data driven discovery of partial differential equations: what are
    the parameters $\lambda$ that best describe the observed data?**

## Continuous Time Models: General Formalism

We define $f(t,x)$ to be given by

$$f := u_t + \mathcal\{N\}[u],$$

Approximate $u(t,x)$ by a deep neural network. This assumption results
in a **physics informed neural network**, $f(t,x)$.

Key characteristics:

-   Chain rule for differentiating compositions of functions using
    automatic differentiation

-   Has the same parameters as the network representing $u(t, x)$

-   Different activation functions due to the action of the differential
    operator $\mathcal\{N\}$

-   The shared parameters between the neural networks $u(t, x)$ and
    $f(t, x)$ can be learned by minimizing the mean squared error loss

### Learning the Neural Network Parameters

Loss function is chosen to be $MSE=MSE_u + MSE_f$

where

$MSE\_u=\frac\{1\}\{N_u\}
\sum\emph\{\{i=1\}\textsuperscript\{\{N\_u\}\textbar\{\}u(t\_u\}i,x\}u\textsuperscript\{i)-u\}i$
and $MSE_f=\frac\{1\}\{N_f\} \sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2$

Initial and boundary training data on $u(t, x)$:
$\{\{t^i_u, x^i_u, u^i\}\}^\{N_u\}_\{i=1\}$

$\{\{t^i_f, x^i_f\}\}^\{N_f\}_\{i=1\}$ specify the collocation points for
$f(t, x)$.

### Example: Burgers' Equation

Consider the **Burgers' equation**. In one space dimension, the Burger's
equation along with Dirichlet boundary conditions reads as

$$\begin\{array\}\{l\}
u_t + u u_x - (0.01/\pi) u_\{xx\} = 0,\ \ \ x \in [-1,1],\ \ \ t \in [0,1],\\
u(0,x) = -\sin(\pi x),\\
u(t,-1) = u(t,1) = 0.
\end\{array\}$$

Let us define $f(t,x)$ to be given by

$$f := u_t + u u_x - (0.01/\pi) u_\{xx\},$$

and proceed by approximating $u(t,x)$ by a deep neural network.

### Example: Burgers' Equation (contd.)

Correspondingly, the physics informed neural network $f(t,x)$ takes the
form

Loss function: $MSE = MSE_u + MSE_f,$

where

$MSE\_u = \frac\{1\}\{N_u\}\sum\emph\{\{i=1\}\^\{\}\{N\}u\}
\textbar\{\}u(t\textsuperscript\{i\_u,x\_u\}i) -
u\textsuperscript\{i\textbar\{\}\}2$ and
$MSE_f = \frac\{1\}\{N_f\}\sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2$

### Example: Burgers' Equation (contd.)

The following figure summarizes the results for the data-driven solution
of the Burgers' equation.

![image](figures/Burgers_CT_inference_book.png) \> *Burgers' equation:*
*Top:* Predicted solution along with the initial and boundary training
data. In addition we are using 10,000 collocation points generated using
a Latin Hypercube Sampling strategy. *Bottom:* Comparison of the
predicted and exact solutions corresponding to the three temporal
snapshots depicted by the white vertical lines in the top panel. Model
training took approximately 60 seconds on a single NVIDIA Titan X GPU
card.

### Example: (Schrödinger Equation) \{#example-shruxf6dinger-equation\}

-   handle periodic boundary conditions

-   complex-valued solutions

-   different types of nonlinearities in the governing partial
    differential equations.

Nonlinear Schrödinger equation:

$$\begin\{array\}\{l\}
i h_t + 0.5 h_\{xx\} + |h|^2 h = 0,\ \ \ x \in [-5, 5],\ \ \ t \in [0, \pi/2],\\
h(0,x) = 2\ \text\{sech\}(x),\\
h(t,-5) = h(t, 5),\\
h_x(t,-5) = h_x(t, 5),
\end\{array\}$$

where $h(t,x)$ is the complex-valued solution. Let us define $f(t,x)$ to
be given by

$$f := i h_t + 0.5 h_\{xx\} + |h|^2 h,$$

Complex-valued neural network prior on $h(t,x)$.

$u$ denotes the real part of $h$ and $v$ is the imaginary part multi-out
neural network prior on

$$h(t,x) = \begin\{bmatrix\}
u(t,x) & v(t,x)
\end\{bmatrix\}$$.

### Example: (Schrödinger Equation) (contd.) \{#example-shruxf6dinger-equation-contd.\}

Results in the complex-valued (multi-output) physic informed neural
network $f(t,x)$. The shared parameters of the neural networks $h(t,x)$
and $f(t,x)$ can be learned by minimizing the mean squared error loss

$MSE = MSE_0 + MSE_b + MSE_f,$

where,

$MSE_0 = \frac\{1\}\{N_0\}\sum_\{i=1\}^\{N_0\} |h(0,x_0^i) - h^i_0|^2,$

$MSE_b = \frac\{1\}\{N_b\}\sum_\{i=1\}^\{N_b\} \left(|h^i(t^i_b,-5) - h^i(t^i_b,5)|^2 + |h^i_x(t^i_b,-5) - h^i_x(t^i_b,5)|^2\right),$

and

$MSE_f = \frac\{1\}\{N_f\}\sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2.$

Initial Data: $\{x_0^i, h^i_0\}_\{i=1\}^\{N_0\}$

Collocation points on the boundary: $\{t^i_b\}_\{i=1\}^\{N_b\}$

Collocation points on $f(t,x)$:$\{t_f^i,x_f^i\}_\{i=1\}^\{N_f\}$

$MSE_0$: loss on the initial data, $MSE_b$: enforces the periodic
boundary conditions, and $MSE_f$ penalizes the Schrödinger equation.

### Example: (Schrödinger Equation) \{#example-shruxf6dinger-equation-1\}

The following figure summarizes the results of the experiment.

![image](figures/NLS_bookl.png) \> *Schrödinger equation:* **Top:**
Predicted solution along with the initial and boundary training data. In
addition we are using 20,000 collocation points generated using a Latin
Hypercube Sampling strategy. **Bottom:** Comparison of the predicted and
exact solutions corresponding to the three temporal snapshots depicted
by the dashed vertical lines in the top panel.

One potential limitation of the continuous time neural network models
considered so far, stems from the need to use a large number of
collocation points $N_f$ in order to enforce physics informed
constraints in the entire spatio-temporal domain. Although this poses no
significant issues for problems in one or two spatial dimensions, it may
introduce a severe bottleneck in higher dimensional problems, as the
total number of collocation points needed to globally enforce a physics
informed constrain (i.e., in our case a partial differential equation)
will increase exponentially. In the next section, we put forth a
different approach that circumvents the need for collocation points by
introducing a more structured neural network representation leveraging
the classical
[Runge-Kutta](https://en.wikipedia.org/wiki/Runge--Kutta_methods)
time-stepping schemes.

##### Discrete Time Models \{#discrete-time-models\}

General form of Runge-Kutta methods with $q$ stages and obtain

$$\begin\{array\}\{ll\}
u^\{n+c_i\} = u^n - \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\}], \ \ i=1,\ldots,q,\\
u^\{n+1\} = u^\{n\} - \Delta t \sum_\{j=1\}^q b_j \mathcal\{N\}[u^\{n+c_j\}].
\end\{array\}$$

Here, $u^\{n+c_j\}(x) = u(t^n + c_j \Delta t, x)$ for $j=1, \ldots, q$.
This general form encapsulates both implicit and explicit time-stepping
schemes, depending on the choice of the parameters $\{a_\{ij\},b_j,c_j\}$.
The above equations can be equivalently expressed as

$$\begin\{array\}\{ll\}
u^\{n\} = u^n_i, \ \ i=1,\ldots,q,\\
u^n = u^n_\{q+1\},
\end\{array\}$$

where

$$\begin\{array\}\{ll\}
u^n_i := u^\{n+c_i\} + \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\}], \ \ i=1,\ldots,q,\\
u^n_\{q+1\} := u^\{n+1\} + \Delta t \sum_\{j=1\}^q b_j \mathcal\{N\}[u^\{n+c_j\}].
\end\{array\}$$

##### The Classical Runge-Kutta Method \{#the-classical-runge-kutta-method\}

Let an initial value problem be specified as follows:

$\frac\{dy\}\{dt\}=f(t,y)$,$\quad y(t_\{0\})=y_\{0\}$

Now pick a step-size h \> 0 and define

$\begin\{aligned\}y_\{n+1\}&=y_\{n\}+\{\frac \{1\}\{6\}\}h\left(k_\{1\}+2k_\{2\}+2k_\{3\}+k_\{4\}\right),\\t_\{n+1\}&=t_\{n\}+h\\\end\{aligned\}$

$\begin\{aligned\}k_\{1\}&=\ f(t_\{n\},y_\{n\}),\\k_\{2\}&=\ f\left(t_\{n\}+\{\frac \{h\}\{2\}\},y_\{n\}+h\{\frac \{k_\{1\}\}\{2\}\}\right),\\k_\{3\}&=\ f\left(t_\{n\}+\{\frac \{h\}\{2\}\},y_\{n\}+h\{\frac \{k_\{2\}\}\{2\}\}\right),\\k_\{4\}&=\ f\left(t_\{n\}+h,y_\{n\}+hk_\{3\}\right).\end\{aligned\}$

##### The Classical Runge-Kutta Method (Schematic)

![Slopes at different step locations used by the classical Runge-Kutta
method. Source: Wiki](figures/RK_book.png)

##### Discrete Time Models (contd.)

We proceed by placing a multi-output neural network prior on

$$\begin\{bmatrix\}
u^\{n+c_1\}(x), \ldots, u^\{n+c_q\}(x), u^\{n+1\}(x)
\end\{bmatrix\}.$$

This prior assumption along with the above equations result in a physics
informed neural network that takes $x$ as an input and outputs

$$\begin\{bmatrix\}
u^n_1(x), \ldots, u^n_q(x), u^n_\{q+1\}(x)
\end\{bmatrix\}.$$

##### Example: Allen-Cahn Equation

Allen-Cahn equation along with periodic boundary conditions

$$\begin\{array\}\{l\}
u_t - 0.0001 u_\{xx\} + 5 u^3 - 5 u = 0, \ \ \ x \in [-1,1], \ \ \ t \in [0,1],\\
u(0, x) = x^2 \cos(\pi x),\\
u(t,-1) = u(t,1),\\
u_x(t,-1) = u_x(t,1).
\end\{array\}$$

The Allen-Cahn equation is a well-known equation from the area of
reaction-diffusion systems. It describes the process of phase separation
in multi-component alloy systems, including order-disorder transitions.
For the Allen-Cahn equation, the nonlinear operator is given by

$$\mathcal\{N\}[u^\{n+c_j\}] = -0.0001 u^\{n+c_j\}_\{xx\} + 5 \left(u^\{n+c_j\}\right)^3 - 5 u^\{n+c_j\},$$

and the shared parameters of the neural networks can be learned by
minimizing the sum of squared errors

$SSE = SSE_n + SSE_b,$

##### Example: Allen-Cahn Equation (contd.)

$$SSE_n = \sum_\{j=1\}^\{q+1\} \sum_\{i=1\}^\{N_n\} |u^n_j(x^\{n,i\}) - u^\{n,i\}|^2,$$
and

$$\begin\{array\}\{rl\}
SSE_b =& \sum_\{i=1\}^q |u^\{n+c_i\}(-1) - u^\{n+c_i\}(1)|^2 + |u^\{n+1\}(-1) - u^\{n+1\}(1)|^2 \\
      +& \sum_\{i=1\}^q |u_x^\{n+c_i\}(-1) - u_x^\{n+c_i\}(1)|^2 + |u_x^\{n+1\}(-1) - u_x^\{n+1\}(1)|^2.
\end\{array\}$$

Here, $\{x^\{n,i\}, u^\{n,i\}\}_\{i=1\}^\{N_n\}$ corresponds to the data at time
$t^n$.

##### Example: Allen-Cahn Equation

![Allen-Cahn equation: **Top:** Solution along with the location of the
initial training snapshot at t=0.1 and the final prediction snapshot at
t=0.9. **Bottom:** Initial training data and final prediction at the
snapshots depicted by the white vertical lines in the top
panel.](figures/AC_book.png)

### Data-driven Discovery of Nonlinear Partial Differential Equations \{#data-driven-discovery-of-nonlinear-partial-differential-equations\}

Parametrized and nonlinear partial differential equations of the general
form

$$u_t + \mathcal\{N\}[u;\lambda] = 0,\ x \in \Omega, \ t\in[0,T],$$

where $u(t,x)$ denotes the latent (hidden) solution,
$\mathcal\{N\}[\cdot;\lambda]$ is a nonlinear operator parametrized by
$\lambda$, and $\Omega$ is a subset of $\mathbb\{R\}^D$.

##### Continuous Time Models \{#continuous-time-models\}

We define $$f(t,x)$$ to be given by

$$f := u_t + \mathcal\{N\}[u;\lambda],\label\{eq:PDE_RHS\}$$

and proceed by approximating $$u(t,x)$$ by a deep neural network. This
assumption results in a physics informed neural network $f(t,x)$.

##### Example: Navier-Stokes Equation

Incompressible fluid flow as described by the ubiquitous Navier-Stokes
equations. The Navier-Stokes equations in two dimensions (2D) given
explicitly by

$$\begin\{array\}\{c\}
u_t + \lambda_1 (u u_x + v u_y) = -p_x + \lambda_2(u_\{xx\} + u_\{yy\}),\\
v_t + \lambda_1 (u v_x + v v_y) = -p_y + \lambda_2(v_\{xx\} + v_\{yy\}),
\end\{array\}$$

where $u(t, x, y)$ denotes the $x$-component of the velocity field,
$v(t, x, y)$ the $y$-component, and $p(t, x, y)$ the pressure. Here,
$\lambda = (\lambda_1, \lambda_2)$ are the unknown parameters. Solutions
to the Navier-Stokes equations are searched in the set of
divergence-free functions; i.e.,

$$u_x + v_y = 0.$$

This extra equation is the continuity equation for incompressible fluids
that describes the conservation of mass of the fluid. We make the
assumption that

$$u = \psi_y,\ \ \ v = -\psi_x,$$

for some latent function $\psi(t,x,y)$. Under this assumption, the
continuity equation will be automatically satisfied. Given noisy
measurements

$$\{t^i, x^i, y^i, u^i, v^i\}_\{i=1\}^\{N\}$$

##### Example: Navier-Stokes Equation (contd.)

We are interested in learning the parameters $\lambda$ as well as the
pressure $p(t,x,y)$. We define $f(t,x,y)$ and $g(t,x,y)$ to be given by

$$\begin\{array\}\{c\}
f := u_t + \lambda_1 (u u_x + v u_y) + p_x - \lambda_2(u_\{xx\} + u_\{yy\}),\\
g := v_t + \lambda_1 (u v_x + v v_y) + p_y - \lambda_2(v_\{xx\} + v_\{yy\}),
\end\{array\}$$

and proceed by jointly approximating $$\begin\{bmatrix\}
\psi(t,x,y) & p(t,x,y)
\end\{bmatrix\}$$ using a single neural network with two outputs. This
prior assumption results into a physics informed neural network
$$\begin\{bmatrix\}
f(t,x,y) & g(t,x,y)
\end\{bmatrix\}$$.

The parameters $\lambda$ of the Navier-Stokes operator as well as the
parameters of the neural networks
$\begin\{bmatrix\} \psi(t,x,y) & p(t,x,y) \end\{bmatrix\}$ and
$\begin\{bmatrix\} f(t,x,y) & g(t,x,y) \end\{bmatrix\}$ can be trained by
minimizing the mean squared error loss

$$\begin\{array\}\{rl\}
MSE :=& \frac\{1\}\{N\}\sum_\{i=1\}^\{N\} \left(|u(t^i,x^i,y^i) - u^i|^2 + |v(t^i,x^i,y^i) - v^i|^2\right) \\
    +& \frac\{1\}\{N\}\sum_\{i=1\}^\{N\} \left(|f(t^i,x^i,y^i)|^2 + |g(t^i,x^i,y^i)|^2\right).
\end\{array\}$$

##### Example: Navier-Stokes Equation (Results)

![Navier-Stokes equation: **Top:** Incompressible flow and dynamic
vortex shedding past a circular cylinder at Re$=100$. The
spatio-temporal training data correspond to the depicted rectangular
region in the cylinder wake. **Bottom:** Locations of training
data-points for the the stream-wise and transverse velocity components.
](figures/NavierStokes_data_book.png)

##### Example: Navier-Stokes Equation (Results)

![Navier-Stokes equation: **Top:** Predicted versus exact instantaneous
pressure field at a representative time instant. By definition, the
pressure can be recovered up to a constant, hence justifying the
different magnitude between the two plots. This remarkable qualitative
agreement highlights the ability of physics-informed neural networks to
identify the entire pressure field, despite the fact that no data on the
pressure are used during model training. **Bottom:** Correct partial
differential equation along with the identified
one.](figures/NavierStokes_prediction_book.png)

Next: High predictive accuracy even when the temporal gap between the
data snapshots is very large.

##### Discrete Time Models \{#discrete-time-models-1\}

We begin by employing the general form of
[Runge-Kutta](https://en.wikipedia.org/wiki/Runge--Kutta_methods)
methods with $$q$$ stages and obtain

$$\begin\{array\}\{ll\}
u^\{n+c_i\} = u^n - \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q,\\
u^\{n+1\} = u^\{n\} - \Delta t \sum_\{j=1\}^q b_j \mathcal\{N\}[u^\{n+c_j\};\lambda].
\end\{array\}$$

Here, $u^\{n+c_j\}(x) = u(t^n + c_j \Delta t, x)$ for $j=1, \ldots, q$.
This general form encapsulates both implicit and explicit time-stepping
schemes, depending on the choice of the parameters $\{a_\{ij\},b_j,c_j\}$.
The above equations can be equivalently expressed as

$$\begin\{array\}\{ll\}
u^\{n\} = u^n_i, \ \ i=1,\ldots,q,\\
u^\{n+1\} = u^\{n+1\}_\{i\}, \ \ i=1,\ldots,q.
\end\{array\}$$

where

$$\begin\{array\}\{ll\}
u^n_i := u^\{n+c_i\} + \Delta t \sum_\{j=1\}^q a_\{ij\} \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q,\\
u^\{n+1\}_\{i\} := u^\{n+c_i\} + \Delta t \sum_\{j=1\}^q (a_\{ij\} - b_j) \mathcal\{N\}[u^\{n+c_j\};\lambda], \ \ i=1,\ldots,q.
\end\{array\}$$

##### Discrete Time Models (contd.)

We proceed by placing a multi-output neural network prior on

$$\begin\{bmatrix\}
u^\{n+c_1\}(x), \ldots, u^\{n+c_q\}(x)
\end\{bmatrix\}.$$

This prior assumption result in two physics informed neural networks

$$\begin\{bmatrix\}
u^\{n\}_1(x), \ldots, u^\{n\}_q(x), u^\{n\}_\{q+1\}(x)
\end\{bmatrix\},$$

and

$$\begin\{bmatrix\}
u^\{n+1\}_1(x), \ldots, u^\{n+1\}_q(x), u^\{n+1\}_\{q+1\}(x)
\end\{bmatrix\}.$$

##### Discrete Time Models (contd.)

Given noisy measurements at two distinct temporal snapshots
$\{\mathbf\{x\}^\{n\}, \mathbf\{u\}^\{n\}\}$ and
$\{\mathbf\{x\}^\{n+1\}, \mathbf\{u\}^\{n+1\}\}$ of the system at times $t^\{n\}$
and $t^\{n+1\}$, respectively, the shared parameters of the neural
networks along with the parameters $\lambda$ of the differential
operator can be trained by minimizing the sum of squared errors

$$SSE = SSE_n + SSE_\{n+1\},$$

where

$$SSE_n := \sum_\{j=1\}^q \sum_\{i=1\}^\{N_n\} |u^n_j(x^\{n,i\}) - u^\{n,i\}|^2,$$

and

$$SSE_\{n+1\} := \sum_\{j=1\}^q \sum_\{i=1\}^\{N_\{n+1\}\} |u^\{n+1\}_j(x^\{n+1,i\}) - u^\{n+1,i\}|^2.$$

Here, $\mathbf\{x\}^n = \left\{x^\{n,i\}\right\}_\{i=1\}^\{N_n\}$,
$\mathbf\{u\}^n = \left\{u^\{n,i\}\right\}_\{i=1\}^\{N_n\}$,
$\mathbf\{x\}^\{n+1\} = \left\{x^\{n+1,i\}\right\}_\{i=1\}^\{N_\{n+1\}\}$, and
$\mathbf\{u\}^\{n+1\} = \left\{u^\{n+1,i\}\right\}_\{i=1\}^\{N_\{n+1\}\}$.

##### Example: Korteweg--de Vries Equation

The final example aims to highlight the ability of the proposed
framework to handle governing partial differential equations involving
higher order derivatives. Here, we consider a mathematical model of
waves on shallow water surfaces; the Korteweg-de Vries (KdV) equation.
The KdV equation reads as

$$u_t + \lambda_1 u u_x + \lambda_2 u_\{xxx\} = 0,$$

with $(\lambda_1, \lambda_2)$ being the unknown parameters. For the KdV
equation, the nonlinear operator is given by

$$\mathcal\{N\}[u^\{n+c_j\}] = \lambda_1 u^\{n+c_j\} u^\{n+c_j\}_x - \lambda_2 u^\{n+c_j\}_\{xxx\}$$

and the shared parameters of the neural networks along with the
parameters $$\lambda = (\lambda_1, \lambda_2)$$ of the KdV equation can
be learned by minimizing the sum of squared errors given above.

##### Example: Korteweg--de Vries Equation (contd.)

![KdV equation: **Top:** Solution along with the temporal locations of
the two training snapshots. Middle: Training data and exact solution
corresponding to the two temporal snapshots depicted by the dashed
vertical lines in the top panel. **Bottom:** Correct partial
differential equation along with the identified
one.](figures/KdV_book.png)\{#fig:my_label\}

### Summary

-   PINNs provide a reqularization framework based on the (partially)
    known physics.

-   Spartially Low-Data Regime: Continuous-Time Models

-   Temporally Low-Data Regime: Discrete-Time Models (RK Method)

### Open Questions

How deep/wide should the neural network be? How much data is really
needed?

Why does the algorithm converge to unique values for the parameters of
the differential operators?

Does the network suffer from vanishing gradients for deeper
architectures and higher order differential operators?

Could this be mitigated by using different activation functions?

Can we improve on initializing the network weights or normalizing the
data?

Are the mean square error and the sum of squared errors the appropriate
loss functions?

Why are these methods seemingly so robust to noise in the data?

How can we quantify the uncertainty associated with our predictions?
