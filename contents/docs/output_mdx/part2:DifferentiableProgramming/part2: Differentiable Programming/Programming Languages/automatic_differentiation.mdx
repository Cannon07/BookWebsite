# Automatic Differentiation \{#chap:autodiff\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:calculus\]](#chap:calculus)\{reference-type="ref+label"
reference="chap:calculus"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

A differentiable programming language is one in which differentiation
becomes a first class primitive in the language. Before we can define
the syntax and semantics of a differentiable programming language, we
need to first provide a mathematical definition for what differentiation
of a program means.

Derivatives can analytically be obtained when a functional form is known
but can get tedious when the functional mapping is complex. Often
derivatives are obtained numerically using a finite difference approach
where the first derivative is computed as
$$f'(x)=\frac\{f(x+h)-f(x)\}\{h\},\quad h \mathrm\{\ is\ small\}.$$ Similarly,
the second derivative is typically numerically computed as
$$f''(x)=\frac\{f(x+h)-2f(x)+f(x-h)\}\{h^2\},\quad h \mathrm\{\ is\ small\}.$$

Automatic differentiation provides methods for automatically computing
exact derivatives (up to floating-point error) given only the function
$f$ itself. Automatic differentiation leverages the fact that every
computer program executes a sequence of elementary arithmetic operations
and elementary functions. By applying the chain rule repeatedly to these
operations, derivatives of arbitrary order can be computed
automatically, accurately to working precision, and using at most a
small constant factor more arithmetic operations than the original
program.

![Automatic Differentiation
Scheme](figures/Differentiable Programming/Programming Languages/automatic_differentiation/AutomaticDifferentiationNutshell0.png)\{#fig:AD\}

Automatic differentiation is distinct from both symbolic differentiation
and numerical differentiation. Symbolic differentiation typically
involves converting a computer program into a single expression, which
could lead to inefficient code, while numerical differentiation by means
of finite differences can introduce round-off errors in the
discretization process and cancellation. Both numerical and symbolic
differentiation are associated with errors and increased complexity when
computing higher derivatives. As a result, these methods are slow at
computing partial derivatives of a function with respect to many inputs,
as is needed for gradient-based optimization algorithms. Automatic
differentiation provides a way to eliminate all of these issues.

### Automatic Differentiation Methodology

The core idea behind automatic differentiation is the chain rule. Every
computer program or function can be viewed as a series of mathematical
transformations or operations to the input variables to arrive at the
output variables. Therefore, the input-output relationship is
essentially a composite function with nested elementary operations. The
chain rule allows computing the derivative of the composite function
given prior knowledge of derivative functions associated with elementary
operations. An implementation of the chain rule can be built into
software packages to provide automatic derivatives for functions.

For a simple case let $y$ represent the output vector, $x$ the input
vector, and $f$, $g$ and $h$ the elementary operations. The mapping can
be written as $y=f(g(h(x)))$. Applying the chain rule implies that

$$\frac\{dy\}\{dx\}=\frac\{dy\}\{dg_x\}\frac\{dg_x\}\{dh_x\}\frac\{dh_x\}\{dx\}$$

where $h_x=h(x)$, $g_x=g(h(x))$, and $y=f(g_x)$.

### Demonstration of Automatic Differentiation for a Simple Program

Let's take a program that encodes function $f(x)$ with variable $x$ as
input and computes $(a \sin(x)+b)^2$, where $a$ and $b$ are constants.
The program might looks like the following:

    def f(x: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}$:
      a, b = 1, 2
      u = sin(x)
      v = a*u+b
      y = v$^2$
      y

For this program, an automatic differentiation package identifies the
elementary operations and computes the derivatives in a serial manner
and returns thee product of the derivatives of individual steps in
accordance with the chain rule as $f'(x)$. The automatic differentiation
computation would involve the following calculations alongside the
program to be able to return the derivatives of the function:

    u' = cos(x)
    v' = a
    y' = 2*v
    f' = y'*v'*u'
    f'

Forward accumulation and reverse accumulation are two different modes of
automatic differentiation that packages use where forward accumulation
involves traversing the chain rule from the innermost operation to the
outermost operation while reverse accumulation involves traversing in
the opposite order.

Let's look at a simple example, a simple neural network with one hidden
layer and two inputs. Let's assume that the weights and biases are
computed through a training process are we are interested in computing
the sensitivity of the output variable, on the input variables. As
discussed earlier, automatic differentiation enables the program to
compute the gradients with respect to the inputs without deriving and
specifying the gradient functions.

``` \{.python language="python"\}
def two_layer_net(w: $\mathbb\{R\}$, h: $\mathbb\{R\}$, weights: $\mathbb\{R\}^6$, biases: $\mathbb\{R\}^3$) -> $\mathbb\{R\}$:
  b1, b2, b3 = biases
  w1, w2, w3, w4, w5, w6 = weights
  g1 = b1 + w1*w + w2*h
  g2 = b2 + w3*w + w4*h
  h1 = $\sigma$(g1)
  h2 = $\sigma$(g2)
  u1 = b3 + w5*h1 + w6*h2
  o1 = $\sigma$(u1)
```

We can now compute the dependence of the two layer network on its first
input as given by the gradient.

``` \{.python language="python"\}
w: $\mathbb\{R\}$, h: $\mathbb\{R\}$
weights: $\mathbb\{R\}^6$, biases: $\mathbb\{R\}^3$
h = two_layer_network(w, h, weights, biases)
$\nabla$(h, w)
```

Automatic differentiation is extremely useful in machine learning owing
to the fact that computing gradients of functions especially in
backpropagation is central to learning algorithms.

## Exercises

1.  Compute the following derivatives. Note that we have multiple
    inputs, which implies that the derivative output has two components
    `w,h`:

    ``` \{.python language="python"\}
    $\nabla$(g1, w) =                    
    $\nabla$(g1, h) =
    $\nabla$(g2, w) =                    
    $\nabla$(g2, h) =
    $\nabla$(h1, g1) =
    $\nabla$(h2, g2) =
    $\nabla$(u1, h1) =              
    $\nabla$(u1, h2) =
    $\nabla$(o, u1) =
    ```
