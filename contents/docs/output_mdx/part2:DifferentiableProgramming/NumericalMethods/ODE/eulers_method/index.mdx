# Euler's Method \{#chap:ode_euler\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:linear_systems\]](#chap:linear_systems)\{reference-type="ref+label"
reference="chap:linear_systems"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter, we learn how to implement the Euler's method for
solving a single ordinary differential equation. We learn about the
meanings of local and global truncation errors and their relationships
to step sizes for one-step methods for solving ODEs. We then solve the
Cahn-Hilliard model using Euler's method and end by explaining the use
of adaptive step-sizing for solving ODEs.

## First Order ODEs

First-order ODEs form the basis for solving governing equations of
engineering systems. Let us first consider different approaches for
solving first-order ODEs. One-step methods and multi-step methods are
two widely used methods for solving these first order ODEs.

*One-Step methods* attempt to trace out the trajectory of the solution
into the future by extrapolating from an old value $y_i$ to a new value
$y_\{i+1\}$ over a distance $h$, given the value of the increment function
(first-derivative of the function) at a single point $i$. They are also
referred to as Runge-Kutta methods after the two applied mathematicians
who first discussed them in the early 1900s. *Multi-Step methods* use
information from several previous points as the basis for extrapolating
to a new value. We focus on studying solution methods for first order
ODEs since higher order ODEs can be reduced to a system of first order
ODEs.

Runge-Kutta methods are a family of iterative methods, usually employed
for solving first order ODEs. Among these methods, the simplest approach
to solve a first order ODE is to use the differential equation to
estimate the slope in the form of the first derivative at $t_i$. In
other words, the slope at the beginning of the interval is taken as an
approximation of the average slope over the whole interval. This
approach is called the *Euler's method*. It is the most basic method for
numerical integration of ordinary differential equations and is the
simplest Runge--Kutta method.

## Euler's Method \{#eulers-method\}

Let's look at the first order ODEs of the general form
$\frac\{dy\}\{dt\} = f(t,y)$. The first derivative of the function provides
a direct estimate of the slope at the initial value $(t_i,y_i)$. This is
given as $$\begin\{aligned\}
\phi = \frac\{dy\}\{dt\} = f(t_i,y_i),
\end\{aligned\}$$ where $f(t_i,y_i)$ is the differential equation
evaluated at $t_i$ and $y_i$. The estimate from Euler's method can be
obtained by moving over by a step size along this slope estimate from
the initial value. The estimate at the next step is given as
$$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + f(t_i,y_i)h
\end\{aligned\}$$

::: marginfigure
![image](figures/part1b/euler_method/Euler_0.png)\{width="2.5in"\}
:::

This formula is referred to as *Euler's method* (or the Euler-Cauchy or
point-slope method). A new value of y is predicted using the slope
(equal to the first derivative at the original value of t) to
extrapolate linearly over the step size $h$. It can be observed that the
Euler's method primarily draws its motivation from the Taylor's series
by utilizing the first two-terms of the function's approximation about
the initial value.

## The Cahn-Hilliard Equation

Let's consider the Cahn-Hilliard Equation which is an example of a
non-linear differential equation describing the process of phase
separation, by which the two components of a binary fluid spontaneously
separate and form domains which are pure in each component. The
Cahn-Hilliard Equation is given as $$\begin\{aligned\}
\dfrac\{\partial c\}\{\partial t\} = D \nabla^2 (c^3 - c - \gamma \nabla^2c )
\vspace\{-6pt\}
\end\{aligned\}$$ Let's try to solve the Cahn-Hilliard equation given
above using the Euler's method from $t=0$ to $t=3$ with initial
conditions as the normalized intensity values of an image. Suppose time
step $dt$ of $0.002$ s, a diffusion coefficient $D$ of $20$ and $\gamma$
of $0.5$.

The Cahn-Hilliard equation can be re-written as $$\begin\{aligned\}
\dfrac\{\partial c\}\{\partial t\} = D (\nabla^2c^3 - \nabla^2 c - \nabla^2 \gamma \nabla^2 c)
\end\{aligned\}$$

We assume for modeling purposes that we can model $c$ by a $N\times N$
sized grid.

``` \{.python language="python"\}
def cahn_hilliard_eulers(C : $\mathbb\{R\}^\{N\times N\}$, $\gamma: \mathbb\{R\}$, D : $\mathbb\{R\}$, dt: $\mathbb\{R\}$, ($t_0$, $t_e$): $\mathbb\{R\}^2$, tol: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}^\{N\times N\}$:
  out = zeros(N, N)
  $t$ = $t_0$ 
  while $t$ < $t_e$:
    dc$^2$ = $\nabla^2$(C, dx) + $\nabla^2$(C, dy)
    dc$^3$ = $\nabla^2$(C$^3$,dx) + $\nabla^2$(C$^3$,dy)
    $\frac\{\textrm\{d\}\gamma\}\{\textrm\{dc\}\}$ = $\nabla^2$(gamma*dc$^2$, dx) + $\nabla^2$(gamma*dc$^2$, dy)
    $\frac\{\textrm\{dc\}\}\{\textrm\{dt\}\}$ = D*(dc$^3$ - dc$^2$ - $\frac\{\textrm\{d\}\gamma\}\{\textrm\{dc\}\}$)
    C = C + $\frac\{\textrm\{d\}\gamma\}\{\textrm\{dc\}\}$*dt
    if $|\|\textrm\{out\}\|-\|C\|)|$ < tol:
      break
    out = C
    time = time + dt
  return out
```

We use the notation $\nabla^2$ above, but in practice we need to use the
central difference to approximate the second-order derivative at a grid
element.

## Error Analysis for Euler's Method

The numerical solution of ODEs involves two types of error

1.  *Truncation* or Discretization Errors - These are caused by the
    nature of the techniques employed to approximate values of y.

2.  *Roundoff* Errors - These are caused by the limited numbers of
    significant digits that can be retained by a computer.

The truncation errors are composed of two parts. The first is a *local
truncation error* that results from an application of the method in
question over a single step. The second is a *propagated truncation
error* that results from the approximations produced during the previous
steps. The sum of the two is the total error. It is referred to as the
*global truncation error*.

Insight into the magnitude and properties of the truncation error can be
gained by deriving Euler's method directly from the Taylor series
expansion. To do this, realize that the differential equation being
integrated will be of the general form of $\frac\{dy\}\{dt\} = f(t,y)$ with
an initial value of ($t_i,y_i$). If the solution, i.e. the function
describing the behavior of y has continuous derivatives, it can be
represented by a Taylor series expansion about a starting value
($t_i ,y_i$), as in $$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + y_\{i\}'h + \frac\{y_i''\}\{2!\}h^2 + \dots + \frac\{y_i^\{n\}\}\{n!\}h^n + R_n
\end\{aligned\}$$

where $h = t_\{i+1\} - t_\{i\}$ and $R_n =$ the remainder term defined as
$$\begin\{aligned\}
R_n = \frac\{y^\{n+1\}(\xi)\}\{(n+1)!\}h^\{n+1\}
\end\{aligned\}$$ where $\xi$ lies somewhere in the interval from $t_i$ to
$t_\{i+1\}$. The above Taylor Series expansion can also be re-written as
$$\begin\{aligned\}
y_\{i+1\} = y_\{i\} + f(t_i,y_i)h + \frac\{f'(t_i,y_i)\}\{2!\}h^2 + \dots + \frac\{f^\{(n-1)\}(t_i,y_i)\}\{n!\}h^n + O(h^\{n+1\})
\end\{aligned\}$$

where $O(h^\{n+1\})$ specifies that the local truncation error is
proportional to the step size raised to the $(n + 1)$th power. From the
above equation, it can be seen that the Euler's method covers only the
first two terms of the Taylor's expansion. Thus, the truncation error in
Euler's method is attributable to the remaining terms in the Taylor
series expansion that were not included and is given as
$$\begin\{aligned\}
E_t = \frac\{f'(t_i,y_i)\}\{2!\}h^2 + \dots + \frac\{f^\{(n-1)\}(t_i,y_i)\}\{n!\}h^n + O(h^\{n+1\})
\end\{aligned\}$$

where $E_t$ is the *true local truncation error*. For sufficiently small
$h$, the higher-order terms are usually negligible, and the result is
often represented as $$\begin\{aligned\}
E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2 = O(h^2)
\end\{aligned\}$$

where $E_a$ is the *approximate local truncation error*.

It can also be shown that the global truncation error is $O(h)$, i.e. it
is proportional to the step size. These observations lead to some useful
conclusions:

1.  The global error can be reduced by decreasing the step size.

2.  The method will provide error-free predictions if the underlying
    function is linear, because for a straight line the second
    derivative would be zero.

This latter conclusion makes intuitive sense because Euler's method uses
straight-line segments to approximate the solution. Hence, Euler's
method is referred to as a *first-order method*.

## Stability of Euler's Method

The stability of a solution method is another important consideration
that must be considered when solving ODEs. A numerical solution is said
to be unstable, if errors grow exponentially for a problem for which
there is a bounded solution. The stability of a particular application
can depend on three factors: the differential equation, the numerical
method, and the step size.

Insight into the step size required for stability can be examined by
studying the following simple ODE: $$\begin\{aligned\}
\frac\{dy\}\{dt\} = -ay
\end\{aligned\}$$

If $y(0)=y_0$, the true solution can be determined using calculus as

$$\begin\{aligned\}
y = y_0e^\{-at\}
\end\{aligned\}$$

The above solution starts at $y_0$ and asymptotically approaches zero.
Solving the same problem using Euler's method gives the following
result,

$$\begin\{aligned\}
y_\{i+1\} &= y_i + \frac\{dy_i\}\{dt\}\\
h &= y_i - ay_ih = y_i(1-ah)
\end\{aligned\}$$

The parenthetical quantity $1-ah$ is called an *amplification factor*.
If its absolute value is greater than unity, the solution will grow in
an unbounded fashion. So clearly, the stability depends on the step size
*h*. Based on this analysis, Euler's method is said to be *conditionally
stable*.

Note that there are certain ODEs where errors always grow regardless of
the method. Such ODEs are called *ill-conditioned*. Inaccuracy and
instability are often confused. This is usually because both represent
situations where the numerical solution breaks down or both are affected
by step size.

## Adaptive Step-Sizing

We have seen how to solve ODEs numerically using Euler's method using a
fixed step size. However, it is noticed that the local trunctation error
is proportional to the step-size chosen. By changing this step-size
dynamically at every step, an upper bound on the local truncation error
can be imposed with minimum number of steps. *Adaptive step-sizing*
methods make use of this to come up with a new step-size at every step.

The local truncation error for Euler's method is given by
$E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2$. The step-size is simply obtained by
establishing an upper bound on this truncation error ($e_\{max\}$).
$$\begin\{aligned\}
    E_a = \frac\{f'(t_i,y_i)\}\{2!\}h^2 <= e_\{max\} \implies h <= \sqrt\{\dfrac\{2e_\{max\}\}\{f'(t_i,y_i)\}\}
    \vspace\{-6pt\}
    
\end\{aligned\}$$

This condition is used for limiting the local truncation error while
achieving greater performance on segments where the derivative of $f$ is
small.

As an example, let's try to estimate $y$ from the ODE $$\begin\{aligned\}
\frac\{dy\}\{dt\} = t-\log(1+t)
\end\{aligned\}$$ using adaptive step-sizing and Euler's method.

::: marginfigure
![image](figures/part1b/euler_method/Euler_0.png)\{width="2.5in"\}
:::

``` \{.python language="python"\}
def adaptive_euler(f: $\mathbb\{R\} \to \mathbb\{R\}$, $(t_0, t_e)$: $\mathbb\{R\}^2$, tol: $\mathbb\{R\}$) $\to \mathbb\{R\}[]$:
  t_adp, y, dt: $\mathbb\{R\}[]$
  t_adp[0] = $t_0$, i = 0
  y[0] = 0
  dt[0] = sqrt(2*tol/$\nabla$f(t_adp[0]))
  while (t_adp[i] < $t_e$):
    y[i+1] = y[i] + f(t_adp[i])*dt[i]
    dt[i+1] = sqrt(2*tol/$\nabla$f(t_adp[i]))
    t_adp[i+1] = t_adp[i] + dt[i+1]
    i = i + 1
  y
```

It can be observed that initially the step-size was high, which
gradually decreases because of the increasing value of $f'$.
