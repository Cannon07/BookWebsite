# SE(3) Transformer \{#chap:se3_transformer\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:transformer\]](#chap:transformer)\{reference-type="ref+label"
reference="chap:transformer"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

The $SE(3)$-Transformer [@fuchs2020se], [@unke2021se] is a version of
the transformer which introduces a self-attention mechanism which is
equivariant to $SE(3)$. This mechanism allows for modeling of larger
atomic point clouds in a fashion equivariant to rotations and
translations. Each layer of the SE(3)-transformer converts a point cloud
to a point cloud.

![<https://arxiv.org/pdf/2006.10503.pdf>](figures/Differentiable Physics/Molecular ML/SE3 Transformer/point_cloud.png)\{#fig:my_label\}

## $SO(3)$ Representations

To design the $SE(3)$ transformer, we use a few useful facts about
representations of $SO(3)$. In particular, all representations $\rho$ of
$SO(3)$ can be written in the form

$$\begin\{aligned\}
\rho(g) &= Q^T \left [ \bigoplus_\ell D_\ell(g) \right ]Q
\end\{aligned\}$$ Here $D_\ell$ is the Wigner-D matrix of order $\ell$ and
$Q$ is an orthogonal change of basis matrix. The Wigner-D matrices
provide irreducible representations of $SO(3)$ and all other
representations can be decomposed in terms of these representations.

## Mathematical Framework

As with the tensor field network, we specify that the input is given by
a point cloud $$\begin\{aligned\}
f(x) &= \sum_\{j=1\}^N f_j \delta(x - x_j)
\end\{aligned\}$$ Here $f_j$ are local point features. Each such vector is
decomposed into subvectors of different types $$\begin\{aligned\}
f_j &= \oplus_\ell f^\ell_j
\end\{aligned\}$$ The type here corresponds to the order of the Wigner-D
matrices we saw earlier. Local point features may include local atomic
features computed by a heuristic molecular featurization algorithm.

We write a tensor field network layer as $\textrm\{TFN\}$. The output of a
point cloud after passing through a tensor field network layer can be
written as $$\begin\{aligned\}
\textrm\{TFN\}(f)^\ell &= \sum_\{k\geq 0\} \sum_\{j=1\}^n W^\{\ell k\}(x_j - x_i) f^k_\{j\}
\end\{aligned\}$$ Here, we have written the output portion of order
$\ell$. The weight kernel $W^\{\ell k\}$ is of type $$\begin\{aligned\}
W^\{\ell k\}: \mathbb\{R\}^3 \mapsto \mathbb\{R\}^\{(2\ell + 1) (2 k + 1)\}
\end\{aligned\}$$ and maps the vector $x_j - x_i \in \mathbb\{R\}^3$ to
transformation matrix. Note that this equation fits into the broader
messsage passing framework we learned about earlier. To make this update
more closely match the transformer update equations, we can split out
the self-interaction term and rewrite the update rule as
$$\begin\{aligned\}
f^\ell_i &= w^\{\ell \ell\} f^\ell_i + \sum_\{k \geq 0\} \sum_\{i \neq j\} W^\{\ell k\}(x_j - x_i) f^k_j
\end\{aligned\}$$ Note that the tensor field network message updates by
construction are $SE(3)$ equivariant.

## $SE(3)$ Transformer Design

![<https://arxiv.org/pdf/2006.10503.pdf>](figures/Differentiable Physics/Molecular ML/SE3 Transformer/message_passing.png)\{#fig:my_label
width="80%"\}

The $SE(3)$ transformer modifies the tensor field network by adding on
attention weights, $\alpha_\{ij\}$ on the update rule. These weights have
to be constructed to be $SE(3)$ invariant. We also specify that messages
are only passed in local neighborhoods (atoms that are within some
distance cutoff of other atoms for example) instead of global updates.
Mathematically, the $SE(3)$ transformer update rule is given by the
equation $$\begin\{aligned\}
\mathrm\{SE3Transformer\}(f_i)^\ell &= W^\{\ell \ell\} + \sum_\{k \geq 0\} \sum_\{j \in \mathcal\{N\}_i \setminus i\} \alpha_\{ij\} W^\{\ell k\}(x_j - x_i) f^k_j
\end\{aligned\}$$ Note that the main difference between the $SE(3)$
transformer and the tensor field network is the addition of
self-attention weights.

We need to make the weights equivariant to $SE(3)$. We do so by the
following update rule $$\begin\{aligned\}
\alpha_\{ij\} &= \frac\{\exp(q^T_i k_\{ij\})\}\{\sum_\{j' \in \mathcal\{N\}_i \setminus i\} \exp (q_i^T k_\{ij'\})\}
\end\{aligned\}$$ where the queries $q_i$ and keys $k_\{ii\}$ are given by
$$\begin\{aligned\}
q_i &= \bigoplus_\{\ell \geq 0\} \sum_\{k \geq 0\} W^\{\ell k\}_Q(x_j - x_i) f^k_i \\
k_\{ij\} &= \bigoplus_\{\ell \geq 0\} \sum_\{k \geq 0\} W^\{\ell k\}_K (x_j - x_i) f^k_j
\end\{aligned\}$$ Here we use the different subscripts
$W^\{\ell k\}_K, W^\{\ell k\}_Q$ to emphasize that these are different
learned matrices or tensor field network filters. It can be proven that
this update rule constructs a $SE(3)$ equivariant attention since we
inherit the $SE(3)$ equivariance of the tensor field networks.

The $SE(3)$ transformer uses one additional technical trick. It replaces
the scalar self attention $w^\{\ell \ell\}$ with a learned update
$$\begin\{aligned\}
w^\{\ell \ell\}_\{c' c\} &= MLP\left ( \bigoplus_\{c c'\}(f^\ell_\{i c'\})^T f^\ell_\{i, c\} \right )
\end\{aligned\}$$ Here $c, c'$ are channels (the update equations above
have to be extended to allow for learning multiple channels in the
natural fashion). This update rule constructs a learnable self-attention
that blends feature information across multiple channels.
