# Physics Informed Neural Networks \{#chap:pinns\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:pde\]](#chap:pde)\{reference-type="ref+label"
reference="chap:pde"\},
[\[chap:numerical_pde\]](#chap:numerical_pde)\{reference-type="ref+label"
reference="chap:numerical_pde"\},
[\[chap:univ_approx\]](#chap:univ_approx)\{reference-type="ref+label"
reference="chap:univ_approx"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Physics-informed neural networks (PINNs) are a family of neural networks
trained to solve supervised learning tasks by using any given law of
physics, typically described by general nonlinear partial differential
equations (PDEs)[@raissi2019physics], as a regularizer. PINNs leverage
the fact that deep neural networks are universal function approximators
to solve nonlinear problems without needing to commit to prior
assumptions such as linearization, or local time-stepping. PINNs
differentiate neural networks with respect to their input coordinates
and model parameters, and can be constrained to respect any symmetries,
invariances, or conservation principles originating from the physical
laws that govern the observed data. This simple yet powerful
construction allows us to construct a new class of numerical solvers for
partial differential equations, as well as new data-driven approaches
for model inversion and systems identification.

## PINNs Formulation

Consider a parametrized and nonlinear partial differential equations of
the following general form $$\begin\{aligned\}
    u_t + \mathcal\{N\}[u;\theta]=0,\quad x \in \Omega, t\in [0,T]
    \label\{eq_generalform\}
\end\{aligned\}$$ where $u(t, x)$ denotes the hidden solution,
$\mathcal\{N[\cdot;\theta]\}$ is a nonlinear operator parametrized by
$\theta$, and $\Omega$ is a subset of $\mathbb\{R\}^D$. This formulation
applies to a wide range of problems in physical systems including
conservation laws, diffusion processes, kinetic equations and physical
systems.

We can speak of two methods of solving PINNs.

1.  Given fixed model parameters $\theta$, what can be said about the
    unknown hidden state $u(t, x)$ of the system?

2.  Can we determine the parameters $\theta$ that best describe observed
    data?

The construction and training of PINNs can be done through two distinct
algorithms based on the specific application area and the nature of the
associated ground truth data:

1.  Continuous-time

2.  Discrete-time

We will learn about these two approaches in the following section.

## Continuous Time Approximation

The approach below defines an algorithm that can be used to construct a
continuous time model. Let's define $f(t, x)$ to be given by the
equation on the left hand side,

$$\begin\{aligned\}
    f &=u_t + \mathcal\{N\}[u; \theta]
    \label\{eq_contTime\}
\end\{aligned\}$$

Our approach is to approximate $u(t,x)$ by a deep neural network. This
assumption along with equation
[\[eq_contTime\]](#eq_contTime)\{reference-type="ref"
reference="eq_contTime"\} gives rise to the physics-informed neural
network $f(t,x)$. The network can be derived by applying automatic
differention to compute $\mathcal\{N\}[u; \theta]$. The shared parameters
$\theta$ between the neural networks $u(t, x)$ and $f(t, x)$ can be
learned by minimizing the mean-squared error loss

$$\begin\{aligned\}
    \mathcal\{L\}_\{MSE\}=\textrm\{MSE\}_u + \textrm\{MSE\}_f 
\end\{aligned\}$$

where $$\begin\{aligned\}
    \textrm\{MSE\}_u &=\frac\{1\}\{N\} \sum_\{i=1\}^\{N\}|u(t^i,x^i)-u^i|^2\\
    \textrm\{MSE\}_f &=\frac\{1\}\{N\} \sum_\{i=1\}^\{N\}|f(t^i,x^i)|^2
\end\{aligned\}$$ Above, the initial and boundary training data on
$u(t, x)$ are denoted by $$\begin\{aligned\}
\{\{t^i, x^i, u^i\}\}^\{N\}_\{i=1\}
\end\{aligned\}$$

The loss function $\textrm\{MSE\}_u$ penalizes deviations from the initial
and boundary data while $\textrm\{MSE\}_f$ enforces the structure imposed
by the general equation (equation
[\[eq_generalform\]](#eq_generalform)\{reference-type="ref"
reference="eq_generalform"\}) at a finite set of collocation points.

We don't need to optimize $u$ and $f$ at the same points so we can also
generalize this loss function to $$\begin\{aligned\}
    \textrm\{MSE\}_u &=\frac\{1\}\{N_u\} \sum_\{i=1\}^\{N_u\}|u(t_u^i,x_u^i)-u^i|^2\\
    \textrm\{MSE\}_f &=\frac\{1\}\{N_f\} \sum_\{i=1\}^\{N_f\}|f(t_f^i,x_f^i)|^2
\end\{aligned\}$$

Here initial and boundary training data on $u(t, x)$ are denoted by
$$\begin\{aligned\}
\{\{t^i_u, x^i_u, u^i\}\}^\{N_u\}_\{i=1\}, \quad \{\{t^i_f, x^i_f\}\}^\{N_f\}_\{i=1\}
\end\{aligned\}$$ specify the collocation points for $f(t, x)$.

``` \{.python language="python"\}

class PINN(equation: Expression, u: Module):

  $\mathcal\{N\}$ = tensorize(equation)
  f = u + $\mathcal\{N\}$(u)

  def $\lambda$(x, t):
    return u(x, t)
    
  def loss(X, U):
    return sum($\|$u(X[i]) - U[i]$\|^2$, i) + sum($\|$f(X[i])$\|^2$, i)
```

## Discrete-Time PINNs

The approach discussed above approximates the solution $u$ across a
continuous range of time points. For some problems, it can be useful to
discretize time instead. The general form of Runge-Kutta methods
([\[chap:runge_kutta\]](#chap:runge_kutta)\{reference-type="ref+label"
reference="chap:runge_kutta"\}) with $q$ stages is applied to the
governing equation to arrive at the following:

$$\begin\{aligned\}
u^\{n+c_i\}&=u^n-\Delta t\sum_\{i=1\}^\{q\}a_\{ij\}\mathcal\{N\}[u^\{n+c_j\}; \theta], \quad i=1,\cdot\cdot\cdot,q,\\
u^\{n+1\}&=u^n-\Delta t\sum_\{i=1\}^\{q\}b_\{j\}\mathcal\{N\}[u^\{n+c_j\}; \theta].
\end\{aligned\}$$

Here $a_\{ij\}$ and $b_j$ are the Runge-Kutta coefficients for $q$ stages.
In the formulation, $$\begin\{aligned\}
u^\{n+c_j\}(x) &=u(t^n+c_j\Delta t,x)
\end\{aligned\}$$ for $j=1,\cdot\cdot\cdot,q$. The above equations can
also be expressed as

$$\begin\{aligned\}
u^n &=u^n_i,\quad i=1,\cdot\cdot\cdot,q,\\
u^n &=u^n_\{q+1\}
\end\{aligned\}$$

where $$\begin\{aligned\}
u_i^n&:=u^\{n+c_i\}+\Delta t\sum_\{j=1\}^\{q\}a_\{ij\}\mathcal\{N\}[u^\{n+c_j\}], \quad i=1,\cdot\cdot\cdot,q,\\
u^\{n\}_\{q+1\}&=u^\{n+1\}+\Delta t\sum_\{i=1\}^\{q\}b_\{j\}\mathcal\{N\}[u^\{n+c_j\}].
\end\{aligned\}$$

We can place a multi-output neural network prior on $$\begin\{aligned\}
\label\{eq_discrete\}
    [u^\{n+c_1\}(x),\cdot\cdot\cdot,u^\{n+c_q\}(x),u^\{n+1\}(x)]
\end\{aligned\}$$ The assumption along with equations
[\[eq_discrete\]](#eq_discrete)\{reference-type="ref"
reference="eq_discrete"\} result in a physics-informed neural network
that takes $x$ as an input and gives as the output $$\begin\{aligned\}
    [u^\{n\}_1(x),\cdot\cdot\cdot,u^\{n\}_q(x),u_\{q+1\}^\{n+1\}(x)]
\end\{aligned\}$$

The discrete time network is trained by minimizing the following sum of
squares loss (SSE)

$$\begin\{aligned\}
    \mathcal\{L\}_\{SSE\} &= \textrm\{SSE\}_n + \textrm\{SSE\}_b
\end\{aligned\}$$ where $$\begin\{aligned\}
    \textrm\{SSE\}_n &= \sum_\{j=1\}^\{q+1\} \sum_\{i=1\}^\{N_n\} |u^n_j(x^\{n,i\}) - u^\{n,i\}|^2 \\
    \textrm\{SSE\}_b &= \sum_\{i=1\}^q f(u^\{n+c_i\}) + f(u^\{n+1\}) \\
\end\{aligned\}$$ where $f$ is the general nonlinear constraint. In the
above equations, the dataset at time $t^n$ is given by $$\begin\{aligned\}
    \{x^\{n,i\}, u^\{n,i\}\}_\{i=1\}^\{N_n\}
\end\{aligned\}$$

## Exercises

1.  Write down the explicit continuous-time PINN loss function for
    Schrodinger's equation in 1 dimension.
