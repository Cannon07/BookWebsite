# Allegro \{#chap:allegro\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

The field of machine learning potentials uses learned potential energy
functions which are trained on a limited library of quantum data to
match energies and forces.

$$\begin\{aligned\}
F_i &= \frac\{\partial E\}\{\partial \vec\{r_i\}\}
\end\{aligned\}$$

The problem of learning machine learning potentials is heavily
constrained by symmetries. In particular, rotations, inversions,
translations and permutations are all symmetries that must be obeyed.
How can we build models that respect these symmetries? For a long time,
this was $O(3)$ rotationally invariant quantities. Invariant features
don't change with rotation while equivariant features rotate or
transform alongside the source data.

## Constructing Equivariant Features

In practice, equivariance dramatically improves machine learning
potentials. They have improved scaling laws and lower data requirements.

Equivariant features are made up of a collection of geometric tensors
which inhabit irreducible representations (irreps). For $O(3)$ the
irreps are index by $$\begin\{aligned\}
(\ell, p)
\end\{aligned\}$$ As $\ell$ grows, you get higher order tensors. $V$
consists of a set of equivariant features.

$$\begin\{aligned\}
(x \otimes y)_\{\ell_\{out\}, m_\{out\}\} &= \sum_\{m_1, m_2\} \begin\{pmatrix\}
\ell_1 & \ell_2 & \ell_\{out\} \\
m_1 & m_2 & m_\{out\}
\end\{pmatrix\} x_\{\ell_1, m_1\} x_\{\ell_2, m_2\}
\end\{aligned\}$$

In message passing architectures, as we increase the number of message
passing iterations, we get a massive increase in the number of atoms
which have to communicate in updates.

Recall that energies are given by $$\begin\{aligned\}
E &= \sum_i E_i
\end\{aligned\}$$ In Allegro, we instead do pre-ordered pairs
$$\begin\{aligned\}
E &= \sum_\{i\} \sum_\{j \in \mathcal\{N\}(i)\} 
\end\{aligned\}$$

## Applying the Density Trick

Allegro has a two track architecture. First is invariants with all
architectures allowed. Then we have equivariants for $E(3)$. The main
reason is that invariants are much cheaper on existing hardware. So we
have a large set of scalar invariants which control a small set of
equivariant. $$\begin\{aligned\}
x^\{ij,L\}, V^\{ij,L\}
\end\{aligned\}$$ The two-body MLP takes as input $$\begin\{aligned\}
(Z_i, Z_j, \|\vec\{r\}_\{ij\}\|), Y
\end\{aligned\}$$ The tensor networks want to get access to higher order
interactions. The tensor layer must interact tensor features of
different orders. The tensor product is a powerful tool to do this.

$$\begin\{aligned\}
V^\{ij, L\} &= \sum_\{k \in N(i)\} w^\{ik,L\} (V^\{ij,L-1\} \otimes \vec\{Y\}^\{ik\})
\end\{aligned\}$$ This gives bad scaling due to the tensor computation. We
can rewrite as $$\begin\{aligned\}
&= \sum_\{k \in N(i)\} V^\{ij,L-1\} \otimes w^\{ik,L\}\vec\{Y\}^\{ik\} \\
&= V^\{ij,L-1\} \otimes \sum_\{k \in N(i)\} w^\{ik,L\}\vec\{Y\}^\{ik\}
\end\{aligned\}$$ This trick allows us to only do one tensor product
rather than one for all neighbors. This update rule also enforces strict
locality.

This technique is commonly called the density trick. The layer maps
scalars and tensors back to scalars and tensors $$\begin\{aligned\}
x^\{ij,L-1\}, x^\{ik, L-1\}, V^\{ij,L-1\}_\{n,\ell,p\} \mapsto x^\{ij,L\}, V^\{ij,L\}_\{n,\ell,p\}
\end\{aligned\}$$

The Allegro model is $O(N)$ scaling in the number of atoms. It is $O(M)$
in the number neighbors/atoms. Note that equivariant transformers are
$O(M^2)$. It is also $O(1)$ in the number of chemical species. Other
local descriptors like SOAP have worse scaling.

## Discussion

The tensor order should be thought of as a type of Taylor expansion.
Higher tensor orders $\ell$ correspond to higher spherical harmonics
potentials. In practice, going up to $\ell=2$ seems to make a big
difference but past that it's not as useful.

There's also early empirical evidence that message passing struggles to
learn long range correlations past a point, which may be why the Allegro
trick works.mThe observed GPU scaling is very clean and scales nicely to
multiple GPUs.

Reactive potentials are the hardest test case for all ML potentials.
Forces converge better than energies but it's not fully understood why.

MACE has a similar goal to Allegro but operates on nodes and has 2
layers of message passing. It has a similar effect in lowering the
effective cutoff. It still has 2 layers with 4 angstrom layers so cutoff
can still be large. MACE iterated tensor product lacks a feedbacking
mechanism between the two tracks.
