# Physics Informed Neural Networks for Fluid Modeling \{#chap:pinns_fluid\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:qft_basics\]](#chap:qft_basics)\{reference-type="ref+label"
reference="chap:qft_basics"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter, we'll look at a physics-informed neural network based
approach to fluid flow field modeling using an approach called hidden
fluid dynamics.

## Fluid Flow Dynamics

Quantifying fluid flow dynamics in physical systems requires detailed
knowledge of velocity and pressure fields. Achieving this quantitative
understanding has been the centerpiece of experimental and theoretical
fluid mechanics for several centuries. Experimental techniques including
point measurements and smoke/dye visualization have been used for
qualitative characterization of entire flow fields. However,
quantitative flow visualizations such as particle image velocimetry and
magnetic resonance imaging are limited to small domains and laboratory
settings. The quantification of velocity fields for internal flows (such
as blood flow) is very difficult or impractical although experimental
measurements of external flows such as flow past a bluff object in small
subdomains are obtained relatively easily. Despite substantial advances
in experimental fluid mechanics, the use of measurements to reliably
infer fluid velocity and pressure or stress fields is not a
straightforward task. Therefore, computational approaches are employed
to achieve detailed knowledge of velocity and pressure fields.

From a theoretical standpoint, the governing equations of fluid
mechanics have been derived from conservation laws (conservation of
mass, momentum, and energy), leading to partial differential equations
(PDEs) such as the well-known Navier-Stokes (NS) equations. Accurate
solutions of such equations are now available using direct numerical
simulations or other approximate forms, but the computational cost is
prohibitively high for realistic conditions and "inverse" problems.
Machine learning has proven useful in answering the question of
leveraging the underlying laws of physics to extract quantitative
information from available flow visualizations.

## Machine Learning Approaches for Fluid Flow Fields

Quantitative information from flow visualizations is typically extracted
in the form of passive scalars, such as the transport of dye or smoke in
physical systems and contrast agents in biological systems. This data
can be used to train machine learning systems to learn fluid flow
fields.

A powerful approach is called hidden fluid mechanics (HFM), that
simultaneously exploits the information available in snapshots of flow
visualizations and the NS equations, combined in the context of physics
informed deep learning. In mathematics, statistics, and computer
science---in particular, in machine learning and inverse
problems---regularization is the process of adding information in order
to prevent overfitting or to solve an ill-posed problem. The prior
knowledge of the NS equations can be used to create important structure
to effectively regularize the minimization procedure in the training of
neural networks.

### Training and Input Data

For this example of fluid flow, let's consider the transport of a
passive $c(t,x,y,z)$ by a velocity field

$$\begin\{aligned\}
u(t,x,y,z)=[u(t,x,y,z), v(t,x,y,z), w(t,x,y,z)]
\end\{aligned\}$$

which obeys the incompressible NS equations. The passive scalar, a dye
in this case, is advected by the flow and diffused but has no dynamical
effect on the fluid motion itself. The only observables are assumed to
be noisy data

$$(t^n,x^n,y^n,z^n,c^n)_\{n=1\}^\{N\}$$

on the concentration $c(t,x,y,z)$ on the passive scalar. This set of
time-space coordinates represents a single point cloud of scattered data
consisting of $N$ data points ($t^n$, $x^n$, $y^n$, $z^n$) and their
corresponding labels $c^n$, the measured concentration value at the
point ($t^n$, $x^n$, $y^n$, $z^n$). Here, the superscript $n$ denotes
the $n$th data point and runs from $1$ to $N$.

Given such data, scattered in space and time, we are interested in
inferring the latent (hidden) quantities of interest $u(t, x, y, z)$,
$v(t, x, y, z)$, and $w(t, x, y, z)$ and pressure $p(t, x, y, z)$. The
aim is to develop a flexible framework that can deal with data acquired
in arbitrarily complex domains such as flow around vehicles or blood
flow in brain or aortic aneurysms. The function

$$(t,x,y,z)\rightarrow(c, u, v,w,p)$$

can be approximated by means of a physics-uninformed deep neural
network, which was followed by a physics-informed deep neural network

$$(t, x, y, z)\rightarrow(e1, e2, e3, e4, e5)$$

in which the coupled dynamics of the passive scalar and the NS equations
were encoded in the outputs $e1$, $e2$, $e3$, $e4$, and $e5$ by using
automatic differentiation. Here, $e1$ is the residual of the transport
equation modeling the dynamics of the passive scalar, and $e2$, $e3$,
and $e4$ represent the momentum equations in $x$, $y$, and $z$
directions, respectively. Moreover, $e5$ corresponds to the residual of
the continuity equation. In the following, we minimize the norms of
these residuals to satisfy the corresponding equations that describe the
underlying laws of fluid mechanics. The shared parameters of the physics
uninformed neural networks for $c$, $u$, $v$, $w$, and $p$ and the
physics-informed ones $e1$, $e2$, $e3$, $e4$, and $e5$ can be learned by
minimizing the following mean squared error loss function as discussed
below.

### Hidden Fluid Mechanics Approach

The approach to learn the velocity fields involves a two-component deep
neural network where the first component learns the mapping from
space-time coordinates to a passive scalars

The loss function can be written as
$$MSE=\frac\{1\}\{N\}\sum_\{n=1\}^\{N\}|c(t^n,x^n,y^n,z^n)-c^n|^2+\sum_\{i=1\}^\{5\}\frac\{1\}\{M\}\sum_\{m=1\}^\{M\}|e_i(t^m,x^m,y^m,z^m)|^2$$
where the first term corresponds to the training data
$(t^n,x^n,y^n,z^n,c^n)_\{n=1\}^\{N\}$ on the concentration of the passive
scalar, whereas the last term enforces the structure imposed by the NS
and transport equations at a finite set of residual points
$(t^m,x^m,y^m,z^m)_\{m=1\}^\{M\}$ whose number and locations can be
different from the actual training data. The number and locations of
these points at which we penalize the equations are in our full control,
whereas the data on the concentration of the passive scalar are
available at the measurement points. Mini-batch gradient descent
algorithms and their modern variants such as the Adam optimizer enable
us to penalize the equations at virtually "infinitely" many points.
Moreover, it can be demonstrated that in addition to the velocity and
pressure fields, it is possible to discover other unknown parameters of
the flow, such as the Reynolds and P$Ã©$clet numbers, directly from the
data on concentration of the passive scalar.

The framework can be applied to life systems where the goal is to learn
the solution field of quantities of interest and can be extended to
other disciplines. One other example from the area of electromagnetics
could involve the inference of the magnetic field when data is given on
the electric field with the knowledge of Maxwell's equations.
