# Multilayer Perceptrons \{#chap:mlpd\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

The limitation of linear models is expressivity. There are many complex
interactions which can't be modeled simply with linear interactions.
Practitioners often bypass this limitation by adding in new \"features\"
that encode pairwise interactions, but ideally we would like a model
that can natively learn more complex interactions. We want to add some
nonlinearity to our function class $\mathcal\{G\}$. Let $\sigma$ be some
nonlinear function. Then we can define a model by

$$\begin\{aligned\}
h[w,b](x) = \sigma(w^T x + b) 
\end\{aligned\}$$

There are many choices commonly made for the function $\sigma$. One
common choice is the sigmoidal function

$$\begin\{aligned\}
\sigma(x) = \frac\{1\}\{1+ e^\{-x\}\}
\end\{aligned\}$$

Note that this function ranges from 0 to 1 (see
FigureÂ [\[fig:logistic\]](#fig:logistic)\{reference-type="ref"
reference="fig:logistic"\}). We can turn this definition into code.

::: marginfigure
![image](figures/Differentiable Models/multilayer_perceptrons/Logisticurve0.png)
:::

``` \{.python language="python"\}
def $\sigma$(x: $\mathbb\{R\}$) $\to$ $\mathbb\{R\}$:
  1/(1 + exp(-x))

class h(w: $\mathbb\{R\}^n$, b: $\mathbb\{R\}$):
  def $\lambda$(x: $\mathbb\{R\}^n$) $\to$ $\mathbb\{R\}$:
    $\sigma$(w$^T$ * x + b)
```

With this choice of $\sigma$, the function $h_\{(w,b)\}$ is commonly
referred to as a \"perceptron.\" It corresponds to a very crude model of
a neuron. The core idea is that this function corresponds to the
following \"firing\" rule

$$\begin\{aligned\}
 h[b](x) = \begin\{cases\}
    1 & \sigma(w^T x + b) > 0.5 \\
    0 & \textrm\{otherwise\} \\
    \end\{cases\} 
\end\{aligned\}$$

Perceptrons spurred a lot of excitement when they were first introduced
in the 1950s. Minsky and Papert showed in their book \"Perceptrons\"
[@minsky2017perceptrons] that perceptrons were incapable of learning
simple functions such as XOR. This caused a broad loss of interest in
Perceptrons and neural networks more broadly. Interest only revived when
researchers realized that many of the limitations of perceptrons could
be overcome by chaining them into multiple layers.

To implement multilayer perceptrons, we alter our function $h[w, b]$ to
return a vector of outputs rather than a scalar. Let
$W \in \mathbb\{R\}^\{P \times M\}$ and let $c \in \mathbb\{R\}^\{P\}$. Then we
defined $h[W, c]: \mathbb\{R\}^M \to \mathbb\{R\}^P$ as

$$\begin\{aligned\}
 h[W, c](x) = W x + c 
\end\{aligned\}$$

We then introduce a convenient bit of notation. If $x$ is a vector, we
let $\sigma(x)$ denote the component wise application of $\sigma$ to
every scalar in the vector. We're now ready to introduce a simple
multilayer perceptron

$$\begin\{aligned\}
 h[W_0, c_0, w_1, b_1](x) = \sigma(w_1^T\sigma(W_0 x + c_0) + b_1) 
\end\{aligned\}$$

<figure id="fig:my_label">
<p><img
src="figures/Differentiable Models/multilayer_perceptrons/mlp_example_0.png"
alt="image" /> <span id="fig:my_label"
data-label="fig:my_label"></span></p>
</figure>

We say this this model has one *hidden layer* since the inner layer
$\sigma(W_0x + c_0)$ isn't directly reflected in the output. We can
transform this definition into Physika code in a straightforward
fashion.

    class OneLayerNet($W_0$: $\mathbb\{R\}$[N], $w_1$: $\mathbb\{R\}$, $b_0$: $\mathbb\{R\}$, $b_1$: $\mathbb\{R\}$):
      def $\lambda$(x: $\mathbb\{R\}$[N]) $\to$ $\mathbb\{R\}$:
        $\sigma$($w_1$*$\sigma$($W_0$*x+$b_0$)+$b_1$)

There's no reason we have to stop at just one layer here. We can go
deeper. Here's an example with two hidden layers for example.

$$h[W_0, c_0, W_1, c_1, w_2, b_2](x) = \sigma(w_1^T\sigma(W_1 \sigma(W_0 x + c_0) + c_1) + b_1)$$

You might now start to see where the term *deep learning* originates
from. As we keep adding more and more hidden layers, we make the model
deeper. Let's take a look at some simple Physika code for a $n$ hidden
layer network.

``` \{.python language="python"\}
class FullyConnectedNetwork($\sigma$: $\mathbb\{R\}$ $\to$ $\mathbb\{R\}$, W: $\mathbb\{R\}$[n, N, N], B: $\mathbb\{R\}$[n, N], w: $\mathbb\{R\}^N$, b: $\mathbb\{R\}$):
  def $\lambda$(x: $\mathbb\{R\}$[N]) $\to$ $\mathbb\{R\}$:
    for i: x = $\sigma$(W[i] * x + B[i])
    w$^T$ x + b
```
