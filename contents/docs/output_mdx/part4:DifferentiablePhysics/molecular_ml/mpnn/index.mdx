# Message Passing Neural Networks \{#chap:mpnn\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:graphconvs\]](#chap:graphconvs)\{reference-type="ref+label"
reference="chap:graphconvs"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Message Passing Neural Networks (MPNN) provide a generalized
mathematical formulation [@gilmer2017neural] that aims to formulate a
single framework for graph convolutions. This formulation splits the
prediction process into two phases, the message passing phase and the
readout phase.

Multiple message passing layers are stacked to extract abstract
information about the graph, while the readout phase maps the graph to
its properties. As an example, we could use a fully connected network as
the message passing function and a set2set model as readout function. In
the message passing phase, an edge-dependent neural network maps all
neighbouring atoms' feature vectors to update messages, which are then
merged using gated recurrent units. In the final readout phase, feature
vectors for all atoms are regarded as a set, then an LSTM using
attention mechanism can be applied on the set for multiple steps, with
its final state used as the output for the molecule.

## Message Passing Stage

Suppose that a graph is defined by the pair of vertices and edges
$$\begin\{aligned\}
\mathcal\{G\} &= (\mathcal\{V\}, \mathcal\{W\})
\end\{aligned\}$$ Let $h_v^t$ denote the feature vector for node $v$ at
step $t$. Let $e_\{vw\}$ denote the bond feature vector between $v$ and
$w$. The message passing equations are given by the update rules

$$\begin\{aligned\}
    m^\{t+1\}_v &= \sum_\{w \in N(v)\} M_t(h^t_v, h^t_w, e_\{vw\}) \\
    h^\{t+1\}_v &= U_t(h^t_v, m^\{t+1\}_v)
\end\{aligned\}$$

Here $N(v)$ denotes the neighbors of that node. $M_t$ and $U_t$ are
message and update functions respectively. $h^t_v$ is the hidden state
at node $v$ and $m^\{t+1\}_v$ is the update message for node $v$ to update
to time $t+1$. The final prediction is made with a readout function $R$
over all the hidden states in the graph. $$\begin\{aligned\}
    \hat\{y\} &= R(\{h^T_v | v \in G \})
\end\{aligned\}$$

We below implement a message passing layer in Physika, where `T` is the
number of message passing layers and `M` and `U` represent the message
processing update functions.

``` \{.python language="python"\}

class MessagePassingNeuralNetwork(T: $\mathbb\{N\}$, M: Module, U: Module, R: $\mathbb\{N\}$):

  def $\lambda$((V, W): Graph):
    for t in T:
      for node in V:
        message = 0
        for nbr in neighbors(node):
          message += M(node.h, nbr.h, edge_weights(node, nbr))
        node.h = U(node.h, message)
    return G
```

## Neural Graph Fingerprints are Message Passing Netorks

Most graph convolutional networks can be recast as message passing
neural networks. We work through one such update. Consider the neural
graph fingerprint [@duvenaud2015convolutional]. In this case, the
message function is concatenation $$\begin\{aligned\}
    M(h_v, h_w, e_\{vw\}) &= (h_w, e_\{vw\})
\end\{aligned\}$$ The update function is the sigmoid function $\sigma$
$$\begin\{aligned\}
    U_t(h^t_v, m^\{t+1\}_v) &= \sigma(H^\{\textrm\{deg\}(v)\}_t m^\{t+1\}_v)
\end\{aligned\}$$ Here $\textrm\{deg\}(v)$ is the degree of node $v$ and
$H^N_t$ is a learned matrix of weights for weight possible node degree.
The update function is given by $$\begin\{aligned\}
    R&= f\left ( \sum_\{v, t\} \textrm\{softmax\}(W_t h^t_v)\right )
\end\{aligned\}$$ where $f$ is a neural network and $W_t$ are learned
weight matrices.

## Exercises

1.  Prove that Weave convolutions are a form of mesage passing neural
    network.
