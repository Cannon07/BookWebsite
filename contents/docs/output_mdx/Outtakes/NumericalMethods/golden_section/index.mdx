# Optimization: Golden Search \{#chap:golden_search\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:optimization\]](#chap:optimization)\{reference-type="ref+label"
reference="chap:optimization"\},
[\[chap:bracketing\]](#chap:bracketing)\{reference-type="ref+label"
reference="chap:bracketing"\},
[\[chap:newton_raphson\]](#chap:newton_raphson)\{reference-type="ref+label"
reference="chap:newton_raphson"\}\
**Difficulty Level:** \*\

------------------------------------------------------------------------

In this chapter we will learn about basic optimization and the
difference between one-dimensional and multidimensional optimization. We
will study the difference between global and local optima and then
define the golden ratio and how to use it to make one dimensional
optimization efficient. We will then explain how to locate the optimum
of a single variable function with golden section search.

## Optimization

Optimization is the process of creating something that is as effective
as possible. Optimization deals with finding the maxima and minima of a
function that depends on one or more variables. The goal is to determine
the values of the variables that yield maxima or minima for the function
which can then be substituted back into the function to compute its
optimal values.

::: marginfigure
![image](figures/part1b/golden_section/optimization.png)\{width="2.4in"\}
:::

Optimization is similar in spirit to the root-location methods we
covered in previous chapters, both involve guessing and searching for a
point on a function. The fundamental difference between the two types of
problems is illustrated in Figure
[\[fig:1\]](#fig:1)\{reference-type="ref" reference="fig:1"\}. Root
location involves searching for the location where the function equals
zero. In contrast, optimization involves searching for the function's
extreme points.

As can be seen in Figure [\[fig:1\]](#fig:1)\{reference-type="ref"
reference="fig:1"\}, the optima are the points where the curve is flat.
In mathematical terms, this corresponds to the $x$ value where the
derivative $f' (x)$ is equal to zero. Additionally, the second
derivative, $f''(x)$, indicates whether the optimum is a minimum or a
maximum: if $f(x) < 0$, the point is a maximum; if $f(x) > 0$, the point
is a minimum. Differentiating the function and locating the root of the
new function $f'(x) = 0$ is a possible strategy for finding the optima.
A variety of direct numerical optimization methods are available for
both one-dimensional and multidimensional problems.

::: marginfigure
![image](figures/part1b/golden_section/onedimensional_1.png)\{width="2.4in"\}
:::

One-dimensional problems involve functions that depend on a single
dependent variable. As in Figure
[\[fig:2\]](#fig:2)\{reference-type="ref" reference="fig:2"\}a, the search
then consists of climbing or descending one-dimensional peaks and
valleys. Multidimensional problems involve functions that depend on two
or more dependent variables. Two-dimensional optimization can again be
visualized as searching out peaks and valleys (Figure
[\[fig:2\]](#fig:2)\{reference-type="ref" reference="fig:2"\}b). The
process of finding a maximum versus finding a minimum is essentially
identical because the same value $x^*$ both minimizes $f(x)$ and
maximizes $-f(x)$. This equivalence is illustrated graphically for a
one-dimensional function in Figure
[\[fig:2\]](#fig:2)\{reference-type="ref" reference="fig:2"\}a.

## One-dimensional Optimization

Consider the function depicted in Figure
[\[fig:3\]](#fig:3)\{reference-type="ref" reference="fig:3"\}. Recall that
root location was complicated by the fact that several roots can occur
for a single function. Similarly, both local and global optima can occur
in optimization. As can be seen from the Figure
[\[fig:3\]](#fig:3)\{reference-type="ref" reference="fig:3"\}, the *global
optimum* represents the very best solution. A *local optimum*, though
not the very best, is better than its immediate neighbors. Cases that
include local optima are called *multimodal*. For simple functions, we
will almost always be interested in finding the global optimum, but we
must be concerned about mistaking a local result for the global optimum.
Just as in root location, optimization in one dimension can be divided
into bracketing and open methods.

::: marginfigure
![image](figures/part1b/golden_section/onedimensional_2.png)\{width="2.4in"\}
:::

## Golden-Section Search

Consider a continuous function $f(x)$ over the interval $[x_l,x_u]$ that
contains a single minimum as shown in Figure
[\[fig:4\]](#fig:4)\{reference-type="ref" reference="fig:4"\}a , and hence
is called *unimodal*. Instead of using a single intermediate value to
detect a sign change, we would need two intermediate function values to
detect whether a minimum occurred. Consider two intermediate points
($x_1$ and $x_2$) within the interval at a distance \"d\" from $x_l$ and
$x_u$ respectively. The function is evaluated at these two points. Two
results can occur:

-   If, as in Figure [\[fig:4\]](#fig:4)\{reference-type="ref"
    reference="fig:4"\}a, $f(x_1)< f(x_2)$, then $f(x_1)$ is the minimum,
    and the domain of $x$ to the left of $x_2$, from $x_l$ to $x_2$, can
    be eliminated because it does not contain the minimum. For this
    case, $x_2$ becomes the new $x_l$ for the next round.

-   If $f(x_2)< f(x_1)$, then $f(x_2)$ is the minimum and the domain of
    $x$ to the right of $x_1$, from $x_1$ to $x_u$ would be eliminated.
    For this case, $x_1$ becomes the new $x_u$ for the next round.

::: marginfigure
![image](figures/part1b/golden_section/goldensection_2.png)\{width="2.4in"\}
:::

As the iterations are repeated, the interval containing the extremum is
reduced rapidly. The key to making this approach efficient is the wise
choice of the intermediate points. As in bisection, the goal is to
minimize function evaluations by replacing old values with new values.\
Now, let's define $\phi$ as indicated in Figure
[\[fig:5\]](#fig:5)\{reference-type="ref" reference="fig:5"\} where
$x_1 - x_l = x_u - x_2 = \phi(x_u - x_l)$. The intermediate points are
chosen such that the reduction in the range is symmetric. Now consider
the function with intermediate points such that $f(x_2) < f(x_1)$ and
hence the extremum will lie in the interval $[x_l,x_1]$. Coinciding
$x_2$ with the new $x_\{2new\}$ for which the function is already
evaluated, we reduce the number of function evaluations to just one and
at $x_\{1new\}$. Assuming the range to be of unit length in Figure
[\[fig:5\]](#fig:5)\{reference-type="ref" reference="fig:5"\}, it can be
seen

$$\begin\{aligned\}
(x_1 - x_l) &= \phi\\
(x_u - x_2) &= \phi
\end\{aligned\}$$ Hence $(x_2 - x_l) = 1 - \phi$. For the second
iteration, the bracket $[x_l,x_1]$ is further divided in the ratio of
$\phi$ to obtain intermediate points as $x_\{1new\}$ and $x_2$ where
$$\begin\{aligned\}
(x_2 - x_l) = \phi
\end\{aligned\}$$ Therefore $$\begin\{aligned\}
\phi(x_1 - x_l) &= (x_2 - x_l)\\
\phi(x_1 - x_l) &= 1 - \phi\\
\phi(\phi) &= 1 - \phi\\
(\phi)^2 + \phi - 1 &= 0\\
\phi &= \dfrac\{-1+\sqrt5\}\{2\}
\end\{aligned\}$$

::: marginfigure
![image](figures/part1b/golden_section/goldensection_3.png)\{width="2.4in"\}
:::

The other root is neglected as we need $\phi$ to be greater than $1/2$
as the reduction in range needs to be symmetric. Therefore,
**$\phi = 0.61803$**. It can be shown that $$\begin\{aligned\}
\dfrac\{\phi\}\{1 - \phi\} = \dfrac\{1 - \phi\}\{\phi\}
\end\{aligned\}$$ Dividing a range in the ratio of $\phi$ to $1 - \phi$
has the effect that the ratio of the shorter segment to the longer
equals to the ratio of the longer to the sum of the two. This rule is
called **golden section**. The uncertainty range is reduced by the ratio
$\phi = 0.61803$ at every stage. Hence, N steps of reduction using the
golden section method reduces the range by the factor
$(\phi)^N \approx (0.61803)^N$.

For the golden-section search, the two intermediate points are chosen
according to the golden ratio: $$\begin\{aligned\}
  x_1 = x_l + \phi(x_u - x_l)\\
 x_2 = x_u - \phi(x_u - x_l)\\
 
\end\{aligned\}$$ The function is evaluated at these two interior points.
For the next iteration we need only determine the $x_\{1new\}$. This is
done based on the new values of $x_l$ and $x_u$. A similar approach
would be used for the alternate case where the $x_\{2new\}$ would be
computed using $x_u$.

An upper bound for golden-section search can be derived as follows: Once
an iteration is complete, the optimum will either fall in one of two
intervals. If the optimum function value is at $x_2$, it will be in the
lower interval ($x_l, x_2, x_1$). If the optimum function value is at
$x_1$, it will be in the upper interval ($x_2, x_1, x_u$). Because the
interior points are symmetrical, either case can be used to define the
error.

Considering the upper interval ($x_2, x_1, x_u$), if the true value were
at the far left, the maximum distance from the estimate can be shown to
be $0.2361(x_u - x_l)$. Similarly, if the true value were at the far
right, the maximum distance from the estimate would be
$0.382 (x_u - x_l)$. Therefore, this case would represent the maximum
error. This result can then be normalized to the optimal value for that
iteration $x_\{opt\}$ to yield

$$\begin\{aligned\}
\varepsilon_a = \phi\arrowvert\dfrac\{x_u - x_l\}\{x_\{opt\}\}\arrowvert \times 100\%
\end\{aligned\}$$ This estimate provides a basis for terminating the
iterations.

::: example
Let's consider an unimodal function and implement the golden-section
search method.

``` 
def f(x: $\mathbb\{R\}$) -> $\mathbb\{R\}$:
  return exp(x)-2*x                                        
xl = -10                                                   
xu = 10 
eps = 1e-6                                                 
phi = (-1+sqrt(5))/2
x1 = xl + phi*(xu-xl)                                      
x2 = xu - phi*(xu-xl)                                      
fx1 = f(x1)                                                
fx2 = f(x2)
while True:
  if fx1 < fx2:
    xl = x2                                                
    x2 = x1                                                
    fx2 = fx1                                              
    x1 = xl + phi*(xu-xl)                                  
    fx1 = f(x1)                                            
  else:
    xu = x1
    x1 = x2 
    fx1 = fx2
    x2 = xu - phi*(xu-xl)                                  
    fx2 = f(x2)                                            
  if abs(xu-xl) < eps:                                     
    break 
print('Extremum bracket')                                  
print([xl, xu])                                            
```
:::

## Exercises

1.  TODO
