# Quantum Advantage in Machine Learning \{#chap:quantum_advantage\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:molecular_hamiltonian\]](#chap:molecular_hamiltonian)\{reference-type="ref+label"
reference="chap:molecular_hamiltonian"\},
[\[chap:dft\]](#chap:dft)\{reference-type="ref+label"
reference="chap:dft"\},
[\[chap:hartree_fock\]](#chap:hartree_fock)\{reference-type="ref+label"
reference="chap:hartree_fock"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

\"The use of quantum computing for machine learning (ML) is among the
most exciting prospective applications of quantum technologies. In this
talk, I will present recent results for characterizing when quantum
advantage in machine learning can be found. We will begin with
theoretical results for understanding whether one can achieve good
prediction performance by learning from significantly fewer quantum data
compared to classical data. Then, we will restrict ourselves to ML
problems with classical data and study the potential for computational
advantages. We show that some problems that are classically hard to
compute can be easily predicted by classical machines learning from
data. Using rigorous prediction error bounds as a foundation, we develop
a methodology for assessing potential quantum advantage in learning
tasks. These constructions explain numerical results showing that with
the help of data, classical machine learning models can be competitive
with quantum models even on quantum many-body problems. We then propose
improvements to existing quantum ML models that provide a simple and
rigorous quantum speed-up for a learning problem in the fault-tolerant
regime. For near-term implementations, we demonstrate a significant
prediction advantage over various classical ML models on engineered data
sets in one of the largest numerical tests for gate-based quantum
machine learning to date, up to 30 qubits.\"

References [@huang2021information], [@huang2021power]
