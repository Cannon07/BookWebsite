# Gauge Equivariant Normalizing Flows for Lattice Field Theory \{#chap:gauge_equivariant_flow\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:normalizing_flows\]](#chap:normalizing_flows)\{reference-type="ref+label"
reference="chap:normalizing_flows"\},
[\[chap:qft_basics\]](#chap:qft_basics)\{reference-type="ref+label"
reference="chap:qft_basics"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Monte Carlo methods allow for sampling quantum field configurations
defined on a discrete space time lattice. Such discrete configurations
are broadly termed lattice field theories. Unfortunately performing this
Monte Carlo sampling can in general be very complex. Normalizing flows
can facilitate sampling of these complex distributions. Critically,
these normalizing flows must respect gauge symmetries, which are
fundamental for the quantum field theories used in the standard model of
particle physics. In this chapter, we study a framework to encode gauge
symmetries into normalizing flows and discuss how to train the resulting
networks [@kanwar2020equivariant], [@albergo2019flow],
[@boyda2021sampling]. The resulting gauge equivariant normalizing flows
can successfully sample distributions on simple lattice field theories
with gauge symmetries.

## A Review of Lattice Field Theory

Quantum field theory provides a structure that unifies relativistic
quantum mechanics with the electromagnetic, weak, and strong forces. As
we have learned earlier, a challenge of the quantum field framework is
that we must construct fields of operators spread over all of spacetime,
a computationally challenging tasks.

Lattice quantum field theories discretize spacetime as a grid. This
discretization allows for computational modeling of quantum field theory
with computation systems. Suppose we have an action $S$ defined as
below. We can derive a probability density $p$ from this action

$$\begin\{aligned\}
S(\phi) &= \sum_x \sum_y \phi(x) \square(x, y) \phi(y) + m^2 \phi(x)^2 + \phi(y)^4 \\
p(\phi) &= e^\{-S(\phi)\}/Z
\end\{aligned\}$$

Lattice configurations can be sampled according to this energy function.
Deriving these samples has traditionally required sophisticate Markov
Chain Monte Carlo (MCMC) samplers. However, there are critical
limitations to MCMC style methods. In particular, mixing rates can be
very slow, requiring extensive computation. Reweighting techniques can
speed up the convergence rate, but it remains a formidable challenge.

Roughly, the sampling process is analogous to that for sampling images.
There is a few differences though; the exact probability distribution
$p(\phi)$ can be computed. Training datasets are also much more limited,
derived from expensive MCMC calculations, but typical lattice sizes can
be very large.

Normalizing flows as we have seen previously in the book are a powerful
tool for sampling probability distributions. We shall show how to use
normalizing flows to sample these complex probability distributions. The
normalizing flow acts as a variational ansatz for the function $q$ that
we sample from. By normalizing this function properly, we can make the
sample close to the true probability distribution.

$$\begin\{aligned\}
 q(\phi) &= r(z) \left | \det_\{ij\} \frac\{\partial [f(z)]_j]\}\{\partial z_j\} \right |^\{-1\}
\end\{aligned\}$$ This last term is the log-det-Jacobian (LDJ). Here $f$
is a composition of many constituent functions $g_i$ $$\begin\{aligned\}
f = g_n \cdot g_\{n-1\} \cdots \cdot g_1
\end\{aligned\}$$ If we construct each $g$ to act on only a subset of the
lattice components, the log-det-Jacobian becomes upper triangular. For
scalar field theory, we can think of this construction as building a
checkerboard mask on the lattice. If the checkerboard is black and
white, we can think of white squares as frozen and the black squares are
updated.

We also introduce context functions, which are convolutional networks
that take the local parameters of the network. These functions take in
the frozen sites. The normalizing flow $q$ can be trained by computing
the KL divergence between $q$ and the true distribution $p$

$$\begin\{aligned\}
\mathcal\{D\}_\{q||p\} &= \mathbb\{E\}_\{\phi \sim q\}[\log(q(\phi)) - \log(p(\phi))]
\end\{aligned\}$$

Samples $p$ are drawn from the true distribution and arecomputed by
simulation. The normalizing flow $q$ is trained to minimize
$\mathcal\{D\}_\{q||p\}$

## Lattice Gauge Theory

Quantum chromodynamics is the quantum field theory of the strong nuclear
force. Gauge fields are the force carriers for the strong nuclear force.
A gauge symmetry is roughly a coordinated local change of variables.

Matter fields live at the vertices of the lattice and gauge fields live
on the edges.

$$\begin\{aligned\}
U_\{\mu\}(x) \in \textrm\{SU\}(N)
\end\{aligned\}$$

Here $\textrm\{SU\}(N)$ is called the Gauge group.

$$\begin\{aligned\}
    S(U) &= -\frac\{\beta\}\{N\} \sum_x \mathrm\{Re\}\mathrm\{tr\}[P_\{01\}(x)] \\
    P_\{\mu\nu\}(x) &= U_\{\mu\}(x)U_\{\nu\}(x + \hat\{mu\})U_\{mu\}^\{\dagger\}(x + \hat\{\nu\})U_\{\nu\}^\{\dagger\}(x)
\end\{aligned\}$$ This last equation is called a Wilson loop. Lattice
gauge theories have a number of symmetries, including discrete
translational symmetry, hypercubic symmetry, and guage symmetry.
Exploiting these symmetries can yield data efficiency in training.

We can impose these symmetries by imposing invariance to the symmetry in
the prior and imposing equivariance on the flow $f$. Translational
equivariance for example can be achieved by using convolutional networks
in context functions and symmetric masking patterns.

Gauge equivariance is trickier. The intuition is that we want to act on
gauge invariant quantities only. Gauge fixing with an explicit
factorization doesn't preserve translational symmetry. Gauge fixing with
an implicit factorization is hard to preserve across the coupling layer.
The solution that is reached is to transform group-valued untraced
Wilson loops, and absorb updates using link representations.
$$\begin\{aligned\}
P_\{\mu\nu\}' &= h(P_\{\mu\nu\}|I) \\
U_\{\mu\}' &= P'_\{\mu\nu\}(P')_\{\mu\nu\}^\{\dagger\}U_\{\mu\} %\\
%   P'_\{\mu\nu\} 
\end\{aligned\}$$

The core idea is to construct an equivariant kernel while keeping the
form of the log-det-Jacobian tractable. We do this by enforcing
equivariance under matrix conjugation. $$\begin\{aligned\}
h(XPX^\{-1\}) = Xh(p)X^\{-1\}
\end\{aligned\}$$

Let's take a look at $U(1)$ gauge theory. In this case, the \"matrices\"
are just scalars. The flow based method seems to agree well with the
known analytical calculations. This can also be done for
$\textrm\{SU\}(N)$. You want to look at the Haar measure to implement the
$\textrm\{SU\}(N)$-invariant kernel for the flow. So far, this has been
done up to $\textrm\{SU\}(3)$, but techniques still need to be worked out
for higher dimensions.
