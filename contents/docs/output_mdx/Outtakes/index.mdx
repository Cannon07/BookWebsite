# Materials Design

Materials are an integral part of every tangible product in our every
day life. Novel materials have enabled modern civilization as we have
progressed with innovations from the light bulb to bridges to airplanes.
Advanced functional materials pervade through our everyday lives from
the LED bulb, which uses nitride-based semiconductors, to the electric
car, which uses lithium moving between two host structures namely
graphite and layered oxides. Despite these amazing success stories, the
journey towards a stable functioning product is a long and arduous one,
typically taking at least two decades from the initial lab demonstration
to a functioning device. Thus, the challenge of designing a new material
for an application is under-appreciated. An excellent illustration of
this challenge is the story of Thomas Edison and the light bulb. Sir
Humphry Davy, an English chemist, invented the world's first electric
lamp, by connecting alternating metal piles of zinc and copper to
charcoal electrodes long before the light bulb. This produced a bright
arc of light, called the arc lamp. \[Insert an illustration of the
Davy's arc lamp\] When Edison performed a seemingly exhaustive search,
at least in his mind, he tried 3000 materials followed by 6000 plants,
but could not identify tungsten. In fact, Coolidge was the one that
identified tungsten in 1910.

With approximately nineteen million known organic compounds and half a
million inorganic materials in addition to the rapidly growing number of
chemical systems synthesized, efficient screening procedures towards
specific engineering applications becomes increasingly important.
Material design calls for a high-dimensional combinatorial search if
more than one material is used for an application, which is the case
more often than not. Coinciding with strides in the materials science
field, machine learning is experiencing its own surge with many
innovations arriving at an unprecedented speed. The combination of these
two fields has itself seen much growth recently with machine learning
presenting an opportunity to quickly sift through large quantities of
materials [@Butler2018MachineScience; @Ramprasad2017MachineProspects].
The book aims to address how deep learning can accelerate materials
discovery and optimization, which is a typical example of an area in the
low-data regime. An overview of high-dimensional material-search
strategies are discussed with surrogate models and machine (deep)
learning as tools for a range of application areas including batteries,
solar photovoltaics and fuel cells.

## Materials for Batteries

Global emissions scenario studies highlight the importance of the
transport sector for climate change mitigation [@edenhofer2015climate]
with transport being responsible for about 23% of total energy-related
carbon dioxide emissions worldwide[@fischedick2014industry]. The sector
is growing more rapidly than most others, with emissions projected to
double by 2050. A major contributor to emissions from the transportation
sector is aviation. Electric aviation with sustainable power presents an
excellent path towards deep decarbonization of this sector. The progress
in this direction currently hinges on development in power generation
systems, which translate to several battery design
challenges[@bills2020performance; @fredericks2018performance] including
a multi-fold increase in the specific energy (energy storage capacity
per weight). These challenges can be achieved through materials
innovation in all the three integral components that batteries are
composed of: an anode, a cathode and an electrolyte, shown in Fig XX.
Therefore, batteries are an illustrative example of a materials design
challenge where machine learning can potentially accelerate material
discovery and optimization to enable the transition to electrification
of mobility.

All step changes have involved innovation on the material side

Batteries are essentially composed of three integral components: anode,
cathode with the electrolyte interfacing between them. It functions by
shuttling Li-ions between the anode and the cathode through the
electrolyte. The performance of the battery is determined by the choice
of these three components.

In this book, we primarily focus on designing the main components rather
than the interfaces. Modern electronics and electric cars are powered by
Li-ion batteries. The innovation story around Li-ion batteries was
honored with the Nobel Prize in Chemistry in 2019 to Stanley
Whittingham, John B. Goodenough and Akira Yoshino.

## Anode Material Design

Modern Li-ion batteries use a graphite host material, originally
demonstrated by Nobel Prize winner, Akira Yoshino. Under a fully charged
state, the lithium ions reside in graphite. When the cell is discharged,
the lithium ions move from the graphite host to the cathode host
material. The energy produced is the difference in lithium's affinity
towards the anode and cathode host materials. Thus, a good anode
material would bind to lithium very weakly, while the opposite is
desired of a cathode. Despite a number of possible host materials, only
a very limited set have seen success in practice. This is due to the
very long design cycle associated with experimental testing.

## Cathode Material Design

On the cathode side, many portable electronics still use lithium cobalt
oxide, invented by John Goodenough in 1980, for which he won the Nobel
Prize. Minor variants of this original idea swapping part of the cobalt
with nickel, manganese and aluminium, power most of the electric cars
today. While there has been impressive progress over the last decade,
there are still numerous sectors of transportation that remain
well-beyond the abilities of current batteries. This include long-haul
trucking and aviation, which require several-fold increase in specific
energy (energy per weight). This requires moving beyond the current
design space and there is an extremely large design space of oxides,
fluorides, oxyfluorides, sulfides that offer promise.

## Electrolyte Design

The electrolyte, often called the secret sauce, is typically a liquid
that comprises of solvent(s) and salt. Modern lithium-ion batteries
contain at least 2-3 main solvents with a dozen additives. The additives
play an extremely important role in determining the longevitity of the
battery, operating window and safety. Given that most of organic
chemistry (millions of compounds) is a valid design space, the design of
an electrolyte is probably the grandest challenge facing a battery
scientist. This is best exemplified by a statement from the famous
battery scientist, Jeff Dahn, "Go get the Sigma-Alrich catalog, put on
the table, open it up, and every single thing there is potentially a
good additive. I am not joking. Then you learn that generally two
additives is better than one, three is better than two, then you have a
huge space to work in."

## Materials for Solar Cells

One important part of bringing down the emissions is to clean up the
grid. Solar energy is one efficient way to bring down the emissions,
using the fusion energy from the sun into electrons that could be used
to do work. Solar cells (also called a photovoltaic cell, PV) work by
converting incoming solar radiation into electrical energy. The
performance of a solar cell is largely determined by the chosen
material. The efficiency timeline of development of solar cells is
carefully
[https://www.nrel.gov/pv/cell-efficiency.html](curated by National Renewable Energy Laboratory (NREL)).

The performance of the device is determined by the ability of the
material to absorb light and create an electron-hole pair. This
electron-hole pair should reach the external circuit before they
recombine. The ability of a material to absorb light is determined by a
quantity known as band gap. Semiconductors are materials that typically
possess moderate band gap (1-3 eV) and are ideal candidates to match
with the incoming solar spectrum. They are characterized by their
efficiency numbers, which quantifies what fraction of the incident
energy from the solar radiation is converted to electrical energy.

Silicon is one of the most popular materials and remains among the most
widely used photovoltaic material today. A single-junction Silicon solar
cell has improved in efficiency from around 13% in 1980s to over 25%
today. Alongside, GaAs has been a material widely studied with
efficiency increase from around 280% in the early 2000s to around 31%
today. Another class of materials widely explored are thin-film solar
cells made up of CIGS (Copper Indium Galium Selenide) and CdTe (Cadium
Telleuride)m which have grown in efficiency from 20% to 23% over the
last couple of decades. Photovoltaic materials can broadly be classified
based on composition into organic and inorganic photovoltaics where
perovskites photovoltaics are typically organic-inorganic hybrid
materials.

*Organics photovoltaics*: The development of organic semiconductors for
photovoltaic devices stems from efforts to find cheaper altearnatives to
semiconductor grade silicon and has led to unexpected performance for an
alternative choice of materials to convert sunlight to
electricity.[@inganas2018organic] Novel materials and developed concepts
have improved the resulting photovoltage in organic photovoltaic
devices, where
[reports](https://www.nrel.gov/pv/insights/assets/pdfs/cell-pv-eff-emergingpv.pdf)
over the last couple of years (2019-2020) of polymer/acceptor based
devices indicate above 17.4% power conversion efficiency in sunlight
(PCE).[@zhao2017molecular; @hou2018organic] The exploration of organic
materials for this application began with the realization that organic
dyes have very strong optical absorption, which can be leveraged to
absorb sections of the solar spectrum. Many dyes have been
studied[@merritt1976organic] including chlorophyll, which absorbs
sunlight in green
plants.[@tang1975transient; @tang1975chlorophyll; @tang1975photovoltaic; @inganas1981charge; @inganas1983photoelectrochemistry]
Dyes in organic optoelectronic devices in the form of bilayer devices in
1986 was the first published demonstration of a photovoltaic devices
with a 1% power conversion efficiency.[@tang1986two] Polymers were
another class of organic materials that gained interest when the field
underwent a fundamental transition from bilayer structures to bulk
heterojunctions with the advantage that bulk heterojunctions host a
large amount of interfaces between donor and acceptor rather than a
single plane as in the case of bilayer junctions. Along this line of
work, key developments in the last couple of decade of the 20th century
include the identification of fullerene
(C$_\{60\}$)[@hummelen1995preparation] and derivatives as promising
acceptors and a range of polymers as donors: poly(paraphenylene
vinylene) (PPV)[@smilowitz1993photoexcitation] and the polythiophene
(PT) family[@yohannes1998photoelectrochemical] that led to organic light
emitting diodes (OLEDs).[@berggren1994light] In the early 2000s, focus
shifted towards developing alternating copolymers[@blouin2008toward]
with donors and acceptors in a main chain polymer as an approach to
enhance the photovoltage, the overlap with the solar spectrum, and
reduce energy losses. Specific alternating copolymers worth highlighting
include alternating copolymers of quinoxaline with thiophene and
isoindigo with thiophene, which led to PCEs in the range 5-9
%.[@wang2013conformational; @wang2011f] Over the last couple of decades,
higher PCEs within organics have been achieved through material
development towards non-fullerene acceptors, novel polymer-polymer
blends and ternary materials, where multiple donors and acceptors are
blended.[@inganas2018organic]

*Perovskite (inorganic-organic) photovoltaics*: The most studied
perovskite solar cells (PSCs) are organic--halide perovskites. Halide
perovskites have revolutionized the emerging photovoltaic technologies
since they have emerged as light harvesters and hole-transport
materials.[@hao2014lead] Organic--inorganic hybrid perovskite compounds
based on metal halides adopt the ABX$_3$ perovskite structure, where the
B atom is a metal cation and X is typically F$_2$, Br$_2$, Cl$_2$ or
I$_2$. A promising family of perovskite compounds can be represented as
CH$_3$NH$_3$PbX$_3$ (X $=$ I, Cl, Br) perovskite absorbers (PCEs $>$
15%), however, efforts toward lead-free perovskites (PCEs $\approx$ 6%)
have led to the identification of a class that can be represented as
CH$_3$NH$_3$SnX$_3$ where (X $=$ I, Cl, Br or combinations).
Furthermore, although there are broad material development prospects for
perovskite solar cells, the associated lead toxicity and instability
resulting from the use of organic--inorganic hybrid halide lead
perovskites limit their application. Over the past few years, the
development of environmentally-friendly, stable and efficient perovskite
materials have long-term practical significance from the standpoint of
commercialization. The highest PCE achieved for an inorganic lead-free
perovskite solar cell was around 4.8% for CsSnX$_3$ but exhibits poor
air stability. Therefore, an active research topic in the field of
photovoltaics is the development of inorganic (or stable
organic-inorganic) lead-free perovskite light absorbing materials.

The material search space is extraordinarily large and rich with the
number of possible organic-inorganic material combinations for
photovoltaics.

## Materials for Superconductivity

Another holy grail towards an electrified future is to bring down the
transmission losses, which can be about 5-10% of the electricity
generation. There has been on-going search for superconducting materials
that would be functional at or near room temperature for several
decades. This has proved to be elusive.

Superconductivity is the property of a material to exhibit zero
resistance to the flow of current. The phenomenon of superconductivity
was originally discovered in 1911 by Heike Onnes when he was measuring
the electrical conductivity of pure metals, mercury and then
subsequently tin and lead. He observed that resistance of a solid
mercury wire at 4.2 K suddenly vanished. Within two years, in 1913, he
won the Nobel Prize in Physics for this discovery. This led to a
century-long race to increase the critical temperature, i.e. the highest
temperature below which a material is superconducting. Over the next
several decades, metals and intermetallics, such as Nb, Nb$_3$Sn,
Nb$_3$Ge were tested and exhibited critical temperatures in the range of
10-30K.

Then, in the late 1980s, there was renewed interest following a seminal
discovery by Bednorz and Müller that perovskites based on Lanthanum
Barium Copper Oxide (LBCO) was superconducting. They won the Nobel Prize
in Physics in 1987 for this discovery and following this, numerous
perovskite based materials were tested. The family of materials, called
cuprate superonductors, were able to reach a critical temperature of
133-138 K. The highest-temperature superconductor in this family is
material comprising of mercury, barium, calcium, copper and oxygen.

In the last decade, high pressure was used to stabilize materials in
exotic structures, which have exhibited superconductivity. Among these,
hydrogen sulfide (H$_2$S) has been shown to be superconducting at 150
gigapascal (GPa) pressure with a critical temperature of 80 K. Following
this, other hydrides such as scandium hydride and lanthanum hydride have
been studied. The current record holder is lanthanum hydride with a
critical temperature of 250 K at a pressure of 170 GPa.

Alongside these developments, another interesting direction is to use
angular twist between materials to trigger superconductivity. In 2018,
it was demonstrated that twisted bi-layer graphene with a magic angle of
1.1$^\circ$ exhibits superconductivity albeit at very low temperatures.

The problem of superconductivity is fundamentally a material search
problem. Given the numerous possible degrees of freedom (twist,
pressure), no serious constraints placed on possible materials, the
search space of possible superconductors is truly enormous. If indeed
deep learning is able discover a new superconductor with a critical
temperature of around 300K, it will undoubtedly deserve a Nobel
Prize.[@kitano2016artificial]


# The Transfer Matrix \{#chap:transfer_matrix\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:schrodinger\]](#chap:schrodinger)\{reference-type="ref+label"
reference="chap:schrodinger"\}\
**Difficulty Level:** \*\*\*\

------------------------------------------------------------------------

[TODO: This is mainly notes from Shankar. May or or may not want to turn
into full chapter]\{style="color: red"\} Consider a 1-dimensional Ising
model $$\begin\{aligned\}
    Z = \sum_\{s_i\} \prod_i e^\{K(s_i s_\{i+1\}-1)\}
\end\{aligned\}$$ We define the transfer matrix as follows
$$\begin\{aligned\}
    T_\{s's\} &= e^\{K(s's-1)\} \\
    T &= \begin\{pmatrix\}
    1 & e^\{-2K\} \\ 
    e^\{-2K\} & 1 \\
    \end\{pmatrix\}
\end\{aligned\}$$ We can rewrite partition function $Z$ in terms of
transfer matrix $T$ as $$\begin\{aligned\}
    Z &= \sum_\{s_1=1,\dotsc,N-1\} T_\{s_Ns_\{N-1\}\}\dotsc T_\{s_2s_2\}T_\{s_1s_0\} \\
    &= \langle s_N | T^N s_0 \rangle
\end\{aligned\}$$ In the case that we have periodic boundary conditions
where $s_0 = s_N$, we then obtain $$\begin\{aligned\}
    Z &= \mathrm\{Tr\} T^N
\end\{aligned\}$$

[TODO: Shankar has a lot of description of the Perron-Frobenius
decomposition. Worth covering? Also, should this identity be
derived?]\{style="color: red"\}

$$\begin\{aligned\}
    T &= e^\{K*\sigma_1\}
\end\{aligned\}$$ If you add a magnetic field $h$ to the Ising model, then
you get $$\begin\{aligned\}
    T&= e^\{K*sigma_1\}e^\{h\sigma_3\} \\
    &= T_K T_h
\end\{aligned\}$$

### The Hamiltonian

The Hamiltonian $H$ is defined as below $$\begin\{aligned\}
    T &= e^\{-H\}\\
    H &= -K^* \sigma_1
\end\{aligned\}$$

### Discussion

[TODO: We may want to discuss how the transfer matrix can act as a
bridge between the partition function and quantum mechanical systems. I
don't fully get this myself yet.]\{style="color: red"\}


# Condensed Matter Physics

Condensed matter is the study of solid state systems and their
properties. This will include properties such as the behavior of
electrons within the material, the behavior of \"phonons\" (which we
will introduce shortly). This also includes the interaction of materials
with light and photons, and exotic states of matter such as
Bose-Einstein condensates.

All particles can be broadly classified into fermions and bosons. These
two types of particles obey different statistical laws. More precisely,
fermions obey Fermi-Dirac statistics, and Bosons obey Bose-Einstein
statistics. We will discuss the mathematical meaning of these
distributions in this chapter.

## Electrons

Electrons are fundamental elementary particles that are fermions, *i.e.*
occupation number cannot be larger than 1. With this information,
together with a correct quantum-mechanical model for the energy levels
of electrons, we can derive the thermodynamic properties of electrons
present inside metals. These electrons can modeled as an ideal gas of
fermions with high densities, assuring the occupation of many single
particle energy levels due to the Pauli exclusion principle, and making
the interaction between particles negligible.

In this chapter we will develop the thermodynamic description of
electrons in metals.

### Fermi-Dirac statistics for conducting electrons in a metal

Electrons in a metal can be modeled as a gas of non-interacting fermions
considering the following assumptions:

-   Electrons in a metal are at high densities (many atoms per volume
    with each contributing to the conducting electrons).

-   Since no two electrons (fermions) can exist in the same state, a
    high density system fills many of the single particle energy levels

-   The lowest unoccupied state will have a kinetic energy much larger
    than $k_BT$, thus thermal excitations result in energetics with
    large kinetic energy and comparatively negligible potential energy
    of interaction.

-   The large kinetic energy associated with these electrons results in
    large conductivity of electrons in metals

Using the results for the thermodynamic behavior of non-interacting
fermions, we want to find the behavior of an ideal gas of electrons in a
metal. Based on the previous assumptions, electrons in a metal act as
non-interacting particles with quantized energies given by:
$$E_\{\vec\{n\}\} = \frac\{h^2\}\{8mL^2\}\left(n_x^2+n_y^2+n_z^2\right) \hspace\{1.0cm\} \text\{(translational modes)\} \notag$$

which are the energy levels for a particle in a box for a particles with
mass $m=9.10938\times 10^\{-31\} kg$.

The average number of electrons in the $\vec\{n\}$ state is: $$\label\{Eq7\}
\langle n_\{\vec\{n\}\} \rangle = \frac\{1\}\{e^\{\beta (E_\{\vec\{n\}\} - \mu)\} + 1\} = F(E_\{\vec\{n\}\})$$

where we have defined the Fermi function
$F(\varepsilon) = \left[e^\{\beta(\varepsilon - \mu)\} + 1 \right] ^\{-1\}$.

The total number of electrons is given by: $$\label\{Eq8\}
\langle N \rangle = 2 \sum_\{n_x = 1\}^\infty \sum_\{n_y = 1\}^\infty \sum_\{n_z = 1\}^\infty \frac\{1\}\{e^\{\beta (E_\{\vec\{n\}\} - \mu)\} + 1\} =  2 \sum_\{n_x = 1\}^\infty \sum_\{n_y = 1\}^\infty \sum_\{n_z = 1\}^\infty F(E_\{\vec\{n\}\})$$

where we include a factor of 2 since the electrons can exist in spin-up
and a spin-down states.

For sufficiently large $V$, the spectrum of translational wavemodes is
effectively a continuum. Therefore, we can convert this summation to an
integral over $\vec\{n\}$, resulting in: $$\begin\{aligned\}
\langle N \rangle &= 2 \int_0^\infty dn_x \int_0^\infty dn_y \int_0^\infty dn_z F(E_\{\vec\{n\}\}) \notag\\
&= 2 \int_0^\infty dk_x \frac\{L\}\{\pi\} \int_0^\infty dk_y \frac\{L\}\{\pi\} \int_0^\infty dk_z \frac\{L\}\{\pi\} F[\varepsilon (\vec\{k\})] \notag\\
&= 2 \frac\{L^3\}\{\pi^3\} \frac\{1\}\{2^3\} \int_\{-\infty\}^\infty dk_x \int_\{-\infty\}^\infty dk_y \int_\{-\infty\}^\infty dk_z F[\varepsilon (\vec\{k\})] \notag\\ 
&= \frac\{2V\}\{(2\pi)^3\} \int d\vec\{k\} F[\varepsilon (\vec\{k\})] \notag
\end\{aligned\}$$

where we have used a coordinate change from $\vec\{n\}$ to
$\vec\{k\} = \frac\{\pi\}\{L\}\vec\{n\}$, and the energy is now written as
$\varepsilon(\vec\{k\}) = \frac\{\hslash^2 k^2\}\{2m\}$.

Define the chemical potential at $T =0$ to be $\mu(T=0)=\varepsilon_F$
(Fermi Energy). To proceed, consider the form of the Fermi function at
$T=0$, $F(\varepsilon) = 1$ for $\varepsilon \le \varepsilon_F$ and
$F(\varepsilon) = 0$ for $\varepsilon > \varepsilon_F$. Define the Fermi
momentum $p_F$ according to
$\varepsilon_F = \frac\{p^2_F\}\{2m\} = \frac\{\hslash^2 k_F^2\}\{2m\}$. Thus,
at $T = 0$, $\langle N \rangle$ is found to be: $$\label\{AveN_electrons\}
\langle N \rangle = \frac\{2V\}\{(2\pi)^3\} \frac\{4\}\{3\}\pi k_F^3 = \frac\{8\}\{3\}\pi\frac\{V(2m)^\{3/2\}\}\{h^3\}\varepsilon_F^\{3/2\}$$
where the fermi energy $\varepsilon_F$ is given explicitly by:
$$\label\{FermiEnergy\}
\varepsilon_F = \left(\frac\{3\rho\}\{8\pi\}\right)^\{2/3\} \frac\{h^2\}\{2m\}$$

A typical metal (*Cu*) has a mass density of $9g/cm^3$. Assuming each
atom donates a single electron to the conducting electron gas, this
density has a Fermi temperature:
$$\theta_F = \frac\{\varepsilon_F\}\{k_B\} \approx 80,000K$$

this verifies that the Fermi energy $\varepsilon_F$ is sufficiently
large to make the ideal gas approximation valid at room temperature.

At room temperature, only states with energy very near
$\varepsilon \approx \varepsilon_F$ will be affected by thermal energy
$k_BT$. The spread in the distribution is approximately $2k_BT$
(Fig. [\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}).

TODO: Add diagram of the Fermi function

The pressure is found using the relationship for Fermi particles
$$\label\{Eq9\}
pV = 2k_BT\sum_\alpha \log \left(1 + e^\{-\beta \varepsilon_\alpha + \beta \mu\} \right)$$

Following a similar derivation as before, we write $$\begin\{aligned\}
\label\{Eq10\}
pV &= \frac\{2V\}\{(2\pi)^3\} \int d\vec\{k\} \log \{ 1 + e^\{-\beta [ \varepsilon (\vec\{k\}) - \mu]\} \} \notag\\
&= \frac\{4\pi V (2m)^\{2/3\}\}\{h^3\} \int_0^\infty d\varepsilon \varepsilon^\{1/2\} \log \left[ 1 + e^\{-\beta ( \varepsilon - \mu)\} \right]
\end\{aligned\}$$

where we have used $\varepsilon = \frac\{\hslash^2 k^2\}\{2m\}$.

In the limit $T \rightarrow 0 (\text\{or \} \beta \rightarrow \infty)$, we
have
$1 + e^\{-\beta ( \varepsilon - \mu)\} \rightarrow  e^\{\beta ( \varepsilon_F - \varepsilon)\}$
for $\varepsilon < \varepsilon_F$ and
$1 + e^\{-\beta ( \varepsilon - \mu)\} \rightarrow 1$ for
$\varepsilon > \varepsilon_F$.

Therefore, the pressure is written as: $$\label\{Eq11\}
pV = \frac\{4\pi V (2m)^\{3/2\}\}\{h^3\} \int_0^\{\varepsilon_F\} d\varepsilon \varepsilon^\{1/2\} (\varepsilon_F - \varepsilon) = \frac\{16\pi V (2m)^\{2/3\}\}\{15h^3\} \varepsilon_F^\{5/2\}$$

This pressure at $T = 0$ is approximately $10^6atm$. This large pressure
plays an important role in halting the collapse of a star (white dwarf)
because this enormous pressure offsets the gravitational forces that
otherwise drive the collapse.

The average energy $\langle E \rangle$ is found using: $$\label\{Eq12\}
\langle E \rangle = 2\sum_\alpha \varepsilon_\alpha \frac\{e^\{-\beta \varepsilon_\alpha + \beta \mu\}\}\{1 + e^\{-\beta \varepsilon_\alpha + \beta \mu\}\}$$

Following a similar derivation as before, we write $$\begin\{aligned\}
\langle E \rangle &= \frac\{2V\}\{(2\pi)^3\} \int d\vec\{k\} \varepsilon(\vec\{k\}) \frac\{1\}\{e^\{\beta [ \varepsilon(\vec\{k\}) - \mu]\}+1\} \notag\\
&= \frac\{4\pi V (2m)^\{2/3\}\}\{h^3\} \int_0^\infty d\varepsilon \varepsilon^\{3/2\} \frac\{1\}\{e^\{\beta [ \varepsilon - \mu]\}+1\} \notag\\
&= \frac\{4\pi V (2m)^\{2/3\}\}\{h^3\} \int_0^\infty d\varepsilon \varepsilon^\{3/2\} F(\varepsilon) \notag
\end\{aligned\}$$

where we have used $\varepsilon = \frac\{\hslash^2 k^2\}\{2m\}$.

TODO: ADD PLOT OF DERIVATIVE OF FERMI FUNCTION

We apply integration by parts to this equation and use
([\[AveN_electrons\]](#AveN_electrons)\{reference-type="ref"
reference="AveN_electrons"\}) to get: $$\label\{Eq13\}
\langle E \rangle = - \frac\{3\langle N \rangle\}\{5\varepsilon_F^\{3/2\}\} \int_0^\{\infty\} d\varepsilon \varepsilon^\{5/2\} \frac\{dF\}\{d\varepsilon\}$$

where
$\frac\{dF\}\{d\varepsilon\} = -\frac\{\beta e^\{\beta (\varepsilon - \mu)\}\}\{[e^\{\beta (\varepsilon - \mu)\} + 1]^2\}$.

In the limit $T\rightarrow0$, the function $-\frac\{dF\}\{d\varepsilon\}$
becomes peaked near $\varepsilon \approx \varepsilon_F$
(Fig. [\[fig2\]](#fig2)\{reference-type="ref" reference="fig2"\}), and we
can effectively expand the integrand near $\varepsilon = \varepsilon_F$
to get:
$$\varepsilon^\{5/2\} \approx \varepsilon_F^\{5/2\} + \frac\{5\}\{2\}\varepsilon_F^\{3/2\}(\varepsilon - \varepsilon_F) + \frac\{15\}\{8\}\varepsilon_F^\{1/2\}(\varepsilon - \varepsilon_F)^2 + ...$$

Since $\frac\{dF(\varepsilon)\}\{d\varepsilon\}$ is even about
$\varepsilon_F$ in this limit, the odd-order terms will integrate to
zero, leaving only the even terms. Thus, the final form of the average
energy is going to be: $$\label\{Eq14\}
\langle E \rangle = \langle N \rangle \varepsilon_F \left[A +B\left(\frac\{T\}\{\theta_F\}\right)^2 +...\right]$$

A precise calculation of this low-temperature expansion (outside scope
of this chapter) gives: $$\label\{Eq15\}
\langle E \rangle = \frac\{3\}\{5\}\langle N \rangle \varepsilon_F \left[1 +\frac\{5\pi^2\}\{12\}\left(\frac\{T\}\{\theta_F\}\right)^2 +...\right]$$

Therefore, the heat capacity for a metal in the limit of small
temperature is given by: $$\label\{Eq16\}
C_V = \frac\{3\}\{5\}\langle N \rangle \varepsilon_F \frac\{5\pi^2\}\{12\}2\frac\{T\}\{\theta_F^2\} = \frac\{\pi^2\}\{2\}\langle N \rangle k_B\frac\{T\}\{\theta_F\}$$

giving a linear temperature dependence in the small-$T$ limit.

The limiting behavior in the small-$T$ limit suggests that a plot of
$C_V/T$ approaches a constant as $T\rightarrow 0$. This proves to be the
behavior of the heat capacity for metals in the limit of small $T$. As
temperature increases, the fluctuations in the metal nuclei also
contribute to the heat capacity
(Fig. [\[fig3\]](#fig3)\{reference-type="ref" reference="fig3"\}).

## Phonons

Analog to the relation of photons to electromagnetic or light energy,
phonons are a definite discrete unit of quantum vibrational mechanical
energy. Phonons and electrons transport are the main mechanism of energy
diffusion in crystals. Thus for non-conducting crystals, heat transport
is mostly carried by phonons transport, and so a proper description of
the thermodynamic of phonons is necessary to understand the origins of
the thermal properties of solids.

In this chapter, we use statistical mechanics to derive the
thermodynamic of phonon gases, which are the normal modes of low
temperature solids. At low temperatures, atoms in a crystal remain close
to their equilibrium position, so a simple model can be developed to
describe the energy states of this particles. At the end of this chapter
we will derive the heat capacity of crystals due to the contribution of
phonons considering two different approaches: *Einstein Model* which
consider all phonons vibrating at the same frequency (valid for optical
phonons); and the *Debye Model* which consider a linear dispersion of
frequencies (valid for acoustic phonons at low temperatures).

### Crystalline Solid

The atoms in a crystal are arranged in a regular array of points in
space with a variety of possible crystalline lattices
(Fig. [\[fig4\]](#fig4)\{reference-type="ref" reference="fig4"\}). At zero
temperature, the atomic coordinates are uniquely locked into spatial
positions that minimize the energy. At finite temperature, the atoms
fluctuate about the energy-minimum positions, leading to lattice
vibrations that govern the thermodynamic behavior

### Thermodynamic contribution of lattice vibrations

Consider N atoms with positions
$\{\vec\{r\}\} = \vec\{r\}_1, \vec\{r\}_2,...,\vec\{r\}_N$ in a crystalline
lattice. Define the potential energy $V\left(\{\vec\{r\}\}\right)$ that
describes the energy for a given system configuration $\{\vec\{r\}\}$. A
minimum-energy configuration
$\{\vec\{r\}^\{(0)\}\} = \vec\{r\}^\{(0)\}_1, \vec\{r\}^\{(0)\}_2,...,\vec\{r\}^\{(0)\}_N$
satisfies the condition
$\nabla_\{\vec\{r\}_i\} V\left(\{\vec\{r\}^\{(0)\}\}\right) = \nabla_\{\vec\{r\}_i\} V|_0 = \vec\{0\}$
for $i = 1,2,...,N$. The atomic positions $\{\vec\{r\}^\{(0)\}\}$ define the
regular crystalline lattice.

Expand the potential energy about $\{\vec\{r\}\} = \{\vec\{r\}^\{(0)\}\}$,
such that: $$\begin\{aligned\}
V\left(\{\vec\{r\}\}\right) \approx  &V\left(\{\vec\{r\}^\{(0)\}\}\right) + \sum_\{i=1\}^N \left(\nabla_\{\vec\{r\}_i\} V|_0\right)\cdot \left(\vec\{r\}_i - \vec\{r\}^\{(0)\}_i \right) + \notag\\ &\frac\{1\}\{2\} \sum_\{i=1\}^N \sum_\{j=1\}^N \left(\nabla_\{\vec\{r\}_i\} \nabla_\{\vec\{r\}_j\} V|_0\right): \left(\vec\{r\}_i - \vec\{r\}^\{(0)\}_i \right) \left(\vec\{r\}_j - \vec\{r\}^\{(0)\}_j \right) + ... \notag
\end\{aligned\}$$

The linear term is zero (by definition), thus the energy is:
$$\label\{Eq17\}
V \approx V_0 + \frac\{1\}\{2\} \sum_\{i=1\}^N \sum_\{j=1\}^N \sum_\{\alpha,\gamma = x,y,z\} s_\{i\alpha\}s_\{j\gamma\}K_\{i\alpha j \gamma\}$$

where
$s_\{i\alpha\} = \hat\{e\}_\alpha \cdot \left(\vec\{r\}_i - \vec\{r\}^\{(0)\}_i \right)$
and $K_\{i\alpha j \gamma\} = \nabla_\{\vec\{r\}_i\} \nabla_\{\vec\{r\}_j\} V|_0$.

The Matrix $K_\{i\alpha j \gamma\}$ can be diagonalized into normal modes
(eigenvectors) with effective elastic constants (eigenvalues). Since
there are $3N-6\approx 3N$ degrees of freedom, there are $3N$ normal
modes.

The potential energy is written as: $$\label\{Eq18\}
V = V_0 + \frac\{1\}\{2\}\sum_\{l=1\}^\{3N\} \tilde\{K\}_l \xi_l^2$$

where $\xi_l$ is the magnitude of the $l$th normal mode.

The total energy of the system is then written as: $$\label\{Eq19\}
E = \sum_\{l=1\}^\{3N\}\frac\{\tilde\{p\}_l^2\}\{2\tilde\{m\}_l\} + \frac\{1\}\{2\}\sum_\{l=1\}^\{3N\}\tilde\{K\}_l \xi_l^2 + V_0$$

where $\tilde\{p\}_l$ and $\tilde\{m\}_l$ are the effective momentum and
mass of the $l$th normal mode, respectively.

The total energy $E$ is decomposed into normal modes with an
individual-mode energy in the form of a harmonic oscillator. These
normal modes are called *phonons*.

Phonons act as quasi-particles which means they are distinguishable and
independent, *i.e.* they don't interact between each other.

The Hamiltonian (sum of kinetic and potential energy) of the $l$th
phonon is given by:
$H_l = \frac\{\tilde\{p\}_l^2\}\{2\tilde\{m\}_l\} + \frac\{1\}\{2\}K_l\xi^2_l$. This
harmonic oscillator energy results in the quantized energy:
$$\label\{Eq20\}
E_l = \left(j_l + \frac\{1\}\{2\}\right)\hslash\omega_l$$

where $j_l = 0,1,2...$, and the phonon frequency is
$\omega_l = \sqrt\{\frac\{\tilde\{K\}_l\}\{\tilde\{m\}_l\}\}$.

The canonical partition function $Q$ is given by: $$\begin\{aligned\}
\label\{Eq21\}
Q(T,V,N) &= \sum_\{j_1 = 0\}^\infty \sum_\{j_2 = 0\}^\infty ... \sum_\{j_\{3N\} = 0\}^\infty e^\{\left[ -\beta V_0 - \beta \sum_\{l=1\}^\{3N\} \left(j_l + \frac\{1\}\{2\}\right)\hslash\omega_l \right]\} \notag\\
&= e^\{ -\beta V_0 \} \prod_\{l=1\}^\{3N\} \sum_\{j_l = 0\}^\infty e^\{-\beta\left(j_l + \frac\{1\}\{2\}\right)\hslash\omega_l\} \notag\\
&=e^\{ -\beta V_0 \} \prod_\{l=1\}^\{3N\} \frac\{e^\{-\beta\frac\{1\}\{2\}\hslash\omega_l\}\}\{1 - e^\{-\beta\hslash\omega_l\}\}
\end\{aligned\}$$

where we use the mathematical property
$\sum_\{n=0\}^\infty z^n= \frac\{1\}\{1 - z\}$.

This gives the Helmholtz free energy:
$$F = -k_BT\log Q = V_0 + k_BT \sum_\{l=1\}^\{3N\}\left[\log\left(1-e^\{-\beta\hslash\omega_l\}\right) + \frac\{1\}\{2\}\beta\hslash\omega_l \right] \notag$$

The average energy is given by: $$\label\{Eq.E_Phonons\}
\langle E \rangle = -\frac\{\partial \log Q\}\{\partial \beta\} = V_0 + \sum_\{l=1\}^\{3N\} \left( \frac\{\hslash\omega_le^\{-\beta\hslash\omega_l\}\}\{1 - e^\{-\beta\hslash\omega_l\}\} + \frac\{1\}\{2\}\hslash\omega_l \right)$$

The next step is to analyze two competing models for the phonon
frequencies $\omega_l$.

### Einstein model

In the Einstein model, we assume there is a single characteristic
frequency of the crystal, defined as the Einstein frequency $\omega_E$.
This model is a good approximation for optical phonons.

The average energy for this model is given by:
$$\langle E \rangle = V_0 + \frac\{3N\hslash\omega_E\}\{2\} + \frac\{3N\hslash\omega_Ee^\{-\beta\hslash\omega_l\}\}\{1-e^\{-\beta\hslash\omega_E\}\} \notag$$

For this model, the heat capacity is given by:
$$C_V = \left( \frac\{\partial \langle E \rangle\}\{\partial T\} \right)_\{V,N\} =  \frac\{3Nk_B \left( \frac\{\hslash\omega_E\}\{k_BT\} \right) ^2 e^\{-\frac\{\hslash\omega_E\}\{k_BT\}\}\}\{\left(1-e^\{\frac\{-\hslash\omega_E\}\{k_BT\}\}\right)^2\} = \frac\{3Nk_B \left( \frac\{\theta_E\}\{T\} \right) ^2 e^\{-\frac\{\theta_E\}\{T\}\}\}\{\left(1-e^\{\frac\{\theta_E\}\{T\}\}\right)^2\} \notag$$

where we define $\theta_E = \frac\{\hslash\omega_E\}\{k_B\}$.

In the limit $T \rightarrow \infty$ the heat capacity
$C_V \rightarrow 3Nk_B$ (Dulong-Petit Law). As we learned before, the
equipartition theorem states that the energy receives $k_BT$ per
thermally active degree of freedom for a harmonic oscillator (quantized
energy is linear in the quantum index). In the limit $T \rightarrow 0$
the heat capacity $C_V \rightarrow 0$. The heat capacity approaches zero
exponentially in the small-$T$ limit

### Debye model

The Debye model treats the solid as an elastic material. This
approximation is usually accurate to describe acoustic phonons at low
temperatures.

Vibrational modes in an elastic solid correspond to sound waves, thus
the frequencies satisfy $\omega=ck$, where $c$ is the speed of sound in
the solid and $k = m\pi/L$, where $m=1,2,...$

Thus, for considering a long particle chain, we convert the sum over
normal modes into an integral over the frequencies $\omega$ using:
$$\begin\{aligned\}
\sum_\{l=1\}^\{3N\} \left( \frac\{1\}\{e^\{\beta\hslash\omega_l\} - 1\} + \frac\{1\}\{2\} \right)\hslash\omega_l &=\sum_\{m_1\}\sum_\{m_2\}\sum_\{m_3\} \left( \frac\{1\}\{e^\{\beta\hslash\omega_l\} - 1\} + \frac\{1\}\{2\} \right)\hslash\omega_l \notag\\
&= \iiint \left[\left( \langle n \rangle + \frac\{1\}\{2\} \right)\hslash\omega_l\right] dm_1dm_2dm_3 \notag\\
&=\iiint_0^\{k_c\} \left(\frac\{L\}\{\pi\}\right)^3 \left[\left( \langle n \rangle + \frac\{1\}\{2\} \right)\hslash\omega_l\right] dk_1dk_2dk_3 \notag\\
&= \int_0^\{k_c\} 4\pi\left(\frac\{L\}\{\pi\}\right)^3 \left( \langle n \rangle + \frac\{1\}\{2\} \right)\hslash\omega_l k^2 dk \notag\\
&= \int_0^\{\omega_D\}D(\omega) \left( \langle n \rangle + \frac\{1\}\{2\} \right) \hslash\omega_l d\omega\notag
\end\{aligned\}$$

where $\displaystyle D(\omega) = \frac\{4L^3\omega^2\}\{\pi^2c^3\}$, is the
*density of states* of acoustic phonons , and
$\displaystyle \langle n \rangle = \frac\{1\}\{e^\{\beta\hslash\omega_l\} - 1\}$,
is the *Bose-Einstein Distribution*. In this equation $k_c$ is a cutoff
wavemode (to be determined), and $\omega_D = Ck_c$ is called the Debye
frequency.

A complete conversion will include one longitudinal mode with $c_l$ and
two transverse modes with $c_t$ . This gives: $$\begin\{aligned\}
\label\{Eq22\}
\sum_l\left( ... \right) &= \int_0^\{\omega_D\} \frac\{4L^3\omega^2\}\{\pi^2\}\left(\frac\{1\}\{c_l^3\} + \frac\{2\}\{c_t^3\} \right) \left( \langle n \rangle + \frac\{1\}\{2\} \right) \hslash\omega_l d\omega \notag\\
&=\int_0^\{\omega_D\} D(\omega) \left( \langle n \rangle + \frac\{1\}\{2\} \right) \hslash\omega_l d\omega
\end\{aligned\}$$

where we defined a new form for the density of states given by
$\displaystyle D(\omega) = \frac\{4L^3\omega^2\}\{\pi^2\}\left(\frac\{1\}\{c_l^3\} + \frac\{2\}\{c_t^3\} \right)$.

To find $\omega_D$ we must enforce that $\sum_l 1 = 3N$, thus we have:
$$\begin\{aligned\}
\sum_l 1 &= \int_0^\{\omega_D\}D(\omega) \left( \langle n \rangle + \frac\{1\}\{2\} \right) d\omega \notag\\
&= \left(\frac\{L\}\{\pi\}\right)^3 \left(\frac\{1\}\{c_l^3\} + \frac\{2\}\{c_t^3\} \right) \frac\{\omega_D^3\}\{3\} = 3N \notag\\&\Rightarrow \omega_D = \frac\{\pi\}\{L\}\left[\frac\{9N\}\{\left(\frac\{1\}\{c_l^3\} + \frac\{2\}\{c_t^3\} \right)\}\right]^\{1/3\} \notag
\end\{aligned\}$$

For convenience, use $\omega_D$ as a parameter, thus we can define the
density of states as: $$\label\{Eq23\}
D(\omega) = \frac\{9N\omega^2\}\{\omega_D\}$$

Define the Debye temperature $\theta_D = \frac\{\hslash\omega_D\}\{k_B\}$,
which defines the temperature scale for vibrational fluctuations. To
test this model, we find the heat capacity:
$$C_V = -k_B\beta^2\left(\frac\{\partial\langle E\rangle\}\{\partial\beta\}\right) = 9Nk_B\frac\{T^3\}\{\theta_D^3\}\int_0^\{\theta_D/T\}dx\frac\{x^4e^\{-x\}\}\{\left(1-e^\{-x\}\right)^2\} \notag$$

In the limit $T \rightarrow \infty$ the heat capacity approaches:
$$C_V \rightarrow 9Nk_B\frac\{T^3\}\{\theta_D^3\}\int_0^\{\theta_D/T\}dxx^2 = 3Nk_B \notag$$

which is expected (Dulong-Petit Law), since at high temperatures all
phonon modes become activated.

In the limit $T \rightarrow 0$ the heat capacity scales as:
$$\begin\{aligned\}
C_V \rightarrow 9Nk_B\frac\{T^3\}\{\theta_D^3\}\int_0^\{\infty\}dx\frac\{x^4e^\{-x\}\}\{\left(1-e^\{-x\}\right)^2\} \sim Nk_B\frac\{T^3\}\{\theta_D^3\} \notag
\end\{aligned\}$$

To obtain a qualitative picture of the Debye $T^3$ law, we suppose that
all phonon modes of wavevector less than $K_T$ have the classical
thermal energy $k_BT$ and that modes between $K_T$ and the Debye cutoff
$K_D$ are not excited at all. Of the $3N$ possible modes, the fraction
excited is $(K_T/K_D)^3=(T/\theta)^3$, because this is the ratio of the
volume of the inner sphere to the outer sphere. The energy is
$U \approx k_BT \cdot 3N(T/\theta)^3$, and the heat capacity is
$C_v=\partial U/\partial T = 12Nk_B(T/\theta)^3$.

The heat capacity predicted by the Einstein and the Debye models are
very similar. However, the low temperature of the heat capacity of
non-conducting solids matches the Debye model, *i.e.* $C_V \sim NT^3$
(Fig. [\[fig7\]](#fig7)\{reference-type="ref" reference="fig7"\}). This is
because at low temperatures only acoustic phonons are activated, which
is better represented in the Debye model.

## Photons

In the last part of this chapter, we analyze the thermodynamics of
photon gas, which constitutes the description of the radiation
properties of Black Bodies. Here we consider an electromagnetic field in
thermal equilibrium with its container. Under this condition, quantum
theory predicts that the Hamiltonian of the system can be written as a
sum of terms, each having the form of the harmonic oscillator. Thus, the
energy of each photon is given by $\hslash\omega$. This provides our
starting point for the derivation of all the thermodynamic properties of
photons from the basis of statistical mechanics. In addition to the
thermodynamic properties of photon gases, some fundamental laws for
Blackbody Radiation will be obtained, such as Planck's Law, Wien's Law,
Rayleigh-Jeans law, and Stefan-Boltzmann Law.

### Black Body Radiation

We are all familiar with the idea that hot objects emit radiation, a
light bulb, for example. In the hot wire filament, an electron,
originally in an excited state drops to a lower energy state and the
energy difference is given off as a photon,
($\epsilon = h\nu = \epsilon_2 -\epsilon_1$). We are also familiar with
the absorption of radiation by surfaces. For example, clothes in the
summer absorb photons from the sun and heat up. Black clothes absorb
more radiation than lighter ones. This means, of course that lighter
colored clothes reflect a larger fraction of the light falling on them.

A black body is defined as one which absorbs all the radiation incident
upon it, *i.e.* a perfect absorber. It also emits the radiation
subsequently. If radiation is falling on a black body, its temperature
rises until it reaches equilibrium with the radiation. At equilibrium,
it re-emits as much radiation as it absorbs so there is no net gain in
energy and the temperature remains constant. In this case, the surface
is in equilibrium with the radiation and the temperature of the surface
must be the same as the temperature of the radiation.

To develop the idea of radiation temperature we construct an enclosure
having walls which are perfect absorbers (see
Fig. [\[Fig8\]](#Fig8)\{reference-type="ref" reference="Fig8"\}). Inside
the enclosure is radiation. Eventually this radiation reaches
equilibrium with the enclosure walls, equal amounts are emitted and
absorbed by the walls. Also, the amount of radiation travelling in each
direction becomes equal and is uniform. In this case the radiation may
be regarded as a gas of photons in equilibrium having a uniform
temperature. The enclosure is then called an isothermal enclosure.

An enclosure of this type containing a small hole is itself a black
body. Any radiation passing through the hole will be absorbed. The
radiation emitted from the hole is characteristic of a black body at the
temperature of the photon gas. The properties of the emitted radiation
is then independent of the materials of the wall provided they are
sufficiently absorbing that essentially all radiation entering the hole
is absorbed. This universal radiation is called Black Body Radiation.

An everyday example of a photon gas is the background radiation in the
universe. This photon gas is at a temperature of about 5 $K$. Thus the
earth's surface, at a temperature of about 300 $K$, is not in
equilibrium with this gas. The earth is a net emitter of radiation
(excluding the sun) and this is why it is dark at night and why it is
coldest on clear nights when there is no cloud cover to increase the
reflection of the earth's radiation back to the earth. A second example
is a Bessemer converter used in steel manufacture containing molten
steel. These vessels actually contain holes like the isothermal
enclosure of Fig. [\[fig8\]](#fig8)\{reference-type="ref"
reference="fig8"\}. The radiation emitted from the hole is used in steel
making to measure the temperature in the vessel, by means of an optical
pyrometer.

## **Statistical Thermodynamics** \{#statistical-thermodynamics .unnumbered\}

To derive Planck's radiation law directly from our statistical
mechanics, we note that number of photons in the gas is not fixed. The
photons are absorbed and re-emitted by the enclosure walls. Since the
photons are non-interacting it is by this absorption and re-emission
that equilibrium is maintained in the gas. Since, also the free energy
$F(T, V, N)$ is constant in equilibrium (at constant $T$ and $V$ ) while
N varies it follows that $\partial F/\partial N = 0$, that is
$$\mu = \frac\{\partial F\}\{\partial N\}\bigg|_\{V,N\} = 0 \notag$$

The photon gas is then a Bose gas with $\mu= 0$, so that the canonical
partition function is given directly as $$\label\{Part_photon\}
Q = \sum_\{s_1,s_2,...s_j,... = 0\}^\infty e^\{- \beta \sum_\{l=1\}^\infty s_l\epsilon_l\} = \prod_\{s=1\}^\infty \frac\{1\}\{1-e^\{-\epsilon_\{s\}\beta\}\}$$

Similarly to our previous treatment for phonons, we express the average
energy of the system per unit of volume $\langle E\rangle/V$ considering
a continuum distribution of energies: $$\begin\{aligned\}
\label\{Energy_Density\}
\frac\{\langle E \rangle\}\{V\} = -\frac\{1\}\{V\}\frac\{\partial \log Q\}\{\partial \beta\} &= \frac\{1\}\{V\}\sum_\{l=1\}^\infty \frac\{\epsilon_le^\{-\beta\epsilon_l\}\}\{1 - e^\{-\beta\epsilon_l\}\} \notag\\
&= \int_0^\infty \frac\{1\}\{V\}g(\epsilon)  \frac\{\epsilon\}\{e^\{\beta\epsilon\}-1\} d\epsilon \notag\\
&= \int_0^\infty \frac\{1\}\{V\}\epsilon(\nu)g(\nu)\bar\{n\}(\nu) d\nu
\end\{aligned\}$$

where $\epsilon=h\nu$, according to the energy states defined by the
quantum theory of electromagnetic field;
$g(\nu)=\frac\{4\pi V\}\{c_0^3\}\nu^2$ is the density of states, and
$\bar\{n\}(\nu)$ is the Bose occupation given by:
$$\bar\{n\}(\nu) = \frac\{1\}\{\exp(\epsilon_\{s\}\beta)-1\} \notag$$

The integrand from
([\[Energy_Density\]](#Energy_Density)\{reference-type="ref"
reference="Energy_Density"\}) can be expressed as: $$\label\{Planck\}
u(\nu) = \frac\{8\pi\}\{c^\{3\}\}\frac\{h\nu\}\{\exp(\beta h\nu)-1\}\nu^\{2\}$$

Which is Planck's Radiation Law.

We may also recover Wien and Rayleigh-Jeans laws as limits of Planck's
law,

1.  Long wavelength, $\frac\{hc\}\{kT\lambda\} << 1$. Here $$\label\{Eq29\}
    \bar\{\epsilon\}= \frac\{hc/\lambda\}\{\exp(hc/kT\lambda)-1\} \simeq kT$$

    And ([\[Eq29\]](#Eq29)\{reference-type="ref" reference="Eq29"\})
    becomes $$\label\{Eq30\}
    u(\lambda)= \frac\{1\}\{V\} \bar\{\epsilon\}(\lambda) g(\lambda) \simeq \frac\{8 \pi\}\{\lambda^\{4\}\} kT$$

    Which is Wien's law valid at long wavelengths.

2.  Short wavelength, $\frac\{hc\}\{kT\lambda\} >> 1$. Here $$\label\{Eq31\}
    \bar\{\epsilon\}= \frac\{hc\}\{\lambda\} \exp(-hc/kT\lambda)$$

    And ([\[Eq30\]](#Eq30)\{reference-type="ref" reference="Eq30"\})
    becomes $$\label\{Eq32\}
    u(\lambda)= \frac\{1\}\{V\} \bar\{\epsilon\}(\lambda) g(\lambda) \simeq \frac\{8 \pi hc\}\{\lambda^\{5\}\} \exp(-hc/kT\lambda)$$

    Which is the Wien's law valid at short wavelength.

Employing our statistical mechanics we readily obtained Planck's
radiation law.

We may also derive the Stefan-Boltzmann law for: $$\label\{Eq33\}
u = \int_0^\{\infty\} u(\nu) d\nu = \frac\{8\pi\}\{c^\{3\}\} \int_0^\{\infty\} \frac\{h\nu\}\{\exp(\beta h\nu)-1\}\nu^\{2\}d\nu$$

Introducing $x=\beta h\nu$, this reduces to $$\label\{Eq34\}
u = \frac\{8\pi k^\{4\}\}\{(hc)^\{3\}\} \int_0^\{\infty\} dx \frac\{x^\{3\}\}\{\exp(x)-1\} T^\{4\} = aT^\{4\}$$

Where $$\label\{Eq35\}
a = \frac\{8\pi k^\{4\}\}\{(hc)^\{3\}\} \frac\{\pi^\{4\}\}\{15\}$$

In this way we obtain, using statistical mechanics, a law derived
previously using thermodynamics including all the numerical factors.
This gives Stefan's constant $\sigma$ in $$\label\{Eq36\}
E = \frac\{1\}\{4\} caT^\{4\} = \sigma T^\{4\}$$

As $$\label\{Eq37\}
\sigma =  \frac\{2\pi ck^\{4\}\}\{(hc)^\{3\}\} \frac\{\pi^\{4\}\}\{15\} = 5.67 \times 10^\{-5\} \frac\{erg\}\{cm^\{2\}-Sec-K^\{4\}\}$$

A measurement of $\sigma$ could then be used, for example, to determine
Planck's constant. Planck, in fact, determined h as the constant needed
in his radiation law to fit the observed spectral distribution law. This
gave him the value h = 6.55 $\times$ 10$^\{-27\}$ erg.sec which compares
with the present value of h = 6.625 $\times$ 10$^\{-27\}$ erg.sec

## Thermodynamic Properties of Photons

We may calculate all the thermodynamic properties of black body
radiation using statistical mechanics through the partition function
$Q$, where $$\label\{Eq38\}
F =  -kT\log(Q)$$

And $Q$ was derived as shown in
([\[Part_photon\]](#Part_photon)\{reference-type="ref"
reference="Part_photon"\}). This is the basic method of statistical
thermodynamics. The aim is to reproduce all the thermodynamic properties
with all factors and constants evaluated. This gives $$\begin\{aligned\}
\label\{Eq39\}
F &=  -kT\log\left(\prod_\{s=1\}^\{r\} (1-\exp(-\epsilon_\{s\}\beta))^\{-1\}\right) \notag \\
F &=  -kT\sum_\{s=1\}^\{r\} \log(1-\exp(-\epsilon_\{s\}\beta))^\{-1\} \notag \\
F &= -2kT  \int \frac\{d\Gamma\}\{h^\{3\}\} \log (1-\exp(-\epsilon_\{s\}\beta))^\{-1\} 
\end\{aligned\}$$

Where the $\epsilon_\{s\}$ are the single photon states and the factor of
2 arises from the two polarizations available to each photon. This can
be integrated in a variety of ways. Perhaps the most direct is to
integrate over phase space $(d\Gamma=dV\,\,4\pi p^\{2\}\,\,dp)$ and write
$\epsilon=pc$. Introducing the dimensionless variable
$x = \beta \epsilon = \beta pc$, the Helmholtz free energy is:
$$\label\{Eq40\}
F = -\frac\{1\}\{3\} \left\lbrace -\frac\{8\pi k^\{4\}\}\{(hc)^\{3\}\} \int_\{0\}^\{\infty\} d(x)^\{3\} \,\, \log(1-\exp(-x)) \right\rbrace \,\, VT^\{4\}$$

The dimensionless integral here can be transformed into that appearing
in the constant of $a$ of ([\[Eq35\]](#Eq35)\{reference-type="ref"
reference="Eq35"\}), by an integration by parts, i.e., $$\begin\{aligned\}
\label\{Eq41\}
I &= -  \int_\{0\}^\{\infty\} d(x)^\{3\} \log(1-\exp(-x)) \notag \\
&= - (x^\{3\}) \log(1-\exp(-x))\bigg|_\{0\}^\{\infty\} + \int_\{0\}^\{\infty\} x^\{3\} d[\log(1-\exp(-x))]
\end\{aligned\}$$

The first term vanishes because:

\(a\)
$\lim_\{x \to \infty\} x^\{3\} \log(1-\exp(-x)) \simeq \lim_\{x \to \infty\} x^\{3\} \exp(-x) = 0$

\(b\)
$\lim_\{x \to 0\} x^\{3\} \log(1-\exp(-x)) \simeq \lim_\{x \to 0\} x^\{3\} \log(x) = 0$

And $$\begin\{aligned\}
\label\{Eq42\}
I &= \int_\{0\}^\{\infty\} dx \frac\{x^\{3\}\}\{\exp(x)-1\} = \frac\{\pi\}\{15\}
\end\{aligned\}$$

Comparing ([\[Eq40\]](#Eq40)\{reference-type="ref" reference="Eq40"\}) and
([\[Eq35\]](#Eq35)\{reference-type="ref" reference="Eq35"\}), we get:
$$\label\{Eq43\}
F = -\frac\{1\}\{3\} aVT^\{4\}$$

From F we may determine all other thermodynamic properties by
differentiation. For example, the entropy is $$\label\{Eq44\}
S = -\left(\frac\{\partial F\}\{\partial T\}\right) \bigg|_\{V\} = \frac\{4\}\{3\} aVT^\{3\}$$

The internal energy is: $$\label\{Eq45\}
U = F +TS = avT^\{4\}$$

The pressure is: $$\label\{Eq46\}
p = -\left(\frac\{\partial F\}\{\partial V\}\right) \bigg|_\{T\}  = \frac\{1\}\{3\} aT^\{4\}$$

Finally, the Gibbs free energy is: s $$\label\{Eq47\}
G = F + pV =  - \frac\{1\}\{3\} aVT^\{4\} + \frac\{1\}\{3\} aVT^\{4\} = 0$$

This is zero as required $G=\mu N$ and the chemical potential $\mu=0$.
We may use these expressions to further verify thermodynamic
consistency, for example that
$C_\{V\} = T\frac\{\partial S\}\{\partial T\}\bigg|_\{V\}= \frac\{\partial U\}\{\partial T\}\bigg|_\{V\}$.

In summary, we have obtained the spectral distribution from the Bose
occupation in much the same way as we obtained the Maxwell-Boltzmann
distribution for a classical gas. The only other required ingredient was
the density of states. We have also obtained all the thermodynamic
properties using the partition function $Q$.

### Electron-Phonon Coupling

Perhaps move to part 3

### Electron-Electron Coupling

Perhaps move to part 3


# Photosynthesis \{#chap:photosynthesis\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:photons\]](#chap:photons)\{reference-type="ref+label"
reference="chap:photons"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter we discuss the interactions of a radiation field with a
photochemical system which absorbs radiation over a broad band of
frequencies. Of primary concern are the evaluation of the chemical
potential difference which is developed within the system, and the
amount of work, *i.e.* free energy, which may appear as a result of
light absorption by such a system.

## Light Absorption Induced Excitations

Consider a two-level photochemical system which consists of a collection
of ground electronic states $G$ and excited electronic states $E$
(Fig. [\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}). In
systems where excitations can migrate easily, these collections of
states are often referred to as the *valence band* and the *conduction
band*, respectively. Each electronic band usually contains a number of
vibrational substates.

::: marginfigure
![image](figures/part2a/statistical_mechanics/photosynthesis/fig1.png)\{width="\\linewidth"\}
:::

When the rate of absorption of light quanta causing excitations from $G$
to $E$ is rapid with respect to the thermal equilibration of populations
between the two bands, then a transition from $E$ to $G$ gives up some
free energy which may be stored or used for chemical synthesis. The
amount of work which can be done as a result of the absorption of each
photon is limited by the product of this free-energy change $\mu$, and
the quantum yield for the de-excitation pathway which is coupled to work
production.

If the sub states within each electronic band remain in thermal
equilibrium regardless of the incident radiation field, then the
potential difference between any substate of $G$ and any substate of $E$
will be independent of the identity of either substate. In other words,
there will be a single well-defined free-energy change for any
transition between the bands.

There are two ways of viewing the excitations caused by the absorption
of light:

-   *Photoelectric view*. Excitation may be considered as producing an
    increase in the population of electrons in a set of states of fixed
    number, with a corresponding decrease in the population of electrons
    in another set of states of fixed number. Adequate if the system
    operates through electron migration.

-   *Photochemical view* Excitation may be considered as producing as an
    increase the number of an excited state molecular species and a
    concomitant decrease in the number of a ground state molecular
    species. Adequate if light absorption is molecular rearrangement.

In this lecture we will consider a *photochemical view*, *i.e.* we
analyze changes in the partial molar free energy of the light-absorbing
molecules in their ground state and in their excited state. The action
of light usually depletes the population of the ground-state molecules
only very slightly, altering the chemical activity of these species to a
negligible extent; in this case the potential difference arising between
the bands is due almost entirely to the greatly increased population of
molecules in the excited state.

## Chemical Potential of Photochemical Systems

In order to evaluate the band-to-band potential difference $\mu$ caused
by a radiation field in any given situation, we first consider the
conditions for equilibrium between the band-to-band transitions and a
radiation field. Reversible reaction implies that there is no change in
entropy accompanying the emission or absorption of radiation by the
photochemical system at any frequency.

The entropy change corresponding to the loss of a photon of frequency
$\nu$ from a radiation field may be evaluated by considering an
equilibrium at $\nu$ between the field and a blackbody. A blackbody is
in equilibrium with a radiation field at $\nu$ when: $$\label\{Eq1\}
I(\nu) = \frac\{8\pi n^2\nu^2\}\{c^2\}\frac\{1\}\{e^\{h\nu/k_BT_B\}-1\}$$

where $T_B$ is the temperature of the blackbody, $n$ is the refractive
index of the medium, and $I$ is the intensity of the radiation field in
units of photons per $4\pi$ solid angle, per unit bandwidth, per unit
area, per unit time.

The entropy gained by a blackbody upon absorption of a photon at $\nu$
is $h\nu/T$. By rearranging ([\[Eq1\]](#Eq1)\{reference-type="ref"
reference="Eq1"\}) to find the temperature of a blackbody in equilibrium
with a radiation field of intensity $I$, we find the entropy change upon
loss of a photon from a radiation field of the photochemical system, to
be: $$\label\{Eq2\}
-\frac\{\partial S\}\{\partial N\} = k\ln\left( 1+  \frac\{8\pi n^2\nu^2\}\{c^2I\}\right)$$

Assuming a canonical ensemble $T,V,N$, we can define a relation between
the potential difference $\mu$ and the change in entropy per photon
absorbed by the photochemical system as: $$\label\{Eq3\}
 \frac\{\partial S\}\{\partial N\} = \frac\{h\nu - \mu\}\{T\}$$ If we assume
that the photochemical absorber is isotropic in its interaction with a
radiation field, we can assume that the entropy change induced by photon
absorbed is equal to the negative of the entropy change due to emission
of a photon. Then by integrating over solid angle, and equating the
entropies, we find that the rate of photon absorption and emission per
unit bandwidth and unit cross section is: $$\label\{photonEm\}
I(\nu,\mu,T) = 8\pi\left(\frac\{n\nu\}\{c\}\right)^2\frac\{1\}\{e^\{(h\nu-\mu)/k_BT\}-1\}$$

where we consider the loss due entropy of photon emitted as
$-\frac\{\partial S\}\{\partial N\}$, due to reversibility.

The number one in ([\[photonEm\]](#photonEm)\{reference-type="ref"
reference="photonEm"\}) corresponds to stimulated emission, which may
usually be neglected simplifying the expression to get: $$\label\{Eq4\}
I(\nu,\mu,T) = 8\pi\left(\frac\{n\nu\}\{c\}\right)^2\exp\left(\frac\{\mu-h\nu\}\{k_BT\}\right)$$

If the absorption cross section for band-to-band excitation is
$\sigma(\nu,\mu,T)$, then the total rate of excitation and emission per
unit bandwidth is equal to $$\label\{Eq5\}
\sigma(\nu,\mu,T) \times I(\nu,\mu,T)$$ for a photochemical system which
has thermal equilibrium at temperature T within its electronic bands,
and a potential difference $\mu$ between the bands, and which is in
equilibrium with an isotropic radiation field at all frequencies.

For simplicity we assume that the absorption spectrum is independent of
$\mu$, although this may not be true. Changes with temperature can be
ignored since $T$ is assumed to be fixed. Then, by adding the absorption
cross section for band-to-band excitation $\sigma(\nu)$ to the frequency
dependent factors of ([\[Eq4\]](#Eq4)\{reference-type="ref"
reference="Eq4"\}), we find that the emission spectrum as a function of
$\nu$ is given by (Fig. [\[fig2\]](#fig2)\{reference-type="ref"
reference="fig2"\}): $$\label\{Eq6\}
\sigma(\nu)n^2(\nu)\nu^2\exp\left(-\frac\{h\nu\}\{k_BT\} \right)$$

::: marginfigure
![image](figures/part2a/statistical_mechanics/photosynthesis/fig2.png)\{width="\\linewidth"\}
:::

The Planck-law relationship between absorption and emission may be used
to calculate the potential developed in a photochemical system whenever
its absorption spectrum and the incident light flux are known.

The rate of band-to-band excitations resulting from an arbitrary
radiative field is equal to:

$$\label\{Eq7\}
R_\{in\} = \int \sigma(\nu)I_S(\nu)d\nu$$

From ([\[Eq6\]](#Eq6)\{reference-type="ref" reference="Eq6"\}) the rate of
radiative decay from a photochemical system having a potential
difference $\mu$ is: $$\begin\{aligned\}
\label\{Eq8\}
R_\{lum\} &= \exp\left(-\frac\{h\mu\}\{k_BT\} \right) \int 8\pi\left(\frac\{n\nu\}\{c\}\right)^2\sigma(\nu)\exp\left(-\frac\{h\nu\}\{k_BT\}\right)d\nu \notag\\
&=\exp\left(-\frac\{h\mu\}\{k_BT\} \right)L
\end\{aligned\}$$ where we abbreviate the integral with $L$.

By equating ([\[Eq7\]](#Eq7)\{reference-type="ref" reference="Eq7"\}) and
([\[Eq8\]](#Eq8)\{reference-type="ref" reference="Eq8"\}), we can find the
maximum possible potential of a photochemical system having an
absorption spectrum $\sigma(\nu)$, and illuminated by a radiation field
of intensity and distribution $I_S(\nu)$: $$\label\{Eq9\}
\mu_\{max\}=k_BT\ln\left( \frac\{R_\{in\}\}\{L\}\right)$$

Nonradiative band-to-band transitions are frequently a significant
source of $E$-to-$G$ relaxation. We assume that the rate of induced
$G$-to-$E$ transitions given by ([\[Eq7\]](#Eq7)\{reference-type="ref"
reference="Eq7"\}) is large with respect to all spontaneous excitations.
Then we specify that the total rate of decay from $E$ to $G$ is $\kappa$
times the rate of radiative decay alone: $$\label\{Eq10\}
R_\{out\}=\kappa(\mu)L\exp\left( \frac\{\mu\}\{k_BT\} \right)$$

By equating $R_\{in\}$ and $R_\{out\}$ we determine the potential developed
in the presence of nonradiative relaxation:: $$\label\{Eq11\}
\mu = k_BT\ln\left( \frac\{R_\{in\}\}\{\kappa(\mu)L\} \right) = \mu_\{max\} - k_BT\ln \kappa(\mu)$$

As $\kappa$ is the reciprocal of the luminescence quantum yield, it may
frequently be determined experimentally. For the remainder of this
section we assume that $\kappa$ is independent of $\mu$, although it
appears that this is generally true only for noninteracting excitations
obeying Boltzmann statistics.

## Power Stored by Light Absorption

Work is one of the more popular commodities which can result from the
photochemical absorption of light, so that frequently one desires to
maximize the amount of power stored by such a system.

The amount of power stored is: $$\label\{Eq12\}
P= (R_\{in\} - R_\{loss\})\mu$$

where $R_\{loss\}$ is the rate of $E$-to-$G$ transitions which are not
coupled to the work-storage process.

We define the quantum yield for the loss processes: $$\label\{Eq14\}
\phi_\{loss\} = \frac\{R_\{loss\}\}\{R_\{out\}\}$$

From ([\[Eq11\]](#Eq11)\{reference-type="ref" reference="Eq11"\}) we know
that: $$\begin\{aligned\}
\label\{Eq15\}
\mu &= \mu_\{max\} - k_BT\ln\left(\frac\{R_\{out\}\}\{R_\{lum\}\}\right) \notag\\
&= \mu_\{max\} - k_BT\ln\left(\frac\{R_\{loss\}\}\{R_\{lum\}\}\right) + k_BT\ln\left(\frac\{R_\{loss\}\}\{R_\{out\}\}\right) \notag\\
&= \mu_0 + k_BT\ln\phi_\{loss\}
\end\{aligned\}$$

where $\mu_0 = \mu_\{max\} - k_B\ln\left( \frac\{R_\{loss\}\}\{R_\{lum\}\}\right)$
is the potential difference in the absence of the workstorage process.

Since $R_\{in\} = R_\{out\}$, we may rewrite the amount of power stored as:
$$\label\{Eq16\}
P=R_\{in\}\mu(1-\phi_\{loss\})$$ This expression may be maximized by
appropriate choice of $\mu$ and $\phi_\{loss\}$.

It can be shown that in the case of $\mu_0/k_BT\gg 1$, the power storage
is approximately maximal when $\phi_\{loss\}=k_BT/\mu_0$, so that the
optimal potential is roughly: $$\label\{Eq17\}
\mu=\mu_0 -k_BT\ln\left(\frac\{\mu_0\}\{k_BT\}\right)$$

The Planck-law relation itself has a number of applications, some of
which are suggested by the work which has already been done. One of the
important applications is the use of an absorption spectrum to calculate
a luminescence spectrum for systems in which luminescence has not been
observed experimentally. Prediction of the luminescence spectrum may be
useful in calculating energy-transfer probabilities, and may assist in
locating the luminescence experimentally. Examination of differences
between predicted and observed luminescence spectra may provide a check
on the experimental methods used, and represents a tool for examining
deviations from the usual assumption of complete vibrational equilibrium
in a two-level system [@Ross1967a].


# Water \{#chap:water\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ideal_gas\]](#chap:ideal_gas)\{reference-type="ref+label"
reference="chap:ideal_gas"\},
[\[chap:liquids\]](#chap:liquids)\{reference-type="ref+label"
reference="chap:liquids"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Water is the key compound for our existence on this planet and it is
involved in nearly all chemical, biological and geological processes.
Access to clean water is also one of the most challenging questions for
mankind in the coming century, in particular with the prospect of global
warming. Although water is the most common molecular substance it is
also the most unusual with many peculiar properties such as increased
density upon melting, decreased viscosity under pressure, density
maximum at $4 ^\{\circ\}C$, high surface tension and many more.

In this chapter, we will study the physical mechanism behind the unique
properties of water, by discussing its molecular and thermodynamic
behavior.

## The Anomaly of Water in the Supercooled region

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig1.png)\{width="\\linewidth"\}
:::

The mysterious properties of water become even more extreme in the
supercooled region below the freezing point, but they appear also under
ambient conditions. One example
(Fig.[\[fig1\]](#fig1)\{reference-type="ref" reference="fig1"\}a) is the
isothermal compressibility, $\kappa_T$, related to volume, or
equivalently density, fluctuations in the liquid, where $\kappa_T$
decreases upon cooling as for a normal liquid, but only down to
$46 ^\{\circ\}C$ where it starts to increase again upon further cooling,
indicating that density fluctuations in the liquid increase as thermal
energy is removed. Another is the heat capacity at constant pressure,
$C_P$, which is related to fluctuations in the entropy and again this
property of water shows an anomalous increase compared to normal liquids
already at $35 ^\{\circ\} C$ (Fig.[\[fig1\]](#fig1)\{reference-type="ref"
reference="fig1"\}b). The thermal expansion coefficient, $\alpha _P$,
which is related to the crosscorrelation between fluctuations in density
and entropy, becomes negative for water below the density maximum at
$4 ^\{\circ\} C$ (Fig.[\[fig1\]](#fig1)\{reference-type="ref"
reference="fig1"\}c). Characteristic for all three is that they are
related to fluctuations in the liquid and that these increase upon
cooling contrary to expectation for normal, simple liquids. Importantly,
while these fluctuations and apparent divergences of thermodynamical
response functions are most evident when water is supercooled below
$0 ^\{\circ\} C$, they start to influence water properties already in the
ambient, biologically relevant regime and grow in importance in a
continuous, but rapidly increasing fashion as thermal energy is removed.
One of the most essential questions to address for a microscopic
understanding of water is then: What is the structure and dynamics of
the hydrogen bonding (hydrogen bonding) network in water that gives rise
to all these unique properties? This question has been discussed
intensively for over 100 years and has not yet been resolved. In order
to gain new unique information regarding the structure of the hydrogen
bond network in water, it is essential to develop new techniques, both
experimental and theoretical.

## Polarizability

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig2.png)\{width="\\linewidth"\}
:::

The water molecule has a bent structure due to the structure of the
$sp^3$ orbitals of the $O-H$ bonds and the unshared electrons of the
oxygen atom (Fig. [\[fig2\]](#fig2)\{reference-type="ref"
reference="fig2"\}). Since oxygen is more electronegative than hydrogen,
the electrons in the $O-H$ bond congregate near the oxygen atom, leading
to a dipole moment of $m = ql\approx 6\times 10^\{-30\}Cm$ ($q$ is charge
separation and $l$ is separation distance). The dipole moment of water
molecules results in electrostatic interactions that tend to order water
in the liquid phase.

The energy of interaction for two monovalent ions (same charge) in a
dielectric medium is written as: $$\begin\{aligned\}
\label\{Eq1\}
E_\{coulomb\}=k_BT\frac\{l_B\}\{r\}
\end\{aligned\}$$

where $T$ is the temperature of the medium, $k_B$ is the Boltzmann
constant, $r$ is the separation of charges, and $l_B$ is the Bjerrum
length, which is the length scale at which the electrostatic energy of
interactions equals the thermal energy of the system, and is given by:

$$\begin\{aligned\}
\label\{Eq2\}
l_B= \frac\{e^2\}\{k_BT\varepsilon\} \notag
\end\{aligned\}$$ where $\varepsilon$ is the dielectric constant of the
medium, and $e$ is the elementary charge of electron
($e=1.602 \times 10^\{-19\} C$).

Consider two dipoles with orientations $\vec\{u\}_1$ and $\vec\{u\}_2$
separated by a vector $\vec\{r\} =\vec\{r\}_1-\vec\{r\}_2$. The valance of the
charge separation is $z$, which is separated by a distance $l$.

The total energy of interaction between the dipole is: $$\begin\{aligned\}
\frac\{E_\{d-d\}\}\{l_Bz^2k_BT\} = &\frac\{1\}\{|\vec\{r\} + \frac\{l\}\{2\}(\vec\{u\}_1 - \vec\{u\}_2)|\} - \frac\{1\}\{|\vec\{r\} + \frac\{l\}\{2\}(\vec\{u\}_1 - \vec\{u\}_2)|\} + \notag\\
&\frac\{1\}\{|\vec\{r\} - \frac\{l\}\{2\}(\vec\{u\}_1 - \vec\{u\}_2)|\} - \frac\{1\}\{|\vec\{r\} - \frac\{l\}\{2\}(\vec\{u\}_1 - \vec\{u\}_2)|\} \notag
\end\{aligned\}$$

For $|\vec\{r\}|\gg l$, we expand $E_\{d-d\}$ is powers of $l$, giving to
order $l^2$.

$$\begin\{aligned\}
E_\{d-d\} &= \frac\{l_Bz^2k_BTl^2\}\{r^3\}(I - 3\vec\{e\}_r\vec\{e\}_r):\vec\{u\}_1\vec\{u\}_2 \notag\\ 
&= \frac\{1\}\{\varepsilon r^3\}(I - 3\vec\{e\}_r\vec\{e\}_r):\vec\{\mu\}_1\vec\{\mu\}_2 \notag
\end\{aligned\}$$

where $\vec\{\mu\}_1 = ezl\vec\{u\}_1$ is the dipole moment and
$\vec\{e\}_r =\vec\{r\}/r$.

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig3.png)\{width="\\linewidth"\}
:::

The dipole-dipole interaction energy is long-ranged
($E_\{d-d\} \sim r^\{-3\}$), thus there can be long-ranged dipole
correlations in a solution. The *polarizability* of a solution can lead
to dramatic consequences in the physical behavior. For example, an ion
in a polarizable medium will order the opposite charged ends of dipoles
around the ion. This effectively shields the interaction between the
ions. This effect is generally captured in the dielectric constant
$\varepsilon$ of the medium, thus the polarizable medium acts as an
effective medium for electrostatic interaction
(Fig. [\[fig3\]](#fig3)\{reference-type="ref" reference="fig3"\}).

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig4.png)\{width="\\linewidth"\}
:::

Since the polarizability leads to shielding of the charge-charge
interactions, the dielectric constant increases with polarizability of
the medium, thus decreasing the Bjerrum length
$l_B = \frac\{e^2\}\{k_BT\varepsilon\}$. Dipole ordering is energetically
favorable and is entropically unfavorable due to the loss of rotational
entropy of the molecules. Therefore, the dielectric constant is
temperature dependent. Raising the temperature prefers the molecules to
engage in rotational fluctuations, thus reducing the shielding effect,
thus the dielectric constant tends to decrease with temperature
(Fig.[\[fig4\]](#fig4)\{reference-type="ref" reference="fig4"\}).

## Hydrogen bonding

Another consequence of the electronegativity of oxygen is *hydrogen
bonding*, which occurs when the partial positive hydrogen on a donor
bonds with the lone pair electrons on an acceptor. The strength of the
hydrogen bonds in water is approximately $8k_BT$, thus significantly
larger than the mean thermal energy.\

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig7.png)\{width="\\linewidth"\}
:::

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig8.png)\{width="\\linewidth"\}
:::

Since each oxygen can accept a hydrogen bond with 2 different water
molecules, liquid water tends to form hydrogen bond networks that
dramatically influence water's physical properties. At low temperatures,
molecules in liquid water arrange in a densely packed near-tetrahedral
network, having about 4.4 neighboring per atom
(Fig. [\[fig7\]](#fig7)\{reference-type="ref" reference="fig7"\}), called
*High density liquid phase* (HDL). At higher temperatures, a less dense
structure is formed, with less than 4 neighbors per atom, named *Low
density liquid phase*. At room temperature these two phases coexist, as
we will see in the next section.

The structure of ice consists of two interpenetrating lattice with
hexagonal close packed symmetry, resulting in four hydrogen bonds per
$H_2O$ molecule (Fig. [\[fig8\]](#fig8)\{reference-type="ref"
reference="fig8"\}). The transition from solid water to liquid water
results in the molecules having more rotational degrees of freedom that
disrupt the hydrogen bond network. Since the hydrogen bond distance no
longer constrains the molecular spacing as much in the liquid phase, the
liquid phase is denser than the solid phase. Further increase of
temperature above the solid-liquid transition leads to further
densification with temperature (up to $4^\{\circ\}C$)

Hydrogen bonding in the liquid state causes water to be substantially
more ordered than the liquid-phase of other substances. Comparing the
pair correlation function of the oxygen in water against the pair
correlation function of argon
(Fig. [\[fig10\]](#fig10)\{reference-type="ref" reference="fig10"\}), we
see:

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig10.png)\{width="\\linewidth"\}
:::

-   The integral under the first peak shows that water molecules have
    about 4.4 first-shell neighbors and argon has about 10 first-shell
    neighbors.

-   However, the water molecules are more ordered than the argon atoms,
    as indicated by the width of the first peak and number of subsequent
    peaks.

Due to hydrogen bonding, water has a significantly higher melting and
boiling points than comparable molecules. Hydrogen bonding also
influences a number of dynamic properties of water (*e.g.* viscosity and
diffusivity) that make water a unique and important substance.

## Phases of Liquid Water

As we learned from water's behavior, there exist two separate liquid
phases, *High Density Liquid* (HDL) and *Low Density Liquid* (LDL), with
a coexistence line in the $p-T$ diagram deep in the supercooled regime
and at elevated pressures (Fig. [\[fig5\]](#fig5)\{reference-type="ref"
reference="fig5"\}). This line ends with decreasing pressure in a
critical point and its extension into the one-phase region corresponds
to the Widom line. At the Widom line the density fluctuations would
reach a maximum consistent with equal population of molecules in HDL and
LDL related structures. The presence of the Widom line leads to
fluctuations that extend over a very large temperature range and can be
observed well above ambient conditions. We note that the HDL is on the
ambient temperature side of the Widom line, whereas LDL is on the low
temperature side. This explains why the high density, hydrogen bond
distorted species dominates at ambient conditions. The origin of the
anomalous properties of water is the increase in density fluctuations as
water is cooled down and approach the Widom line leading to fluctuating
tetrahedral patches growing in size as directional hydrogen bonding
becomes relatively more important. We should also note that in the
schematic picture, the boundary between the HDL and LDL regions is not
sharp and the density should vary in a continuous fashion. Furthermore,
the regions are not of one particular size and shape but should
represent a distribution, where the average changes with temperature.

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig5.png)\{width="\\linewidth"\}
:::

The driving force to energetically allow for the formation of HDL must
be the more isotropic van der Waals interactions together with quantum
effects leading to both weaker hydrogen bond strength and, in addition,
to quantized vibrational modes which become thermally inaccessible in
tetrahedral coordination. Tetrahedral ice is quite unique among all the
condensed structures with an extremely low coordination number of 4.
Most other cubic and hexagonal solids have a nearest neighbor
coordination of 12. This low coordination is the result of the
directional hydrogen bonds in the tetrahedral coordination. What makes
HDL have a density significantly lower than for the high-pressure ices
is that the first shell is distorted in order to allow thermal
excitation of vibrational modes. It has been speculated that these
distortions are not symmetrical but instead asymmetrical in order to
minimize loss of enthalpy leading to a doubly hydrogen bonded local
structure. It turns out that the hydrogen bond energy is maximized when
there is an equal number of donor and acceptor bonds and a particularly
favorable situation in terms of energy per hydrogen bond is when each
molecule forms only one bond of each kind. This would lead to a
stabilization of doubly hydrogen bonded structures which interact with
the surrounding through more isotropic forces and gain entropy since the
vibrational modes would become thermally accessible. Such a picture is
supported based on thermodynamic grounds where it has been argued that
over a large range of the liquid-vapor coexistence line the averaged
water interaction potential should resemble that of liquid Argon, *i.e.*
hydrogen bonding is not dominating resulting in mostly a doubly hydrogen
bonded structure. Furthermore, one model suggests, also based on
thermodynamic arguments, that the hydrogen bonding structure upon
melting of ice should mostly correspond to breaking of hydrogen bonds
between rings in the tetrahedral ice structure while keeping the
internal ring structure intact. This would also lead to a local doubly
hydrogen bonded picture.

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig6.png)\{width="\\linewidth"\}
:::

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/fig9.png)\{width="\\linewidth"\}
:::

The varying phases of liquid water can be studied by means of XRD
scans - they allow us to differentiate between the phases, because of
their differing average bond distance. On a given scan, for a similar
$r$-value, LDL has a higher-intensity peak, and HDL has a
lower-intensity but wider peak
(Fig.[\[fig6\]](#fig6)\{reference-type="ref" reference="fig6"\}). The
structure of liquid water is still a subject of intense debate and the
picture is one that is still evolving.

## Hydrophobicity

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/LotusEffect.png)\{width="\\linewidth"\}
:::

::: marginfigure
![image](figures/part2a/statistical_mechanics/water/LotusLeafSurface.png)\{width="\\linewidth"\}
:::

A further consequence of hydrogen bonding is the *hydrophobic effect*,
where non-polar substances are excluded by water in order to maintain a
confluent hydrogen bonded network. Hydrophobicity does not arise due to
favorable \"hydrophobic interaction\", rather the non-polar molecules
come together because putting water molecules next to them would
eliminate the opportunity for those molecules to hydrogen bond with
other water molecules. Essentially, the surrounding water presses the
hydrophobic groups together to maximize their contacts with each other.
Hydrophobicity is responsible for the de-mixing of oil (nonpolar
hydrocarbons) and water. Hydrophobic amino acids drive the collapse of
proteins in order to sequester these side chains from the surrounding
solution. Another phenomena related to Hydrophobicity, is the so-called
*Lotus effect* (Fig. [\[lotusleaf\]](#lotusleaf)\{reference-type="ref"
reference="lotusleaf"\}), which refers to the self cleaning properties of
lotus leaves, as a result of the high water repellence that forbids the
adhesion of water drops into the surface, leading to surface cleaning as
the dirt particles are picked by the water drops. Lotus effect is a
direct concequence of the micro- and nanoscopic architecture on the
surface (Fig. [\[lotussurface\]](#lotussurface)\{reference-type="ref"
reference="lotussurface"\}), and can be found in other plants such as
taro leaf, cane, and also on the wings of certain insects.
Superhydrophobic surfaces can be used for a variety of applications like
self-cleaning glasses, efficient power plant condensers, or
*lab-on-a-chip* microfluidic devices.
