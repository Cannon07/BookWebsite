# Graph Convolutional Neural Networks \{#chap:graphconvs\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\},
[\[chap:convnets\]](#chap:convnets)\{reference-type="ref+label"
reference="chap:convnets"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

Graph convolutional neural networks are one of the most foundational
machine learning tools for modeling physical systems. The core reason
for this is that graphs map naturally onto many physical systems. For
example, a molecule can be converted into a graph by taking atoms as
nodes and bonds as edges. Crystals can be converted into periodic graphs
the same way.

The graph neural network starts with some initial representation for
each node (often each atom) in the system. This representation is
updated iteratively by combining information across edges (chemical
bonds). After multiple rounds of message passing, the training reaches
learned node feature vectors (atomic feature vectors) which are
subsequently combined to create a system-level embedding vector.

Graph neural networks also have a number of connections to fundamental
theory. RNNs, convolutional neural networks and transformers can all be
viewed as special instances of the more generic framework of graph
neural networks, providing a hint at their general capabilities.

![Graph Convolutional Networks repeatedly update feature vectors at each
node (and/or edge) of the graph, and take readouts at the final
layer.](figures/Differentiable Models/gcns/Graph_convolutional_network.png)\{#fig:gcn_image\}

## The Type of a Graph

A graph is made up of a collection of nodes and edges. Each node and
each edge has associated with it a local feature descriptor. Each node
has a set of neighbors. The neighborhood structure of the graph is
encoded implicitly by the collection of its edges.

``` \{.python language="python"\}
class Node(feat: $\mathbb\{R\}^N$)
class Edge(u: Node, v: Node, feat: $\mathbb\{R\}^M$)
class Graph(nodes: Node[K], edges: Edge[M])
```

It can often be useful to construct an explicit \"adjacency\" matrix $A$
which encodes at position $A_\{i,j\}$ whether there is an edge between $i$
and $j$. We leave it as an exercise for the reader to transform the
representation of a graph specified above into an adjacency matrix.

## Graph Convolutional Layers

We can represent a graph convolutional layer by the equation

$$\begin\{aligned\}
    f(H_i, A) &= \sigma(AH_i W_i)
\end\{aligned\}$$

Here $W_i$ is the weight matrix for this layer and $\sigma$ is a
nonlinear activation function. $A$ is the adjacency matrix. $H_i$ is the
current feature representation for the graph. For $i=0$, $H_i = X$, the
matrix of input features.

Note that if $X$ is a feature matrix, $A X$ sums feature representation
across the neighbors of the node. Self-links are usually added to a
adjacency matrix $A$ to allow the original feature vector for a node to
be retained. $$\begin\{aligned\}
    \hat\{A\} &= A + I
\end\{aligned\}$$

Another challenge with this basic computation is that nodes with many
neighbors will rapidly build larger feature representations. To correct
for this effect, we will need to normalize feature representations by
the node degree. To control for node degree, we need to divide each node
through by its degree. Let $D$ be the degree matrix which has the degree
of each node on its diagonal. Note that $D$ consists of the row sums of
$A$

$$\begin\{aligned\}
    D_\{ii\} &= \sum_j A_\{ij\}
\end\{aligned\}$$

Then we consider the updated rule

$$\begin\{aligned\}
    f(X, A) &= \sigma(D^\{-1\}AXW)
\end\{aligned\}$$

``` \{.python language="python"\}

class GraphConvolution(W: $\mathbb\{R\}$[d, c]):
  def $\lambda$(A: $\mathbb\{N\}$[N, N], X: $\mathbb\{R\}$[N, d]):
    D = for j: sum(for i: A[i, j])
    $\sigma$(D$^\{-1\}$*A*X*W)
    
```

## Graph Attention

Let $1,\dotsc,n$ be the nodes in the graph. Let $\mathcal\{N\}_i$ be the
set of neighbors of node $i$. A graph attention mechanism can be written
as

$$\begin\{aligned\}
    y_i = \frac\{1\}\{\mathcal\{C\}(\{f_j \in \mathcal\{N\}_i\})\} \sum_\{j \in \mathcal\{N\}_i\} w(f_i, f_j) h(f_j)
\end\{aligned\}$$

Here $h, w$ are neural networks, and $\mathcal\{C\}$ normalizes the sum of
local features in neighborhood $\mathcal\{N\}_i$. Graph attention
architectures may prove more useful in certain learning problems than
plain graph convolutions.
