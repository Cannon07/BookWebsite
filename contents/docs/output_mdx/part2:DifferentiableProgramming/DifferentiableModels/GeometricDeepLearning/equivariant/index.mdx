# Equivariant Neural Networks \{#chap:equivariant_networks\}

------------------------------------------------------------------------

\
**Prerequisites:**
[\[chap:ml_basics\]](#chap:ml_basics)\{reference-type="ref+label"
reference="chap:ml_basics"\},
[\[chap:grad_descent\]](#chap:grad_descent)\{reference-type="ref+label"
reference="chap:grad_descent"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

In this chapter we define what it means for a neural network to be
equivariant to a group $G$. This definition will prove useful as we
define concrete equivariant networks in subsequent chapters.

## $G$-function Transformations

Suppose that $\mathcal\{X\}$ is a space upon which group $G$ acts. An
element $g \in G$ has an action on functions $$\begin\{aligned\}
f: \mathcal\{X\} \to \mathbb\{R\}^n
\end\{aligned\}$$ by the definition $$\begin\{aligned\}
g \cdot f (x) &= f(g^\{-1\}x)
\end\{aligned\}$$ The transformation $$\begin\{aligned\}
f \mapsto g \cdot f
\end\{aligned\}$$ is sometimes referred to as a $G$-function transform.

## Equivariant Neural Layers

We can define a neural network as a function $$\begin\{aligned\}
\psi_\theta: \mathcal\{V\} \to \mathcal\{Y\}
\end\{aligned\}$$ where $\theta$ is a set of weight parameters. Note here
that $$\begin\{aligned\}
\mathcal\{V\}, \mathcal\{Y\} \subseteq \mathbb\{R\}^n
\end\{aligned\}$$ are subsets of $\mathbb\{R\}^n$.

Equivariant networks attempt to learn a function that is invariant to
the action of some group $G$. That is, we seek to learn a function
$\psi_\theta$ such that

$$\begin\{aligned\}
\psi_\theta(g \cdot x) &= g \cdot \psi_\theta(x)
\end\{aligned\}$$ Given a set of group representations $$\begin\{aligned\}
T_g: \mathcal\{V\} \to \mathcal\{V\} 
\end\{aligned\}$$ for $g \in G$, then a function $\psi$ is equivariant to
$G$ if there exists a transformation $$\begin\{aligned\}
 S_g: \mathcal\{Y\} \to \mathcal\{Y\}   
\end\{aligned\}$$ such that $$\begin\{aligned\}
S_g(\psi_\theta(v)) &= \psi_\theta(T_g(v))
\end\{aligned\}$$ In general given $(T_g, S_g)$ we want to learn
$\psi_\theta$ from data. We say that such a $\psi_\theta$ is a
$G$-equivariant neural network.

You will prove in the exercises that a composition of equivariant maps
is equivariant, so if we can construct a suitable set of equivariant
layers $e_i$, we can build rich families of equivariant architectures
through composition $$\begin\{aligned\}
e &= e_1 \circ e_2 \circ \dotsc \circ e_k
\end\{aligned\}$$

## Linear Layers through Equivariant Convolutions

We will address the construction of linear and nonlinear equivariant
layers separately since they require different transformations.

The linear portion of traditional convolutional layer we have defined
previously is naturally equivariant to translations. (This true up to
artifacts of padding and edge effects; to make it mathematically true,
consider an infinitely large grid $\mathbb\{Z\}^2$ for the image in
question).

We can define a general notion of a group convolution for group $G$ by
the equation $$\begin\{aligned\}
(\psi \ast \phi)(h) &= \sum_g \psi(g) \phi(g^\{-1\} h)
\end\{aligned\}$$ Here is a conversion of this basic definition into
Physika

``` \{.python language="python"\}
class DiscreteGroupConvolution(G: Group, w: $\mathbb\{R\}[|G|]$):
  def $\lambda$(X: $\mathbb\{R\}[|G|]$) -> $\mathbb\{R\}[|G|]$:
    X$'$ = zeros(|G|)
    for g h:
      X$'$[h] += w[g]*X[$g^\{-1\}$*h]
    return X$'$
```

If the group is continuous, the definition is given by $$\begin\{aligned\}
(\psi \ast \phi)(h) &= \int_g \psi(g) \phi(g^\{-1\} h) d\mu
\end\{aligned\}$$ The integral is defined with respect to the \"Haar
measure\", which provides a mathematically sound way of integrating
across broad families of continuous groups. Constructing a general layer
for a continuous group is trickier than for a discrete group since the
infinite summation cannot be done directly and has to to be performed by
some finite approximation (such as summing up terms in a fourier
decomposition).

It turns out that such group convolutions provide a natural framework to
construct $G$-equivariant layers. In fact, under broad conditions, a
linear function is $G$-equivariant if and only if it is defined by a
group convolution.

For finite groups, the group convolution equation can be directly
implemented. For continuous groups, matters become more complicated.

## Representation Theory

Group representations prove useful for modeling $G$-equivariant
functions when $G$ is a continuous group.

Suppose that group $G$ has irreducible representations $$\begin\{aligned\}
\rho_0,\dotsc, \rho_k
\end\{aligned\}$$ Suppose that we had functions $$\begin\{aligned\}
f_0: \rho_0(G) \to \mathbb\{R\}^n,\dotsc, f_k: \rho_k(G) \to \mathbb\{R\}^n
\end\{aligned\}$$ that acted respectively on the sets of matrices output
by the representations $\rho_i$. Suppose that
$\rho_i(G) \subseteq \mathbb\{R\}^\{n \times n\}$ Then we can construct a
function $f$ by \"summing\" these constituent functions
$$\begin\{aligned\}
f &: \rho_0(G) \oplus \dotsc \oplus \rho_k(G) \to \mathbb\{R\}^\{kn\} \\
f &= f_0 \oplus \dotsc \oplus f_k
\end\{aligned\}$$

For the specific case of linear functions, $f_i$ can be represented by a
vector of components $w_i$ (since each $\rho_i(g)$ is a $n \times n$
matrix for any $g \in G$). In this case, $f$ is just a vector of
parameters

$$\begin\{aligned\}
&f_i \in \mathbb\{R\}^\{n\} \\
&f \in \mathbb\{R\}^\{kn\}
\end\{aligned\}$$

Irreducible representations provide a sort of \"basis set\" for the
group, so it is in fact possible to decompose any function on the group
$G$ into functions on the irreducible representations. Consequently, if
you can construct meaningful group convolutions for each of the
irreducible representations, you can construct a group convolution for
the full group. In future chapters, we will explore how to construct
explicit equivariant layers for more complex groups.

## Equivariant Nonlinearities

A challenge in construcing equivariant architectures is that we want to
construct nonlinear operations that are equivariant. The Clebsch-Gordon
coefficients we saw previously prove very useful for providing a method
of blending information across the irreducible representations of a
group $$\begin\{aligned\}
f_i' &= \sum_j \sum_k CG_\{j,k,i\} f_i f_j
\end\{aligned\}$$

If we view the $f_i$ as vectors, then we can view this Clebsch-Gordon
transformation as an equivariant nonlinear transformation.

## Exercises

1.  Prove that the composition of two $G$-equivariant functions is
    $G$-equivariant.
