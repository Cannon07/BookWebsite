# Neural Model Discovery \{#chap:neural_cde\}

------------------------------------------------------------------------

\
**Prerequisites:** [\[chap:pde\]](#chap:pde)\{reference-type="ref+label"
reference="chap:pde"\},
[\[chap:numerical_pde\]](#chap:numerical_pde)\{reference-type="ref+label"
reference="chap:numerical_pde"\},
[\[chap:neural_ode\]](#chap:neural_ode)\{reference-type="ref+label"
reference="chap:neural_ode"\}\
**Difficulty Level:** \*\*\

------------------------------------------------------------------------

When there is noise added to a signal, traditional model recovery
methods like SINDy start to break down. Not at the first derivative but
at the higher order derivatives.

One idea is maybe you can use the PINNs for modeling the equation and
then using recovery? The problem in a PINNs is that we need to know the
equation.

Key idea of DeepMoD is

1.  Use a neural network to model the data

2.  Constrain the network to a list of features

3.  Add a sparsity promoting penalty to the features

We take in input data, do functional approximation to learn a model of
the data, next calculate features of the data using automatic
differentiation. Add in terms like $u_x, u, u^2, u u_x, u^2 u_\{xx\}$.
These terms cover most known partial differential equations. You can
then determine a mask. This was originally done non-differentiably, but
now more recently differentiable masking has been developed. This
sparsity mask can be applied. This mask is then fed back into the
original PINNs model to construct a fixed loss. It's a full loop.

Calculating the mask can be slow so update only once every few
iterations to speed up the process.

## Discovering Burgers Equation

Can we discover Burgers equation?

$$\begin\{aligned\}
u_t &= \nu u_\{xx\} - u u_x
\end\{aligned\}$$

Iterative discovery is performed from the loss. There can maybe be loss
spikes?

DeepMoD is notably robust against noise in the source data ([TODO: This
solves a problem that Venkat asked about neural discovery
methods]\{style="color: red"\}).

## Fully Differentiable Model Discovery

The non-differentiable masking is a bit of a wart. This adds in new
hyperparameters.

Existing differentiable masking is limited at the vector level sparsity.
So came up with a Bayesian approach. We can use the Bayesian approach to
encode prior knowledge about solutions. This approach is in general
called sparse Bayesian learning. This approach isn't very well
developed, but it is analytically tractable.

Place a Gaussian prior on each coefficient. We learn the variance
parameter on this distribution. We can combine this with the PINN. Using
the Gaussian prior instead of the $L^2$ term.

The core idea is to place a different amount of regularization on each
term. Then some terms basically drop out(?). This roughly acts like a
differentiable masking operation.

## Kuramoto-Sivashinsky Problem

$$\begin\{aligned\}
\frac\{\partial u\}\{\partial t\} &= \frac\{\partial^2 u\}\{\partial x^2\} - \frac\{\partial^4 u\}\{\partial x^4\} - u \frac\{\partial u\}\{\partial x\}
\end\{aligned\}$$

Past approaches break at .1 percent noise since the 4th order term
amplifies the noise.

## Random Walk

$$\begin\{aligned\}
x_\{i+1\} - x_i &= v \Delta t + \mathcal\{N\}(0, \sqrt\{2 D \Delta t\})
\end\{aligned\}$$

We can turn this into a population description $$\begin\{aligned\}
\frac\{\partial c\}\{\partial t\} &= D \frac\{\partial^2 c\}\{\partial x^2\} - v \frac\{\partial c\}\{\partial x\}
\end\{aligned\}$$ where $c$ is the density of the walkers. This is simply
the advection diffusion equation. Estimating the density is quite
challenging. How can we construct a time dependent probability? The core
idea is to construct a normalizing flow.

We can make a two dimensional normalizing flow. $$\begin\{aligned\}
p(x, t) &= \pi(z, \tau) \left |\frac\{dz\}\{dx\}\frac\{d\tau\}\{dt\} - \frac\{d \tau\}\{dx\}\frac\{dz\}\{dt\} \right |
\end\{aligned\}$$

We do multiscale model discovery. ([TODO: How?]\{style="color: red"\}) The
normalizing flow transforms walkers to a density. Then we can do model
discovery on the normalizing flow.

## Persepctives

How can this work on coefficient fields where equations have varying
coefficients? (Like phase fields for example). How can we do data fusion
and combine knowledge and data from different sources? How can we
incorporate symmetries?

It's also not clear if model discovery is actually useful since it's
mostly been used to rediscover known equations. Maybe this could be
useful in biophysics?
